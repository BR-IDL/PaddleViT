Traceback (most recent call last):
  File "main_multi_gpu_pretrain.py", line 24, in <module>
    import paddle
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/__init__.py", line 25, in <module>
    from .fluid import monkey_patch_variable
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/__init__.py", line 45, in <module>
    from . import dataset
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/dataset.py", line 19, in <module>
    from ..utils import deprecated
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/utils/__init__.py", line 26, in <module>
    from . import download  # noqa: F401
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/utils/download.py", line 23, in <module>
    import requests
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/requests/__init__.py", line 112, in <module>
    from . import utils
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/requests/utils.py", line 24, in <module>
    from . import certs
  File "<frozen importlib._bootstrap>", line 971, in _find_and_load
  File "<frozen importlib._bootstrap>", line 955, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 665, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 674, in exec_module
  File "<frozen importlib._bootstrap_external>", line 764, in get_code
  File "<frozen importlib._bootstrap_external>", line 833, in get_data
KeyboardInterrupt
merging config from ./configs/vit_base_patch16_224_pretrain_dec1.yaml
----- Imagenet2012 image train list len = 1281167
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:30053', '127.0.0.1:54949', '127.0.0.1:41862', '127.0.0.1:28777', '127.0.0.1:55177', '127.0.0.1:18423', '127.0.0.1:46681']
I1219 16:59:41.631045 23562 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:30053 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:54949', '127.0.0.1:41862', '127.0.0.1:28777', '127.0.0.1:55177', '127.0.0.1:18423', '127.0.0.1:46681']
I1219 16:59:44.247634 23580 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:54949 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:41862', '127.0.0.1:28777', '127.0.0.1:55177', '127.0.0.1:18423', '127.0.0.1:46681']
I1219 16:59:46.636570 23595 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:41862 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:28777', '127.0.0.1:55177', '127.0.0.1:18423', '127.0.0.1:46681']
I1219 16:59:48.816335 23610 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:28777 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:55177', '127.0.0.1:18423', '127.0.0.1:46681']
I1219 16:59:51.517431 23627 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:55177 successful.
I1219 16:59:53.801396 23642 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:18423 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:46681']
I1219 16:59:56.182962 23659 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:46681 successful.
I1219 16:59:56.935767 23580 nccl_context.cc:74] init nccl context nranks: 8 local rank: 2 gpu id: 2 ring id: 0
I1219 16:59:56.935765 23562 nccl_context.cc:74] init nccl context nranks: 8 local rank: 1 gpu id: 1 ring id: 0
I1219 16:59:56.935781 23627 nccl_context.cc:74] init nccl context nranks: 8 local rank: 5 gpu id: 5 ring id: 0
I1219 16:59:56.935775 23595 nccl_context.cc:74] init nccl context nranks: 8 local rank: 3 gpu id: 3 ring id: 0
I1219 16:59:56.935791 23642 nccl_context.cc:74] init nccl context nranks: 8 local rank: 6 gpu id: 6 ring id: 0
I1219 16:59:56.935806 23610 nccl_context.cc:74] init nccl context nranks: 8 local rank: 4 gpu id: 4 ring id: 0
I1219 16:59:56.935818 23659 nccl_context.cc:74] init nccl context nranks: 8 local rank: 7 gpu id: 7 ring id: 0
I1219 16:59:56.935837 23545 nccl_context.cc:74] init nccl context nranks: 8 local rank: 0 gpu id: 0 ring id: 0
W1219 17:00:00.904070 23545 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:00:00.904078 23562 device_context.cc:447] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:00:00.904153 23595 device_context.cc:447] Please NOTE: device: 3, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:00:00.904173 23610 device_context.cc:447] Please NOTE: device: 4, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:00:00.904186 23659 device_context.cc:447] Please NOTE: device: 7, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:00:00.904246 23642 device_context.cc:447] Please NOTE: device: 6, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:00:00.904264 23627 device_context.cc:447] Please NOTE: device: 5, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:00:00.906248 23580 device_context.cc:447] Please NOTE: device: 2, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:00:00.957355 23562 device_context.cc:465] device: 1, cuDNN Version: 7.6.
W1219 17:00:00.957355 23659 device_context.cc:465] device: 7, cuDNN Version: 7.6.
W1219 17:00:00.957358 23595 device_context.cc:465] device: 3, cuDNN Version: 7.6.
W1219 17:00:00.957360 23545 device_context.cc:465] device: 0, cuDNN Version: 7.6.
W1219 17:00:00.957374 23610 device_context.cc:465] device: 4, cuDNN Version: 7.6.
W1219 17:00:00.957383 23642 device_context.cc:465] device: 6, cuDNN Version: 7.6.
W1219 17:00:00.957394 23580 device_context.cc:465] device: 2, cuDNN Version: 7.6.
W1219 17:00:00.957394 23627 device_context.cc:465] device: 5, cuDNN Version: 7.6.
INFO:local_logger:----- world_size = 8, local_rank = 6
INFO:local_logger:----- world_size = 8, local_rank = 3
INFO:master_logger:
AMP: False
BASE: ['']
DATA:
  BATCH_SIZE: 256
  BATCH_SIZE_EVAL: 8
  CROP_PCT: 0.875
  DATASET: imagenet2012
  DATA_PATH: /dataset/imagenet
  IMAGE_SIZE: 224
  NUM_WORKERS: 4
EVAL: False
LOCAL_RANK: 0
MODEL:
  ATTENTION_DROPOUT: 0.1
  DROPOUT: 0.1
  DROPPATH: 0.0
  MAE_PRETRAIN: True
  NAME: vit_base_patch16_224_dec1
  NUM_CLASSES: 1000
  PRETRAINED: None
  RESUME: None
  TRANS:
    DECODER:
      DEPTH: 1
      EMBED_DIM: 512
      NUM_HEADS: 8
    ENCODER:
      DEPTH: 12
      EMBED_DIM: 768
      NUM_HEADS: 12
    MASK_RATIO: 0.75
    MLP_RATIO: 4.0
    PATCH_SIZE: 16
    QKV_BIAS: True
  TYPE: MAE
NGPUS: 8
REPORT_FREQ: 100
SAVE: ./output/train-20211219-16-59-32
SAVE_FREQ: 1
SEED: 0
TAG: default
TRAIN:
  ACCUM_ITER: 2
  BASE_LR: 0.00015
  CUTMIX_ALPHA: 1.0
  CUTMIX_MINMAX: None
  END_LR: 0.0005
  GRAD_CLIP: 1
  LAST_EPOCH: 0
  LINEAR_SCALED_LR: None
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    MILESTONES: 30, 60, 90
    NAME: warmupcosine
  MIXUP_ALPHA: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  NORMALIZE_TARGET: True
  NUM_EPOCHS: 800
  OPTIMIZER:
    BETAS: (0.9, 0.95)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: AdamW
  RAND_AUGMENT: False
  RAND_AUGMENT_LAYERS: 9
  RAND_AUGMENT_MAGNITUDE: 5
  SMOOTHING: 0.1
  WARMUP_EPOCHS: 40
  WARMUP_START_LR: 1e-06
  WEIGHT_DECAY: 0.05
VALIDATE_FREQ: 100
INFO:local_logger:----- world_size = 8, local_rank = 0
INFO:master_logger:----- world_size = 8, local_rank = 0
INFO:local_logger:----- world_size = 8, local_rank = 7
INFO:local_logger:----- world_size = 8, local_rank = 5
INFO:local_logger:----- world_size = 8, local_rank = 1
INFO:local_logger:----- world_size = 8, local_rank = 2
INFO:local_logger:----- world_size = 8, local_rank = 4
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:master_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:master_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:master_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 ERROR: Unexpected BUS error encountered in DataLoader worker. This might be caused by insufficient shared memory (shm), please check whether use_shared_memory is set and storage space in /dev/shm is enough
 Exception in thread Thread-1:
Traceback (most recent call last):
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/dataloader/dataloader_iter.py", line 583, in _get_data
    data = self._data_queue.get(timeout=self._timeout)
  File "/opt/conda/envs/py36/lib/python3.6/multiprocessing/queues.py", line 105, in get
    raise Empty
queue.Empty

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/conda/envs/py36/lib/python3.6/threading.py", line 916, in _bootstrap_inner
    self.run()
  File "/opt/conda/envs/py36/lib/python3.6/threading.py", line 864, in run
    self._target(*self._args, **self._kwargs)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/dataloader/dataloader_iter.py", line 505, in _thread_loop
    batch = self._get_data()
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/dataloader/dataloader_iter.py", line 599, in _get_data
    "pids: {}".format(len(failed_workers), pids))
RuntimeError: DataLoader 1 workers exit unexpectedly, pids: 23832



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1639904442 (unix time) try "date -d @1639904442" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x5be5) received by PID 23545 (TID 0x7f5dda7df700) from PID 23525 ***]

/opt/conda/envs/py36/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 20 leaked semaphores to clean up at shutdown
  len(cache))
Traceback (most recent call last):
  File "main_multi_gpu_pretrain.py", line 416, in <module>
    main()
  File "main_multi_gpu_pretrain.py", line 412, in main
    dist.spawn(main_worker, args=(config, dataset_train, ), nprocs=config.NGPUS)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/distributed/spawn.py", line 502, in spawn
    while not context.join():
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/distributed/spawn.py", line 312, in join
    self._throw_exception(error_index)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/distributed/spawn.py", line 330, in _throw_exception
    raise Exception(msg)
Exception: 

----------------------------------------------
Process 3 terminated with the following error:
----------------------------------------------

Traceback (most recent call last):
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/distributed/spawn.py", line 261, in _func_wrapper
    result = func(*args)
  File "/workspace/ppvit_github/PaddleViT_raw/PaddleViT/image_classification/MAE/main_multi_gpu_pretrain.py", line 368, in main_worker
    master_logger=master_logger)
  File "/workspace/ppvit_github/PaddleViT_raw/PaddleViT/image_classification/MAE/main_multi_gpu_pretrain.py", line 157, in train
    reconstructed_patches = model(images, masks)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/dygraph/layers.py", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/dygraph/parallel.py", line 695, in forward
    outputs = self._layers(*inputs, **kwargs)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/dygraph/layers.py", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/workspace/ppvit_github/PaddleViT_raw/PaddleViT/image_classification/MAE/transformer.py", line 537, in forward
    enc_out = self.encoder(no_mask_x)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/dygraph/layers.py", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/workspace/ppvit_github/PaddleViT_raw/PaddleViT/image_classification/MAE/transformer.py", line 364, in forward
    x = layer(x)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/dygraph/layers.py", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/workspace/ppvit_github/PaddleViT_raw/PaddleViT/image_classification/MAE/transformer.py", line 310, in forward
    x = self.mlp(x)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/dygraph/layers.py", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/workspace/ppvit_github/PaddleViT_raw/PaddleViT/image_classification/MAE/transformer.py", line 245, in forward
    x = self.fc1(x)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/dygraph/layers.py", line 914, in __call__
    outputs = self.forward(*inputs, **kwargs)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/nn/layer/common.py", line 172, in forward
    x=input, weight=self.weight, bias=self.bias, name=self.name)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/nn/functional/common.py", line 1474, in linear
    False)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/multiprocess_utils.py", line 134, in __handler__
    core._throw_error_if_process_failed()
SystemError: (Fatal) DataLoader process (pid   1. If run DataLoader by DataLoader.from_generator(...), queue capacity is set by from_generator(..., capacity=xx, ...).
  2. If run DataLoader by DataLoader(dataset, ...), queue capacity is set as 2 times of the max value of num_workers and len(places).
  3. If run by DataLoader(dataset, ..., use_shared_memory=True), set use_shared_memory=False for not using shared memory.) exited is killed by signal: 23723.
  It may be caused by insufficient shared storage space. This problem usually occurs when using docker as a development environment.
  Please use command `df -h` to check the storage space of `/dev/shm`. Shared storage space needs to be greater than (DataLoader Num * DataLoader queue capacity * 1 batch data size).
  You can solve this problem by increasing the shared storage space or reducing the queue capacity appropriately.
Bus error (at /paddle/paddle/fluid/imperative/data_loader.cc:177)


merging config from ./configs/vit_base_patch16_224_pretrain_dec1.yaml
----- Imagenet2012 image train list len = 1281167
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:58819', '127.0.0.1:34756', '127.0.0.1:44071', '127.0.0.1:12661', '127.0.0.1:44311', '127.0.0.1:14139', '127.0.0.1:51679']
I1219 17:02:09.309500 24382 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:58819 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:34756', '127.0.0.1:44071', '127.0.0.1:12661', '127.0.0.1:44311', '127.0.0.1:14139', '127.0.0.1:51679']
I1219 17:02:11.901250 24397 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:34756 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:44071', '127.0.0.1:12661', '127.0.0.1:44311', '127.0.0.1:14139', '127.0.0.1:51679']
I1219 17:02:14.341609 24414 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:44071 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:12661', '127.0.0.1:44311', '127.0.0.1:14139', '127.0.0.1:51679']
I1219 17:02:17.001890 24429 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:12661 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:44311', '127.0.0.1:14139', '127.0.0.1:51679']
I1219 17:02:19.379423 24447 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:44311 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:14139', '127.0.0.1:51679']
I1219 17:02:22.029084 24463 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:14139 successful.
I1219 17:02:24.569348 24481 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:51679 successful.
I1219 17:02:24.931157 24382 nccl_context.cc:74] init nccl context nranks: 8 local rank: 1 gpu id: 1 ring id: 0
I1219 17:02:24.931161 24397 nccl_context.cc:74] init nccl context nranks: 8 local rank: 2 gpu id: 2 ring id: 0
I1219 17:02:24.931192 24414 nccl_context.cc:74] init nccl context nranks: 8 local rank: 3 gpu id: 3 ring id: 0
I1219 17:02:24.931200 24429 nccl_context.cc:74] init nccl context nranks: 8 local rank: 4 gpu id: 4 ring id: 0
I1219 17:02:24.931208 24447 nccl_context.cc:74] init nccl context nranks: 8 local rank: 5 gpu id: 5 ring id: 0
I1219 17:02:24.931213 24463 nccl_context.cc:74] init nccl context nranks: 8 local rank: 6 gpu id: 6 ring id: 0
I1219 17:02:24.931216 24481 nccl_context.cc:74] init nccl context nranks: 8 local rank: 7 gpu id: 7 ring id: 0
I1219 17:02:24.931238 24365 nccl_context.cc:74] init nccl context nranks: 8 local rank: 0 gpu id: 0 ring id: 0
W1219 17:02:28.374552 24365 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:02:28.374681 24397 device_context.cc:447] Please NOTE: device: 2, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:02:28.374711 24414 device_context.cc:447] Please NOTE: device: 3, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:02:28.374712 24429 device_context.cc:447] Please NOTE: device: 4, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:02:28.374729 24447 device_context.cc:447] Please NOTE: device: 5, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:02:28.374773 24382 device_context.cc:447] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:02:28.374810 24463 device_context.cc:447] Please NOTE: device: 6, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:02:28.376953 24481 device_context.cc:447] Please NOTE: device: 7, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:02:28.382552 24414 device_context.cc:465] device: 3, cuDNN Version: 7.6.
W1219 17:02:28.382556 24365 device_context.cc:465] device: 0, cuDNN Version: 7.6.
W1219 17:02:28.382561 24447 device_context.cc:465] device: 5, cuDNN Version: 7.6.
W1219 17:02:28.382565 24397 device_context.cc:465] device: 2, cuDNN Version: 7.6.
W1219 17:02:28.382582 24463 device_context.cc:465] device: 6, cuDNN Version: 7.6.
W1219 17:02:28.382568 24429 device_context.cc:465] device: 4, cuDNN Version: 7.6.
W1219 17:02:28.382580 24382 device_context.cc:465] device: 1, cuDNN Version: 7.6.
W1219 17:02:28.382681 24481 device_context.cc:465] device: 7, cuDNN Version: 7.6.
INFO:local_logger:----- world_size = 8, local_rank = 1
INFO:local_logger:----- world_size = 8, local_rank = 5
INFO:local_logger:----- world_size = 8, local_rank = 3
INFO:local_logger:----- world_size = 8, local_rank = 2
INFO:local_logger:----- world_size = 8, local_rank = 7
INFO:local_logger:----- world_size = 8, local_rank = 6
INFO:master_logger:
AMP: False
BASE: ['']
DATA:
  BATCH_SIZE: 256
  BATCH_SIZE_EVAL: 8
  CROP_PCT: 0.875
  DATASET: imagenet2012
  DATA_PATH: /dataset/imagenet
  IMAGE_SIZE: 224
  NUM_WORKERS: 4
EVAL: False
LOCAL_RANK: 0
MODEL:
  ATTENTION_DROPOUT: 0.1
  DROPOUT: 0.1
  DROPPATH: 0.0
  MAE_PRETRAIN: True
  NAME: vit_base_patch16_224_dec1
  NUM_CLASSES: 1000
  PRETRAINED: None
  RESUME: None
  TRANS:
    DECODER:
      DEPTH: 1
      EMBED_DIM: 512
      NUM_HEADS: 8
    ENCODER:
      DEPTH: 12
      EMBED_DIM: 768
      NUM_HEADS: 12
    MASK_RATIO: 0.75
    MLP_RATIO: 4.0
    PATCH_SIZE: 16
    QKV_BIAS: True
  TYPE: MAE
NGPUS: 8
REPORT_FREQ: 100
SAVE: ./output/train-20211219-17-02-00
SAVE_FREQ: 1
SEED: 0
TAG: default
TRAIN:
  ACCUM_ITER: 2
  BASE_LR: 0.00015
  CUTMIX_ALPHA: 1.0
  CUTMIX_MINMAX: None
  END_LR: 0.0005
  GRAD_CLIP: 1
  LAST_EPOCH: 0
  LINEAR_SCALED_LR: None
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    MILESTONES: 30, 60, 90
    NAME: warmupcosine
  MIXUP_ALPHA: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  NORMALIZE_TARGET: True
  NUM_EPOCHS: 800
  OPTIMIZER:
    BETAS: (0.9, 0.95)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: AdamW
  RAND_AUGMENT: False
  RAND_AUGMENT_LAYERS: 9
  RAND_AUGMENT_MAGNITUDE: 5
  SMOOTHING: 0.1
  WARMUP_EPOCHS: 40
  WARMUP_START_LR: 1e-06
  WEIGHT_DECAY: 0.05
VALIDATE_FREQ: 100
INFO:local_logger:----- world_size = 8, local_rank = 0
INFO:master_logger:----- world_size = 8, local_rank = 0
INFO:local_logger:----- world_size = 8, local_rank = 4
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:master_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:master_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:master_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1452
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1431
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1469
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1481
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1408
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1501
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1475
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1440
INFO:master_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1457


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
No stack trace in paddle, may be caused by external reasons.

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1639904603 (unix time) try "date -d @1639904603" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x5f17) received by PID 24365 (TID 0x7f5d5ca46700) from PID 24343 ***]

Traceback (most recent call last):
  File "main_multi_gpu_pretrain.py", line 416, in <module>
    main()
  File "main_multi_gpu_pretrain.py", line 412, in main
    dist.spawn(main_worker, args=(config, dataset_train, ), nprocs=config.NGPUS)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/distributed/spawn.py", line 502, in spawn
    while not context.join():
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/distributed/spawn.py", line 312, in join
    self._throw_exception(error_index)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/distributed/spawn.py", line 330, in _throw_exception
    raise Exception(msg)
Exception: 

----------------------------------------------
Process 1 terminated with the following error:
----------------------------------------------

Traceback (most recent call last):
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/distributed/spawn.py", line 261, in _func_wrapper
    result = func(*args)
  File "/workspace/ppvit_github/PaddleViT_raw/PaddleViT/image_classification/MAE/main_multi_gpu_pretrain.py", line 368, in main_worker
    master_logger=master_logger)
  File "/workspace/ppvit_github/PaddleViT_raw/PaddleViT/image_classification/MAE/main_multi_gpu_pretrain.py", line 163, in train
    loss.backward()
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/decorator.py", line 232, in fun
    return caller(func, *(extras + args), **kw)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/wrapped_decorator.py", line 25, in __impl__
    return wrapped_func(*args, **kwargs)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/framework.py", line 229, in __impl__
    return func(*args, **kwargs)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/fluid/dygraph/varbase_patch_methods.py", line 239, in backward
    framework._dygraph_tracer())
OSError: (External) ResourceExhaustedError: 

Out of memory error on GPU 1. Cannot allocate 394.000244MB memory on GPU 1, 15.719788GB memory has been allocated and available memory is only 63.437500MB.

Please check whether there is any other process using GPU 1.
1. If yes, please stop them, or start PaddlePaddle on another GPU.
2. If no, please decrease the batch size of your model. 

 (at /paddle/paddle/fluid/memory/allocation/cuda_allocator.cc:79)
 (at /paddle/paddle/fluid/imperative/basic_engine.cc:568)


merging config from ./configs/vit_base_patch16_224_pretrain_dec1.yaml
----- Imagenet2012 image train list len = 1281167
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:45480', '127.0.0.1:58605', '127.0.0.1:23406', '127.0.0.1:16014', '127.0.0.1:60086', '127.0.0.1:60603', '127.0.0.1:46782']
I1219 17:07:49.286090 25456 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:45480 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:58605', '127.0.0.1:23406', '127.0.0.1:16014', '127.0.0.1:60086', '127.0.0.1:60603', '127.0.0.1:46782']
I1219 17:07:51.690086 25473 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:58605 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:23406', '127.0.0.1:16014', '127.0.0.1:60086', '127.0.0.1:60603', '127.0.0.1:46782']
I1219 17:07:54.058967 25488 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:23406 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:16014', '127.0.0.1:60086', '127.0.0.1:60603', '127.0.0.1:46782']
I1219 17:07:57.064612 25503 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:16014 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:60086', '127.0.0.1:60603', '127.0.0.1:46782']
I1219 17:07:59.496040 25520 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:60086 successful.
server not ready, wait 3 sec to retry...
not ready endpoints:['127.0.0.1:60603', '127.0.0.1:46782']
I1219 17:08:02.203279 25537 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:60603 successful.
I1219 17:08:04.597697 25554 gen_comm_id_helper.cc:190] Server listening on: 127.0.0.1:46782 successful.
I1219 17:08:05.017540 25473 nccl_context.cc:74] init nccl context nranks: 8 local rank: 2 gpu id: 2 ring id: 0
I1219 17:08:05.017537 25456 nccl_context.cc:74] init nccl context nranks: 8 local rank: 1 gpu id: 1 ring id: 0
I1219 17:08:05.017560 25488 nccl_context.cc:74] init nccl context nranks: 8 local rank: 3 gpu id: 3 ring id: 0
I1219 17:08:05.017565 25537 nccl_context.cc:74] init nccl context nranks: 8 local rank: 6 gpu id: 6 ring id: 0
I1219 17:08:05.017578 25503 nccl_context.cc:74] init nccl context nranks: 8 local rank: 4 gpu id: 4 ring id: 0
I1219 17:08:05.017585 25520 nccl_context.cc:74] init nccl context nranks: 8 local rank: 5 gpu id: 5 ring id: 0
I1219 17:08:05.017601 25554 nccl_context.cc:74] init nccl context nranks: 8 local rank: 7 gpu id: 7 ring id: 0
I1219 17:08:05.017613 25441 nccl_context.cc:74] init nccl context nranks: 8 local rank: 0 gpu id: 0 ring id: 0
W1219 17:08:09.206136 25441 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:08:09.206564 25456 device_context.cc:447] Please NOTE: device: 1, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:08:09.206579 25554 device_context.cc:447] Please NOTE: device: 7, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:08:09.206670 25488 device_context.cc:447] Please NOTE: device: 3, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:08:09.206694 25520 device_context.cc:447] Please NOTE: device: 5, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:08:09.206728 25503 device_context.cc:447] Please NOTE: device: 4, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:08:09.209081 25537 device_context.cc:447] Please NOTE: device: 6, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:08:09.209785 25473 device_context.cc:447] Please NOTE: device: 2, GPU Compute Capability: 7.0, Driver API Version: 10.2, Runtime API Version: 10.2
W1219 17:08:09.212059 25456 device_context.cc:465] device: 1, cuDNN Version: 7.6.
W1219 17:08:09.212066 25554 device_context.cc:465] device: 7, cuDNN Version: 7.6.
W1219 17:08:09.212080 25503 device_context.cc:465] device: 4, cuDNN Version: 7.6.
W1219 17:08:09.212086 25520 device_context.cc:465] device: 5, cuDNN Version: 7.6.
W1219 17:08:09.212086 25488 device_context.cc:465] device: 3, cuDNN Version: 7.6.
W1219 17:08:09.212239 25441 device_context.cc:465] device: 0, cuDNN Version: 7.6.
W1219 17:08:09.213409 25537 device_context.cc:465] device: 6, cuDNN Version: 7.6.
W1219 17:08:09.214195 25473 device_context.cc:465] device: 2, cuDNN Version: 7.6.
INFO:local_logger:----- world_size = 8, local_rank = 4
INFO:local_logger:----- world_size = 8, local_rank = 1
INFO:local_logger:----- world_size = 8, local_rank = 2
INFO:master_logger:
AMP: True
BASE: ['']
DATA:
  BATCH_SIZE: 256
  BATCH_SIZE_EVAL: 8
  CROP_PCT: 0.875
  DATASET: imagenet2012
  DATA_PATH: /dataset/imagenet
  IMAGE_SIZE: 224
  NUM_WORKERS: 2
EVAL: False
LOCAL_RANK: 0
MODEL:
  ATTENTION_DROPOUT: 0.0
  DROPOUT: 0.0
  DROPPATH: 0.0
  MAE_PRETRAIN: True
  NAME: vit_base_patch16_224_dec1
  NUM_CLASSES: 1000
  PRETRAINED: None
  RESUME: None
  TRANS:
    DECODER:
      DEPTH: 1
      EMBED_DIM: 512
      NUM_HEADS: 8
    ENCODER:
      DEPTH: 12
      EMBED_DIM: 768
      NUM_HEADS: 12
    MASK_RATIO: 0.75
    MLP_RATIO: 4.0
    PATCH_SIZE: 16
    QKV_BIAS: True
  TYPE: MAE
NGPUS: 8
REPORT_FREQ: 100
SAVE: ./output/train-20211219-17-07-40
SAVE_FREQ: 1
SEED: 0
TAG: default
TRAIN:
  ACCUM_ITER: 2
  BASE_LR: 0.00015
  CUTMIX_ALPHA: 1.0
  CUTMIX_MINMAX: None
  END_LR: 0.0005
  GRAD_CLIP: 1
  LAST_EPOCH: 0
  LINEAR_SCALED_LR: None
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    MILESTONES: 30, 60, 90
    NAME: warmupcosine
  MIXUP_ALPHA: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  NORMALIZE_TARGET: True
  NUM_EPOCHS: 800
  OPTIMIZER:
    BETAS: (0.9, 0.95)
    EPS: 1e-08
    MOMENTUM: 0.9
    NAME: AdamW
  RAND_AUGMENT: False
  RAND_AUGMENT_LAYERS: 9
  RAND_AUGMENT_MAGNITUDE: 5
  SMOOTHING: 0.1
  WARMUP_EPOCHS: 40
  WARMUP_START_LR: 1e-06
  WEIGHT_DECAY: 0.05
VALIDATE_FREQ: 100
INFO:local_logger:----- world_size = 8, local_rank = 0
INFO:master_logger:----- world_size = 8, local_rank = 0
INFO:local_logger:----- world_size = 8, local_rank = 6
INFO:local_logger:----- world_size = 8, local_rank = 5
INFO:local_logger:----- world_size = 8, local_rank = 7
INFO:local_logger:----- world_size = 8, local_rank = 3
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:master_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:master_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:master_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:----- Total # of train batch (single gpu): 626
INFO:local_logger:Start training from epoch 1.
INFO:local_logger:Now training epoch 1. LR=0.000005
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1468
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1446
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1495
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1428
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1450
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1461
INFO:master_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1454
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1459
INFO:local_logger:Epoch[001/800], Step[0000/0626], Avg Loss: 1.1427
INFO:local_logger:Epoch[001/800], Step[0100/0626], Avg Loss: 1.1136
INFO:local_logger:Epoch[001/800], Step[0100/0626], Avg Loss: 1.1140
INFO:local_logger:Epoch[001/800], Step[0100/0626], Avg Loss: 1.1137
INFO:local_logger:Epoch[001/800], Step[0100/0626], Avg Loss: 1.1132
INFO:local_logger:Epoch[001/800], Step[0100/0626], Avg Loss: 1.1132
INFO:master_logger:Epoch[001/800], Step[0100/0626], Avg Loss: 1.1136
INFO:local_logger:Epoch[001/800], Step[0100/0626], Avg Loss: 1.1135
INFO:local_logger:Epoch[001/800], Step[0100/0626], Avg Loss: 1.1138
INFO:local_logger:Epoch[001/800], Step[0100/0626], Avg Loss: 1.1139
INFO:local_logger:Epoch[001/800], Step[0200/0626], Avg Loss: 1.0903
INFO:local_logger:Epoch[001/800], Step[0200/0626], Avg Loss: 1.0904
INFO:local_logger:Epoch[001/800], Step[0200/0626], Avg Loss: 1.0904
INFO:local_logger:Epoch[001/800], Step[0200/0626], Avg Loss: 1.0908
INFO:local_logger:Epoch[001/800], Step[0200/0626], Avg Loss: 1.0903
INFO:local_logger:Epoch[001/800], Step[0200/0626], Avg Loss: 1.0900
INFO:local_logger:Epoch[001/800], Step[0200/0626], Avg Loss: 1.0904
INFO:local_logger:Epoch[001/800], Step[0200/0626], Avg Loss: 1.0902
INFO:master_logger:Epoch[001/800], Step[0200/0626], Avg Loss: 1.0904
INFO:local_logger:Epoch[001/800], Step[0300/0626], Avg Loss: 1.0723
INFO:local_logger:Epoch[001/800], Step[0300/0626], Avg Loss: 1.0717
INFO:local_logger:Epoch[001/800], Step[0300/0626], Avg Loss: 1.0718
INFO:local_logger:Epoch[001/800], Step[0300/0626], Avg Loss: 1.0716
INFO:local_logger:Epoch[001/800], Step[0300/0626], Avg Loss: 1.0719
INFO:master_logger:Epoch[001/800], Step[0300/0626], Avg Loss: 1.0719
INFO:local_logger:Epoch[001/800], Step[0300/0626], Avg Loss: 1.0718
INFO:local_logger:Epoch[001/800], Step[0300/0626], Avg Loss: 1.0720
INFO:local_logger:Epoch[001/800], Step[0300/0626], Avg Loss: 1.0720
INFO:local_logger:Epoch[001/800], Step[0400/0626], Avg Loss: 1.0576
INFO:local_logger:Epoch[001/800], Step[0400/0626], Avg Loss: 1.0572
INFO:local_logger:Epoch[001/800], Step[0400/0626], Avg Loss: 1.0572
INFO:local_logger:Epoch[001/800], Step[0400/0626], Avg Loss: 1.0570
INFO:local_logger:Epoch[001/800], Step[0400/0626], Avg Loss: 1.0573
INFO:local_logger:Epoch[001/800], Step[0400/0626], Avg Loss: 1.0570
INFO:local_logger:Epoch[001/800], Step[0400/0626], Avg Loss: 1.0573
INFO:master_logger:Epoch[001/800], Step[0400/0626], Avg Loss: 1.0572
INFO:local_logger:Epoch[001/800], Step[0400/0626], Avg Loss: 1.0574
INFO:local_logger:Epoch[001/800], Step[0500/0626], Avg Loss: 1.0461
INFO:local_logger:Epoch[001/800], Step[0500/0626], Avg Loss: 1.0459
INFO:local_logger:Epoch[001/800], Step[0500/0626], Avg Loss: 1.0459
INFO:local_logger:Epoch[001/800], Step[0500/0626], Avg Loss: 1.0461
INFO:local_logger:Epoch[001/800], Step[0500/0626], Avg Loss: 1.0457
INFO:local_logger:Epoch[001/800], Step[0500/0626], Avg Loss: 1.0461
INFO:master_logger:Epoch[001/800], Step[0500/0626], Avg Loss: 1.0460
INFO:local_logger:Epoch[001/800], Step[0500/0626], Avg Loss: 1.0463
INFO:local_logger:Epoch[001/800], Step[0500/0626], Avg Loss: 1.0461
INFO:local_logger:Epoch[001/800], Step[0600/0626], Avg Loss: 1.0374
INFO:local_logger:Epoch[001/800], Step[0600/0626], Avg Loss: 1.0374
INFO:local_logger:Epoch[001/800], Step[0600/0626], Avg Loss: 1.0375
INFO:local_logger:Epoch[001/800], Step[0600/0626], Avg Loss: 1.0375
INFO:master_logger:Epoch[001/800], Step[0600/0626], Avg Loss: 1.0375
INFO:local_logger:Epoch[001/800], Step[0600/0626], Avg Loss: 1.0372
INFO:local_logger:Epoch[001/800], Step[0600/0626], Avg Loss: 1.0377
INFO:local_logger:Epoch[001/800], Step[0600/0626], Avg Loss: 1.0379
INFO:local_logger:Epoch[001/800], Step[0600/0626], Avg Loss: 1.0374
INFO:local_logger:----- Epoch[001/800], Train Loss: 1.0359, time: 934.80
INFO:local_logger:Now training epoch 2. LR=0.000008
INFO:local_logger:----- Epoch[001/800], Train Loss: 1.0356, time: 934.81
INFO:local_logger:Now training epoch 2. LR=0.000008
INFO:local_logger:----- Epoch[001/800], Train Loss: 1.0354, time: 934.86
INFO:local_logger:Now training epoch 2. LR=0.000008
INFO:local_logger:----- Epoch[001/800], Train Loss: 1.0361, time: 934.98
INFO:local_logger:Now training epoch 2. LR=0.000008
INFO:local_logger:----- Epoch[001/800], Train Loss: 1.0358, time: 935.03
INFO:master_logger:----- Epoch[001/800], Train Loss: 1.0357, time: 935.03
INFO:local_logger:----- Epoch[001/800], Train Loss: 1.0358, time: 935.07
INFO:local_logger:Now training epoch 2. LR=0.000008
INFO:local_logger:----- Epoch[001/800], Train Loss: 1.0356, time: 935.07
INFO:local_logger:Now training epoch 2. LR=0.000008
INFO:local_logger:----- Epoch[001/800], Train Loss: 1.0357, time: 935.09
INFO:local_logger:Now training epoch 2. LR=0.000008
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-1-Loss-1.0357822933105671.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-1-Loss-1.0357822933105671.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-1-Loss-1.0357822933105671.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-1-Loss-1.0357822933105671.pdopt
INFO:local_logger:Now training epoch 2. LR=0.000008
INFO:master_logger:Now training epoch 2. LR=0.000008
INFO:local_logger:Epoch[002/800], Step[0000/0626], Avg Loss: 0.9953
INFO:master_logger:Epoch[002/800], Step[0000/0626], Avg Loss: 0.9905
INFO:local_logger:Epoch[002/800], Step[0000/0626], Avg Loss: 0.9836
INFO:local_logger:Epoch[002/800], Step[0000/0626], Avg Loss: 0.9941
INFO:local_logger:Epoch[002/800], Step[0000/0626], Avg Loss: 0.9887
INFO:local_logger:Epoch[002/800], Step[0000/0626], Avg Loss: 0.9872
INFO:local_logger:Epoch[002/800], Step[0000/0626], Avg Loss: 0.9919
INFO:local_logger:Epoch[002/800], Step[0000/0626], Avg Loss: 0.9949
INFO:local_logger:Epoch[002/800], Step[0000/0626], Avg Loss: 0.9885
INFO:local_logger:Epoch[002/800], Step[0100/0626], Avg Loss: 0.9896
INFO:local_logger:Epoch[002/800], Step[0100/0626], Avg Loss: 0.9894
INFO:local_logger:Epoch[002/800], Step[0100/0626], Avg Loss: 0.9900
INFO:local_logger:Epoch[002/800], Step[0100/0626], Avg Loss: 0.9895
INFO:local_logger:Epoch[002/800], Step[0100/0626], Avg Loss: 0.9901
INFO:local_logger:Epoch[002/800], Step[0100/0626], Avg Loss: 0.9887
INFO:local_logger:Epoch[002/800], Step[0100/0626], Avg Loss: 0.9897
INFO:master_logger:Epoch[002/800], Step[0100/0626], Avg Loss: 0.9896
INFO:local_logger:Epoch[002/800], Step[0100/0626], Avg Loss: 0.9900
INFO:local_logger:Epoch[002/800], Step[0200/0626], Avg Loss: 0.9880
INFO:local_logger:Epoch[002/800], Step[0200/0626], Avg Loss: 0.9889
INFO:local_logger:Epoch[002/800], Step[0200/0626], Avg Loss: 0.9887
INFO:local_logger:Epoch[002/800], Step[0200/0626], Avg Loss: 0.9883
INFO:local_logger:Epoch[002/800], Step[0200/0626], Avg Loss: 0.9887
INFO:local_logger:Epoch[002/800], Step[0200/0626], Avg Loss: 0.9887
INFO:local_logger:Epoch[002/800], Step[0200/0626], Avg Loss: 0.9883
INFO:master_logger:Epoch[002/800], Step[0200/0626], Avg Loss: 0.9885
INFO:local_logger:Epoch[002/800], Step[0200/0626], Avg Loss: 0.9883
INFO:local_logger:Epoch[002/800], Step[0300/0626], Avg Loss: 0.9878
INFO:local_logger:Epoch[002/800], Step[0300/0626], Avg Loss: 0.9874
INFO:local_logger:Epoch[002/800], Step[0300/0626], Avg Loss: 0.9873
INFO:local_logger:Epoch[002/800], Step[0300/0626], Avg Loss: 0.9875
INFO:master_logger:Epoch[002/800], Step[0300/0626], Avg Loss: 0.9876
INFO:local_logger:Epoch[002/800], Step[0300/0626], Avg Loss: 0.9877
INFO:local_logger:Epoch[002/800], Step[0300/0626], Avg Loss: 0.9880
INFO:local_logger:Epoch[002/800], Step[0300/0626], Avg Loss: 0.9878
INFO:local_logger:Epoch[002/800], Step[0300/0626], Avg Loss: 0.9872
INFO:local_logger:Epoch[002/800], Step[0400/0626], Avg Loss: 0.9872
INFO:local_logger:Epoch[002/800], Step[0400/0626], Avg Loss: 0.9870
INFO:local_logger:Epoch[002/800], Step[0400/0626], Avg Loss: 0.9867
INFO:local_logger:Epoch[002/800], Step[0400/0626], Avg Loss: 0.9867
INFO:local_logger:Epoch[002/800], Step[0400/0626], Avg Loss: 0.9870
INFO:local_logger:Epoch[002/800], Step[0400/0626], Avg Loss: 0.9871
INFO:local_logger:Epoch[002/800], Step[0400/0626], Avg Loss: 0.9870
INFO:local_logger:Epoch[002/800], Step[0400/0626], Avg Loss: 0.9868
INFO:master_logger:Epoch[002/800], Step[0400/0626], Avg Loss: 0.9869
INFO:local_logger:Epoch[002/800], Step[0500/0626], Avg Loss: 0.9862
INFO:local_logger:Epoch[002/800], Step[0500/0626], Avg Loss: 0.9865
INFO:local_logger:Epoch[002/800], Step[0500/0626], Avg Loss: 0.9861
INFO:local_logger:Epoch[002/800], Step[0500/0626], Avg Loss: 0.9864
INFO:local_logger:Epoch[002/800], Step[0500/0626], Avg Loss: 0.9863
INFO:local_logger:Epoch[002/800], Step[0500/0626], Avg Loss: 0.9861
INFO:local_logger:Epoch[002/800], Step[0500/0626], Avg Loss: 0.9862
INFO:local_logger:Epoch[002/800], Step[0500/0626], Avg Loss: 0.9863
INFO:master_logger:Epoch[002/800], Step[0500/0626], Avg Loss: 0.9863
INFO:local_logger:Epoch[002/800], Step[0600/0626], Avg Loss: 0.9856
INFO:local_logger:Epoch[002/800], Step[0600/0626], Avg Loss: 0.9858
INFO:local_logger:Epoch[002/800], Step[0600/0626], Avg Loss: 0.9858
INFO:local_logger:Epoch[002/800], Step[0600/0626], Avg Loss: 0.9855
INFO:local_logger:Epoch[002/800], Step[0600/0626], Avg Loss: 0.9855
INFO:local_logger:Epoch[002/800], Step[0600/0626], Avg Loss: 0.9856
INFO:master_logger:Epoch[002/800], Step[0600/0626], Avg Loss: 0.9856
INFO:local_logger:Epoch[002/800], Step[0600/0626], Avg Loss: 0.9856
INFO:local_logger:Epoch[002/800], Step[0600/0626], Avg Loss: 0.9856
INFO:local_logger:----- Epoch[002/800], Train Loss: 0.9857, time: 891.36
INFO:local_logger:Now training epoch 3. LR=0.000012
INFO:local_logger:----- Epoch[002/800], Train Loss: 0.9855, time: 891.28
INFO:local_logger:Now training epoch 3. LR=0.000012
INFO:local_logger:----- Epoch[002/800], Train Loss: 0.9853, time: 891.70
INFO:local_logger:Now training epoch 3. LR=0.000012
INFO:local_logger:----- Epoch[002/800], Train Loss: 0.9855, time: 891.46
INFO:local_logger:Now training epoch 3. LR=0.000012
INFO:local_logger:----- Epoch[002/800], Train Loss: 0.9853, time: 891.66
INFO:local_logger:Now training epoch 3. LR=0.000012
INFO:local_logger:----- Epoch[002/800], Train Loss: 0.9855, time: 891.47
INFO:local_logger:Now training epoch 3. LR=0.000012
INFO:local_logger:----- Epoch[002/800], Train Loss: 0.9857, time: 891.56
INFO:local_logger:Now training epoch 3. LR=0.000012
INFO:local_logger:----- Epoch[002/800], Train Loss: 0.9854, time: 887.62
INFO:master_logger:----- Epoch[002/800], Train Loss: 0.9855, time: 887.62
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-2-Loss-0.9854484576284688.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-2-Loss-0.9854484576284688.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-2-Loss-0.9854484576284688.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-2-Loss-0.9854484576284688.pdopt
INFO:local_logger:Now training epoch 3. LR=0.000012
INFO:master_logger:Now training epoch 3. LR=0.000012
INFO:local_logger:Epoch[003/800], Step[0000/0626], Avg Loss: 0.9859
INFO:local_logger:Epoch[003/800], Step[0000/0626], Avg Loss: 0.9784
INFO:local_logger:Epoch[003/800], Step[0000/0626], Avg Loss: 0.9751
INFO:master_logger:Epoch[003/800], Step[0000/0626], Avg Loss: 0.9809
INFO:local_logger:Epoch[003/800], Step[0000/0626], Avg Loss: 0.9834
INFO:local_logger:Epoch[003/800], Step[0000/0626], Avg Loss: 0.9795
INFO:local_logger:Epoch[003/800], Step[0000/0626], Avg Loss: 0.9809
INFO:local_logger:Epoch[003/800], Step[0000/0626], Avg Loss: 0.9833
INFO:local_logger:Epoch[003/800], Step[0000/0626], Avg Loss: 0.9810
INFO:local_logger:Epoch[003/800], Step[0100/0626], Avg Loss: 0.9816
INFO:local_logger:Epoch[003/800], Step[0100/0626], Avg Loss: 0.9810
INFO:local_logger:Epoch[003/800], Step[0100/0626], Avg Loss: 0.9814
INFO:local_logger:Epoch[003/800], Step[0100/0626], Avg Loss: 0.9810
INFO:master_logger:Epoch[003/800], Step[0100/0626], Avg Loss: 0.9813
INFO:local_logger:Epoch[003/800], Step[0100/0626], Avg Loss: 0.9813
INFO:local_logger:Epoch[003/800], Step[0100/0626], Avg Loss: 0.9814
INFO:local_logger:Epoch[003/800], Step[0100/0626], Avg Loss: 0.9813
INFO:local_logger:Epoch[003/800], Step[0100/0626], Avg Loss: 0.9814
INFO:local_logger:Epoch[003/800], Step[0200/0626], Avg Loss: 0.9807
INFO:local_logger:Epoch[003/800], Step[0200/0626], Avg Loss: 0.9808
INFO:local_logger:Epoch[003/800], Step[0200/0626], Avg Loss: 0.9808
INFO:local_logger:Epoch[003/800], Step[0200/0626], Avg Loss: 0.9806
INFO:local_logger:Epoch[003/800], Step[0200/0626], Avg Loss: 0.9806
INFO:local_logger:Epoch[003/800], Step[0200/0626], Avg Loss: 0.9804
INFO:local_logger:Epoch[003/800], Step[0200/0626], Avg Loss: 0.9804
INFO:local_logger:Epoch[003/800], Step[0200/0626], Avg Loss: 0.9804
INFO:master_logger:Epoch[003/800], Step[0200/0626], Avg Loss: 0.9806
INFO:local_logger:Epoch[003/800], Step[0300/0626], Avg Loss: 0.9797
INFO:local_logger:Epoch[003/800], Step[0300/0626], Avg Loss: 0.9799
INFO:local_logger:Epoch[003/800], Step[0300/0626], Avg Loss: 0.9799
INFO:local_logger:Epoch[003/800], Step[0300/0626], Avg Loss: 0.9802
INFO:local_logger:Epoch[003/800], Step[0300/0626], Avg Loss: 0.9797
INFO:master_logger:Epoch[003/800], Step[0300/0626], Avg Loss: 0.9799
INFO:local_logger:Epoch[003/800], Step[0300/0626], Avg Loss: 0.9798
INFO:local_logger:Epoch[003/800], Step[0300/0626], Avg Loss: 0.9799
INFO:local_logger:Epoch[003/800], Step[0300/0626], Avg Loss: 0.9798
INFO:local_logger:Epoch[003/800], Step[0400/0626], Avg Loss: 0.9791
INFO:local_logger:Epoch[003/800], Step[0400/0626], Avg Loss: 0.9790
INFO:local_logger:Epoch[003/800], Step[0400/0626], Avg Loss: 0.9793
INFO:local_logger:Epoch[003/800], Step[0400/0626], Avg Loss: 0.9789
INFO:local_logger:Epoch[003/800], Step[0400/0626], Avg Loss: 0.9789
INFO:master_logger:Epoch[003/800], Step[0400/0626], Avg Loss: 0.9790
INFO:local_logger:Epoch[003/800], Step[0400/0626], Avg Loss: 0.9789
INFO:local_logger:Epoch[003/800], Step[0400/0626], Avg Loss: 0.9789
INFO:local_logger:Epoch[003/800], Step[0400/0626], Avg Loss: 0.9791
INFO:local_logger:Epoch[003/800], Step[0500/0626], Avg Loss: 0.9780
INFO:local_logger:Epoch[003/800], Step[0500/0626], Avg Loss: 0.9782
INFO:local_logger:Epoch[003/800], Step[0500/0626], Avg Loss: 0.9782
INFO:local_logger:Epoch[003/800], Step[0500/0626], Avg Loss: 0.9783
INFO:master_logger:Epoch[003/800], Step[0500/0626], Avg Loss: 0.9782
INFO:local_logger:Epoch[003/800], Step[0500/0626], Avg Loss: 0.9781
INFO:local_logger:Epoch[003/800], Step[0500/0626], Avg Loss: 0.9786
INFO:local_logger:Epoch[003/800], Step[0500/0626], Avg Loss: 0.9783
INFO:local_logger:Epoch[003/800], Step[0500/0626], Avg Loss: 0.9781
INFO:local_logger:Epoch[003/800], Step[0600/0626], Avg Loss: 0.9776
INFO:local_logger:Epoch[003/800], Step[0600/0626], Avg Loss: 0.9776
INFO:local_logger:Epoch[003/800], Step[0600/0626], Avg Loss: 0.9774
INFO:local_logger:Epoch[003/800], Step[0600/0626], Avg Loss: 0.9774
INFO:local_logger:Epoch[003/800], Step[0600/0626], Avg Loss: 0.9778
INFO:master_logger:Epoch[003/800], Step[0600/0626], Avg Loss: 0.9775
INFO:local_logger:Epoch[003/800], Step[0600/0626], Avg Loss: 0.9774
INFO:local_logger:Epoch[003/800], Step[0600/0626], Avg Loss: 0.9773
INFO:local_logger:Epoch[003/800], Step[0600/0626], Avg Loss: 0.9773
INFO:local_logger:----- Epoch[003/800], Train Loss: 0.9774, time: 893.09
INFO:local_logger:Now training epoch 4. LR=0.000016
INFO:local_logger:----- Epoch[003/800], Train Loss: 0.9772, time: 893.23
INFO:local_logger:Now training epoch 4. LR=0.000016
INFO:local_logger:----- Epoch[003/800], Train Loss: 0.9776, time: 893.27
INFO:local_logger:Now training epoch 4. LR=0.000016
INFO:local_logger:----- Epoch[003/800], Train Loss: 0.9771, time: 893.31
INFO:local_logger:Now training epoch 4. LR=0.000016
INFO:local_logger:----- Epoch[003/800], Train Loss: 0.9772, time: 893.74
INFO:local_logger:Now training epoch 4. LR=0.000016
INFO:local_logger:----- Epoch[003/800], Train Loss: 0.9772, time: 889.63
INFO:master_logger:----- Epoch[003/800], Train Loss: 0.9773, time: 889.63
INFO:local_logger:----- Epoch[003/800], Train Loss: 0.9773, time: 893.40
INFO:local_logger:Now training epoch 4. LR=0.000016
INFO:local_logger:----- Epoch[003/800], Train Loss: 0.9775, time: 893.56
INFO:local_logger:Now training epoch 4. LR=0.000016
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-3-Loss-0.9772286424963117.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-3-Loss-0.9772286424963117.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-3-Loss-0.9772286424963117.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-3-Loss-0.9772286424963117.pdopt
INFO:local_logger:Now training epoch 4. LR=0.000016
INFO:master_logger:Now training epoch 4. LR=0.000016
INFO:local_logger:Epoch[004/800], Step[0000/0626], Avg Loss: 0.9778
INFO:local_logger:Epoch[004/800], Step[0000/0626], Avg Loss: 0.9751
INFO:local_logger:Epoch[004/800], Step[0000/0626], Avg Loss: 0.9713
INFO:master_logger:Epoch[004/800], Step[0000/0626], Avg Loss: 0.9734
INFO:local_logger:Epoch[004/800], Step[0000/0626], Avg Loss: 0.9753
INFO:local_logger:Epoch[004/800], Step[0000/0626], Avg Loss: 0.9753
INFO:local_logger:Epoch[004/800], Step[0000/0626], Avg Loss: 0.9704
INFO:local_logger:Epoch[004/800], Step[0000/0626], Avg Loss: 0.9683
INFO:local_logger:Epoch[004/800], Step[0000/0626], Avg Loss: 0.9740
INFO:local_logger:Epoch[004/800], Step[0100/0626], Avg Loss: 0.9727
INFO:local_logger:Epoch[004/800], Step[0100/0626], Avg Loss: 0.9724
INFO:local_logger:Epoch[004/800], Step[0100/0626], Avg Loss: 0.9730
INFO:master_logger:Epoch[004/800], Step[0100/0626], Avg Loss: 0.9728
INFO:local_logger:Epoch[004/800], Step[0100/0626], Avg Loss: 0.9731
INFO:local_logger:Epoch[004/800], Step[0100/0626], Avg Loss: 0.9730
INFO:local_logger:Epoch[004/800], Step[0100/0626], Avg Loss: 0.9729
INFO:local_logger:Epoch[004/800], Step[0100/0626], Avg Loss: 0.9730
INFO:local_logger:Epoch[004/800], Step[0100/0626], Avg Loss: 0.9726
INFO:local_logger:Epoch[004/800], Step[0200/0626], Avg Loss: 0.9724
INFO:local_logger:Epoch[004/800], Step[0200/0626], Avg Loss: 0.9725
INFO:local_logger:Epoch[004/800], Step[0200/0626], Avg Loss: 0.9721
INFO:local_logger:Epoch[004/800], Step[0200/0626], Avg Loss: 0.9721
INFO:local_logger:Epoch[004/800], Step[0200/0626], Avg Loss: 0.9721
INFO:local_logger:Epoch[004/800], Step[0200/0626], Avg Loss: 0.9720
INFO:local_logger:Epoch[004/800], Step[0200/0626], Avg Loss: 0.9722
INFO:local_logger:Epoch[004/800], Step[0200/0626], Avg Loss: 0.9724
INFO:master_logger:Epoch[004/800], Step[0200/0626], Avg Loss: 0.9722
INFO:local_logger:Epoch[004/800], Step[0300/0626], Avg Loss: 0.9715
INFO:local_logger:Epoch[004/800], Step[0300/0626], Avg Loss: 0.9717
INFO:local_logger:Epoch[004/800], Step[0300/0626], Avg Loss: 0.9717
INFO:local_logger:Epoch[004/800], Step[0300/0626], Avg Loss: 0.9720
INFO:master_logger:Epoch[004/800], Step[0300/0626], Avg Loss: 0.9717
INFO:local_logger:Epoch[004/800], Step[0300/0626], Avg Loss: 0.9712
INFO:local_logger:Epoch[004/800], Step[0300/0626], Avg Loss: 0.9718
INFO:local_logger:Epoch[004/800], Step[0300/0626], Avg Loss: 0.9718
INFO:local_logger:Epoch[004/800], Step[0300/0626], Avg Loss: 0.9716
INFO:local_logger:Epoch[004/800], Step[0400/0626], Avg Loss: 0.9712
INFO:local_logger:Epoch[004/800], Step[0400/0626], Avg Loss: 0.9711
INFO:local_logger:Epoch[004/800], Step[0400/0626], Avg Loss: 0.9711
INFO:local_logger:Epoch[004/800], Step[0400/0626], Avg Loss: 0.9715
INFO:local_logger:Epoch[004/800], Step[0400/0626], Avg Loss: 0.9712
INFO:local_logger:Epoch[004/800], Step[0400/0626], Avg Loss: 0.9709
INFO:master_logger:Epoch[004/800], Step[0400/0626], Avg Loss: 0.9712
INFO:local_logger:Epoch[004/800], Step[0400/0626], Avg Loss: 0.9714
INFO:local_logger:Epoch[004/800], Step[0400/0626], Avg Loss: 0.9714
INFO:local_logger:Epoch[004/800], Step[0500/0626], Avg Loss: 0.9707
INFO:local_logger:Epoch[004/800], Step[0500/0626], Avg Loss: 0.9706
INFO:local_logger:Epoch[004/800], Step[0500/0626], Avg Loss: 0.9709
INFO:local_logger:Epoch[004/800], Step[0500/0626], Avg Loss: 0.9709
INFO:local_logger:Epoch[004/800], Step[0500/0626], Avg Loss: 0.9707
INFO:local_logger:Epoch[004/800], Step[0500/0626], Avg Loss: 0.9708
INFO:local_logger:Epoch[004/800], Step[0500/0626], Avg Loss: 0.9705
INFO:master_logger:Epoch[004/800], Step[0500/0626], Avg Loss: 0.9707
INFO:local_logger:Epoch[004/800], Step[0500/0626], Avg Loss: 0.9706
INFO:local_logger:Epoch[004/800], Step[0600/0626], Avg Loss: 0.9701
INFO:local_logger:Epoch[004/800], Step[0600/0626], Avg Loss: 0.9704
INFO:local_logger:Epoch[004/800], Step[0600/0626], Avg Loss: 0.9703
INFO:local_logger:Epoch[004/800], Step[0600/0626], Avg Loss: 0.9701
INFO:local_logger:Epoch[004/800], Step[0600/0626], Avg Loss: 0.9703
INFO:local_logger:Epoch[004/800], Step[0600/0626], Avg Loss: 0.9701
INFO:local_logger:Epoch[004/800], Step[0600/0626], Avg Loss: 0.9704
INFO:master_logger:Epoch[004/800], Step[0600/0626], Avg Loss: 0.9703
INFO:local_logger:Epoch[004/800], Step[0600/0626], Avg Loss: 0.9704
INFO:local_logger:----- Epoch[004/800], Train Loss: 0.9702, time: 854.73
INFO:local_logger:Now training epoch 5. LR=0.000020
INFO:local_logger:----- Epoch[004/800], Train Loss: 0.9703, time: 851.06
INFO:master_logger:----- Epoch[004/800], Train Loss: 0.9702, time: 851.06
INFO:local_logger:----- Epoch[004/800], Train Loss: 0.9703, time: 854.82
INFO:local_logger:Now training epoch 5. LR=0.000020
INFO:local_logger:----- Epoch[004/800], Train Loss: 0.9700, time: 855.11
INFO:local_logger:Now training epoch 5. LR=0.000020
INFO:local_logger:----- Epoch[004/800], Train Loss: 0.9700, time: 855.36
INFO:local_logger:Now training epoch 5. LR=0.000020
INFO:local_logger:----- Epoch[004/800], Train Loss: 0.9703, time: 855.48
INFO:local_logger:----- Epoch[004/800], Train Loss: 0.9700, time: 855.31
INFO:local_logger:Now training epoch 5. LR=0.000020
INFO:local_logger:Now training epoch 5. LR=0.000020
INFO:local_logger:----- Epoch[004/800], Train Loss: 0.9702, time: 855.19
INFO:local_logger:Now training epoch 5. LR=0.000020
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-4-Loss-0.97028241060033.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-4-Loss-0.97028241060033.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-4-Loss-0.97028241060033.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-4-Loss-0.97028241060033.pdopt
INFO:local_logger:Now training epoch 5. LR=0.000020
INFO:master_logger:Now training epoch 5. LR=0.000020
INFO:local_logger:Epoch[005/800], Step[0000/0626], Avg Loss: 0.9655
INFO:local_logger:Epoch[005/800], Step[0000/0626], Avg Loss: 0.9667
INFO:local_logger:Epoch[005/800], Step[0000/0626], Avg Loss: 0.9651
INFO:master_logger:Epoch[005/800], Step[0000/0626], Avg Loss: 0.9667
INFO:local_logger:Epoch[005/800], Step[0000/0626], Avg Loss: 0.9671
INFO:local_logger:Epoch[005/800], Step[0000/0626], Avg Loss: 0.9619
INFO:local_logger:Epoch[005/800], Step[0000/0626], Avg Loss: 0.9712
INFO:local_logger:Epoch[005/800], Step[0000/0626], Avg Loss: 0.9685
INFO:local_logger:Epoch[005/800], Step[0000/0626], Avg Loss: 0.9674
INFO:local_logger:Epoch[005/800], Step[0100/0626], Avg Loss: 0.9675
INFO:local_logger:Epoch[005/800], Step[0100/0626], Avg Loss: 0.9674
INFO:local_logger:Epoch[005/800], Step[0100/0626], Avg Loss: 0.9672
INFO:local_logger:Epoch[005/800], Step[0100/0626], Avg Loss: 0.9682
INFO:local_logger:Epoch[005/800], Step[0100/0626], Avg Loss: 0.9673
INFO:local_logger:Epoch[005/800], Step[0100/0626], Avg Loss: 0.9671
INFO:local_logger:Epoch[005/800], Step[0100/0626], Avg Loss: 0.9679
INFO:master_logger:Epoch[005/800], Step[0100/0626], Avg Loss: 0.9675
INFO:local_logger:Epoch[005/800], Step[0100/0626], Avg Loss: 0.9672
INFO:local_logger:Epoch[005/800], Step[0200/0626], Avg Loss: 0.9670
INFO:local_logger:Epoch[005/800], Step[0200/0626], Avg Loss: 0.9665
INFO:local_logger:Epoch[005/800], Step[0200/0626], Avg Loss: 0.9669
INFO:local_logger:Epoch[005/800], Step[0200/0626], Avg Loss: 0.9669
INFO:local_logger:Epoch[005/800], Step[0200/0626], Avg Loss: 0.9666
INFO:local_logger:Epoch[005/800], Step[0200/0626], Avg Loss: 0.9673
INFO:local_logger:Epoch[005/800], Step[0200/0626], Avg Loss: 0.9672
INFO:local_logger:Epoch[005/800], Step[0200/0626], Avg Loss: 0.9671
INFO:master_logger:Epoch[005/800], Step[0200/0626], Avg Loss: 0.9669
INFO:local_logger:Epoch[005/800], Step[0300/0626], Avg Loss: 0.9661
INFO:local_logger:Epoch[005/800], Step[0300/0626], Avg Loss: 0.9663
INFO:local_logger:Epoch[005/800], Step[0300/0626], Avg Loss: 0.9665
INFO:local_logger:Epoch[005/800], Step[0300/0626], Avg Loss: 0.9665
INFO:local_logger:Epoch[005/800], Step[0300/0626], Avg Loss: 0.9664
INFO:local_logger:Epoch[005/800], Step[0300/0626], Avg Loss: 0.9667
INFO:master_logger:Epoch[005/800], Step[0300/0626], Avg Loss: 0.9665
INFO:local_logger:Epoch[005/800], Step[0300/0626], Avg Loss: 0.9668
INFO:local_logger:Epoch[005/800], Step[0300/0626], Avg Loss: 0.9665
INFO:local_logger:Epoch[005/800], Step[0400/0626], Avg Loss: 0.9661
INFO:master_logger:Epoch[005/800], Step[0400/0626], Avg Loss: 0.9660
INFO:local_logger:Epoch[005/800], Step[0400/0626], Avg Loss: 0.9662
INFO:local_logger:Epoch[005/800], Step[0400/0626], Avg Loss: 0.9660
INFO:local_logger:Epoch[005/800], Step[0400/0626], Avg Loss: 0.9661
INFO:local_logger:Epoch[005/800], Step[0400/0626], Avg Loss: 0.9658
INFO:local_logger:Epoch[005/800], Step[0400/0626], Avg Loss: 0.9660
INFO:local_logger:Epoch[005/800], Step[0400/0626], Avg Loss: 0.9658
INFO:local_logger:Epoch[005/800], Step[0400/0626], Avg Loss: 0.9660
INFO:local_logger:Epoch[005/800], Step[0500/0626], Avg Loss: 0.9655
INFO:local_logger:Epoch[005/800], Step[0500/0626], Avg Loss: 0.9655
INFO:local_logger:Epoch[005/800], Step[0500/0626], Avg Loss: 0.9657
INFO:local_logger:Epoch[005/800], Step[0500/0626], Avg Loss: 0.9657
INFO:local_logger:Epoch[005/800], Step[0500/0626], Avg Loss: 0.9656
INFO:local_logger:Epoch[005/800], Step[0500/0626], Avg Loss: 0.9656
INFO:local_logger:Epoch[005/800], Step[0500/0626], Avg Loss: 0.9657
INFO:master_logger:Epoch[005/800], Step[0500/0626], Avg Loss: 0.9656
INFO:local_logger:Epoch[005/800], Step[0500/0626], Avg Loss: 0.9654
INFO:local_logger:Epoch[005/800], Step[0600/0626], Avg Loss: 0.9651
INFO:local_logger:Epoch[005/800], Step[0600/0626], Avg Loss: 0.9653
INFO:local_logger:Epoch[005/800], Step[0600/0626], Avg Loss: 0.9653
INFO:local_logger:Epoch[005/800], Step[0600/0626], Avg Loss: 0.9654
INFO:local_logger:Epoch[005/800], Step[0600/0626], Avg Loss: 0.9652
INFO:local_logger:Epoch[005/800], Step[0600/0626], Avg Loss: 0.9652
INFO:local_logger:Epoch[005/800], Step[0600/0626], Avg Loss: 0.9649
INFO:master_logger:Epoch[005/800], Step[0600/0626], Avg Loss: 0.9652
INFO:local_logger:Epoch[005/800], Step[0600/0626], Avg Loss: 0.9651
INFO:local_logger:----- Epoch[005/800], Train Loss: 0.9648, time: 889.02
INFO:local_logger:Now training epoch 6. LR=0.000023
INFO:local_logger:----- Epoch[005/800], Train Loss: 0.9651, time: 889.10
INFO:local_logger:Now training epoch 6. LR=0.000023
INFO:local_logger:----- Epoch[005/800], Train Loss: 0.9652, time: 889.53
INFO:local_logger:Now training epoch 6. LR=0.000023
INFO:local_logger:----- Epoch[005/800], Train Loss: 0.9652, time: 885.85
INFO:master_logger:----- Epoch[005/800], Train Loss: 0.9651, time: 885.85
INFO:local_logger:----- Epoch[005/800], Train Loss: 0.9651, time: 889.20
INFO:local_logger:Now training epoch 6. LR=0.000023
INFO:local_logger:----- Epoch[005/800], Train Loss: 0.9650, time: 889.56
INFO:local_logger:Now training epoch 6. LR=0.000023
INFO:local_logger:----- Epoch[005/800], Train Loss: 0.9650, time: 889.67
INFO:local_logger:Now training epoch 6. LR=0.000023
INFO:local_logger:----- Epoch[005/800], Train Loss: 0.9653, time: 890.15
INFO:local_logger:Now training epoch 6. LR=0.000023
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-5-Loss-0.9652042168475674.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-5-Loss-0.9652042168475674.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-5-Loss-0.9652042168475674.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-5-Loss-0.9652042168475674.pdopt
INFO:local_logger:Now training epoch 6. LR=0.000023
INFO:master_logger:Now training epoch 6. LR=0.000023
INFO:local_logger:Epoch[006/800], Step[0000/0626], Avg Loss: 0.9660
INFO:local_logger:Epoch[006/800], Step[0000/0626], Avg Loss: 0.9643
INFO:master_logger:Epoch[006/800], Step[0000/0626], Avg Loss: 0.9604
INFO:local_logger:Epoch[006/800], Step[0000/0626], Avg Loss: 0.9476
INFO:local_logger:Epoch[006/800], Step[0000/0626], Avg Loss: 0.9637
INFO:local_logger:Epoch[006/800], Step[0000/0626], Avg Loss: 0.9585
INFO:local_logger:Epoch[006/800], Step[0000/0626], Avg Loss: 0.9542
INFO:local_logger:Epoch[006/800], Step[0000/0626], Avg Loss: 0.9641
INFO:local_logger:Epoch[006/800], Step[0000/0626], Avg Loss: 0.9651
INFO:local_logger:Epoch[006/800], Step[0100/0626], Avg Loss: 0.9622
INFO:local_logger:Epoch[006/800], Step[0100/0626], Avg Loss: 0.9624
INFO:local_logger:Epoch[006/800], Step[0100/0626], Avg Loss: 0.9627
INFO:local_logger:Epoch[006/800], Step[0100/0626], Avg Loss: 0.9625
INFO:local_logger:Epoch[006/800], Step[0100/0626], Avg Loss: 0.9626
INFO:local_logger:Epoch[006/800], Step[0100/0626], Avg Loss: 0.9626
INFO:master_logger:Epoch[006/800], Step[0100/0626], Avg Loss: 0.9626
INFO:local_logger:Epoch[006/800], Step[0100/0626], Avg Loss: 0.9627
INFO:local_logger:Epoch[006/800], Step[0100/0626], Avg Loss: 0.9632
INFO:local_logger:Epoch[006/800], Step[0200/0626], Avg Loss: 0.9624
INFO:local_logger:Epoch[006/800], Step[0200/0626], Avg Loss: 0.9619
INFO:local_logger:Epoch[006/800], Step[0200/0626], Avg Loss: 0.9619
INFO:local_logger:Epoch[006/800], Step[0200/0626], Avg Loss: 0.9624
INFO:master_logger:Epoch[006/800], Step[0200/0626], Avg Loss: 0.9622
INFO:local_logger:Epoch[006/800], Step[0200/0626], Avg Loss: 0.9620
INFO:local_logger:Epoch[006/800], Step[0200/0626], Avg Loss: 0.9622
INFO:local_logger:Epoch[006/800], Step[0200/0626], Avg Loss: 0.9624
INFO:local_logger:Epoch[006/800], Step[0200/0626], Avg Loss: 0.9620
INFO:local_logger:Epoch[006/800], Step[0300/0626], Avg Loss: 0.9613
INFO:local_logger:Epoch[006/800], Step[0300/0626], Avg Loss: 0.9620
INFO:local_logger:Epoch[006/800], Step[0300/0626], Avg Loss: 0.9620
INFO:local_logger:Epoch[006/800], Step[0300/0626], Avg Loss: 0.9620
INFO:local_logger:Epoch[006/800], Step[0300/0626], Avg Loss: 0.9615
INFO:local_logger:Epoch[006/800], Step[0300/0626], Avg Loss: 0.9618
INFO:local_logger:Epoch[006/800], Step[0300/0626], Avg Loss: 0.9621
INFO:master_logger:Epoch[006/800], Step[0300/0626], Avg Loss: 0.9618
INFO:local_logger:Epoch[006/800], Step[0300/0626], Avg Loss: 0.9617
INFO:local_logger:Epoch[006/800], Step[0400/0626], Avg Loss: 0.9610
INFO:local_logger:Epoch[006/800], Step[0400/0626], Avg Loss: 0.9615
INFO:local_logger:Epoch[006/800], Step[0400/0626], Avg Loss: 0.9612
INFO:local_logger:Epoch[006/800], Step[0400/0626], Avg Loss: 0.9614
INFO:local_logger:Epoch[006/800], Step[0400/0626], Avg Loss: 0.9614
INFO:local_logger:Epoch[006/800], Step[0400/0626], Avg Loss: 0.9612
INFO:local_logger:Epoch[006/800], Step[0400/0626], Avg Loss: 0.9616
INFO:master_logger:Epoch[006/800], Step[0400/0626], Avg Loss: 0.9613
INFO:local_logger:Epoch[006/800], Step[0400/0626], Avg Loss: 0.9614
INFO:local_logger:Epoch[006/800], Step[0500/0626], Avg Loss: 0.9609
INFO:local_logger:Epoch[006/800], Step[0500/0626], Avg Loss: 0.9609
INFO:local_logger:Epoch[006/800], Step[0500/0626], Avg Loss: 0.9612
INFO:local_logger:Epoch[006/800], Step[0500/0626], Avg Loss: 0.9608
INFO:local_logger:Epoch[006/800], Step[0500/0626], Avg Loss: 0.9611
INFO:local_logger:Epoch[006/800], Step[0500/0626], Avg Loss: 0.9608
INFO:local_logger:Epoch[006/800], Step[0500/0626], Avg Loss: 0.9610
INFO:master_logger:Epoch[006/800], Step[0500/0626], Avg Loss: 0.9610
INFO:local_logger:Epoch[006/800], Step[0500/0626], Avg Loss: 0.9612
INFO:local_logger:Epoch[006/800], Step[0600/0626], Avg Loss: 0.9604
INFO:local_logger:Epoch[006/800], Step[0600/0626], Avg Loss: 0.9608
INFO:local_logger:Epoch[006/800], Step[0600/0626], Avg Loss: 0.9606
INFO:local_logger:Epoch[006/800], Step[0600/0626], Avg Loss: 0.9607
INFO:local_logger:Epoch[006/800], Step[0600/0626], Avg Loss: 0.9605
INFO:local_logger:Epoch[006/800], Step[0600/0626], Avg Loss: 0.9606
INFO:local_logger:Epoch[006/800], Step[0600/0626], Avg Loss: 0.9603
INFO:local_logger:Epoch[006/800], Step[0600/0626], Avg Loss: 0.9609
INFO:master_logger:Epoch[006/800], Step[0600/0626], Avg Loss: 0.9606
INFO:local_logger:----- Epoch[006/800], Train Loss: 0.9605, time: 860.53
INFO:local_logger:Now training epoch 7. LR=0.000027
INFO:local_logger:----- Epoch[006/800], Train Loss: 0.9607, time: 860.72
INFO:local_logger:Now training epoch 7. LR=0.000027
INFO:local_logger:----- Epoch[006/800], Train Loss: 0.9604, time: 857.72
INFO:master_logger:----- Epoch[006/800], Train Loss: 0.9605, time: 857.72
INFO:local_logger:----- Epoch[006/800], Train Loss: 0.9607, time: 861.65
INFO:local_logger:Now training epoch 7. LR=0.000027
INFO:local_logger:----- Epoch[006/800], Train Loss: 0.9602, time: 861.47
INFO:local_logger:Now training epoch 7. LR=0.000027
INFO:local_logger:----- Epoch[006/800], Train Loss: 0.9603, time: 861.13
INFO:local_logger:----- Epoch[006/800], Train Loss: 0.9608, time: 861.53
INFO:local_logger:Now training epoch 7. LR=0.000027
INFO:local_logger:Now training epoch 7. LR=0.000027
INFO:local_logger:----- Epoch[006/800], Train Loss: 0.9603, time: 861.59
INFO:local_logger:Now training epoch 7. LR=0.000027
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-6-Loss-0.9604088297024008.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-6-Loss-0.9604088297024008.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-6-Loss-0.9604088297024008.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-6-Loss-0.9604088297024008.pdopt
INFO:local_logger:Now training epoch 7. LR=0.000027
INFO:master_logger:Now training epoch 7. LR=0.000027
INFO:local_logger:Epoch[007/800], Step[0000/0626], Avg Loss: 0.9534
INFO:local_logger:Epoch[007/800], Step[0000/0626], Avg Loss: 0.9591
INFO:local_logger:Epoch[007/800], Step[0000/0626], Avg Loss: 0.9552
INFO:master_logger:Epoch[007/800], Step[0000/0626], Avg Loss: 0.9581
INFO:local_logger:Epoch[007/800], Step[0000/0626], Avg Loss: 0.9540
INFO:local_logger:Epoch[007/800], Step[0000/0626], Avg Loss: 0.9572
INFO:local_logger:Epoch[007/800], Step[0000/0626], Avg Loss: 0.9591
INFO:local_logger:Epoch[007/800], Step[0000/0626], Avg Loss: 0.9645
INFO:local_logger:Epoch[007/800], Step[0000/0626], Avg Loss: 0.9624
INFO:local_logger:Epoch[007/800], Step[0100/0626], Avg Loss: 0.9583
INFO:local_logger:Epoch[007/800], Step[0100/0626], Avg Loss: 0.9576
INFO:local_logger:Epoch[007/800], Step[0100/0626], Avg Loss: 0.9586
INFO:local_logger:Epoch[007/800], Step[0100/0626], Avg Loss: 0.9575
INFO:local_logger:Epoch[007/800], Step[0100/0626], Avg Loss: 0.9582
INFO:local_logger:Epoch[007/800], Step[0100/0626], Avg Loss: 0.9584
INFO:local_logger:Epoch[007/800], Step[0100/0626], Avg Loss: 0.9589
INFO:master_logger:Epoch[007/800], Step[0100/0626], Avg Loss: 0.9582
INFO:local_logger:Epoch[007/800], Step[0100/0626], Avg Loss: 0.9584
INFO:local_logger:Epoch[007/800], Step[0200/0626], Avg Loss: 0.9580
INFO:local_logger:Epoch[007/800], Step[0200/0626], Avg Loss: 0.9575
INFO:local_logger:Epoch[007/800], Step[0200/0626], Avg Loss: 0.9573
INFO:master_logger:Epoch[007/800], Step[0200/0626], Avg Loss: 0.9578
INFO:local_logger:Epoch[007/800], Step[0200/0626], Avg Loss: 0.9580
INFO:local_logger:Epoch[007/800], Step[0200/0626], Avg Loss: 0.9581
INFO:local_logger:Epoch[007/800], Step[0200/0626], Avg Loss: 0.9578
INFO:local_logger:Epoch[007/800], Step[0200/0626], Avg Loss: 0.9578
INFO:local_logger:Epoch[007/800], Step[0200/0626], Avg Loss: 0.9577
INFO:local_logger:Epoch[007/800], Step[0300/0626], Avg Loss: 0.9571
INFO:local_logger:Epoch[007/800], Step[0300/0626], Avg Loss: 0.9570
INFO:local_logger:Epoch[007/800], Step[0300/0626], Avg Loss: 0.9575
INFO:local_logger:Epoch[007/800], Step[0300/0626], Avg Loss: 0.9573
INFO:local_logger:Epoch[007/800], Step[0300/0626], Avg Loss: 0.9570
INFO:local_logger:Epoch[007/800], Step[0300/0626], Avg Loss: 0.9574
INFO:master_logger:Epoch[007/800], Step[0300/0626], Avg Loss: 0.9573
INFO:local_logger:Epoch[007/800], Step[0300/0626], Avg Loss: 0.9574
INFO:local_logger:Epoch[007/800], Step[0300/0626], Avg Loss: 0.9577
INFO:local_logger:Epoch[007/800], Step[0400/0626], Avg Loss: 0.9566
INFO:local_logger:Epoch[007/800], Step[0400/0626], Avg Loss: 0.9566
INFO:local_logger:Epoch[007/800], Step[0400/0626], Avg Loss: 0.9567
INFO:local_logger:Epoch[007/800], Step[0400/0626], Avg Loss: 0.9572
INFO:local_logger:Epoch[007/800], Step[0400/0626], Avg Loss: 0.9568
INFO:master_logger:Epoch[007/800], Step[0400/0626], Avg Loss: 0.9568
INFO:local_logger:Epoch[007/800], Step[0400/0626], Avg Loss: 0.9568
INFO:local_logger:Epoch[007/800], Step[0400/0626], Avg Loss: 0.9570
INFO:local_logger:Epoch[007/800], Step[0400/0626], Avg Loss: 0.9568
INFO:local_logger:Epoch[007/800], Step[0500/0626], Avg Loss: 0.9563
INFO:local_logger:Epoch[007/800], Step[0500/0626], Avg Loss: 0.9568
INFO:local_logger:Epoch[007/800], Step[0500/0626], Avg Loss: 0.9561
INFO:local_logger:Epoch[007/800], Step[0500/0626], Avg Loss: 0.9565
INFO:local_logger:Epoch[007/800], Step[0500/0626], Avg Loss: 0.9565
INFO:local_logger:Epoch[007/800], Step[0500/0626], Avg Loss: 0.9563
INFO:master_logger:Epoch[007/800], Step[0500/0626], Avg Loss: 0.9564
INFO:local_logger:Epoch[007/800], Step[0500/0626], Avg Loss: 0.9565
INFO:local_logger:Epoch[007/800], Step[0500/0626], Avg Loss: 0.9563
INFO:local_logger:Epoch[007/800], Step[0600/0626], Avg Loss: 0.9564
INFO:local_logger:Epoch[007/800], Step[0600/0626], Avg Loss: 0.9561
INFO:local_logger:Epoch[007/800], Step[0600/0626], Avg Loss: 0.9561
INFO:local_logger:Epoch[007/800], Step[0600/0626], Avg Loss: 0.9559
INFO:local_logger:Epoch[007/800], Step[0600/0626], Avg Loss: 0.9559
INFO:local_logger:Epoch[007/800], Step[0600/0626], Avg Loss: 0.9558
INFO:local_logger:Epoch[007/800], Step[0600/0626], Avg Loss: 0.9558
INFO:master_logger:Epoch[007/800], Step[0600/0626], Avg Loss: 0.9560
INFO:local_logger:Epoch[007/800], Step[0600/0626], Avg Loss: 0.9561
INFO:local_logger:----- Epoch[007/800], Train Loss: 0.9560, time: 889.20
INFO:local_logger:Now training epoch 8. LR=0.000031
INFO:local_logger:----- Epoch[007/800], Train Loss: 0.9558, time: 888.65
INFO:local_logger:Now training epoch 8. LR=0.000031
INFO:local_logger:----- Epoch[007/800], Train Loss: 0.9557, time: 889.07
INFO:local_logger:Now training epoch 8. LR=0.000031
INFO:local_logger:----- Epoch[007/800], Train Loss: 0.9563, time: 888.69
INFO:local_logger:Now training epoch 8. LR=0.000031
INFO:local_logger:----- Epoch[007/800], Train Loss: 0.9559, time: 888.70
INFO:local_logger:Now training epoch 8. LR=0.000031
INFO:local_logger:----- Epoch[007/800], Train Loss: 0.9558, time: 888.74
INFO:local_logger:Now training epoch 8. LR=0.000031
INFO:local_logger:----- Epoch[007/800], Train Loss: 0.9557, time: 885.04
INFO:local_logger:----- Epoch[007/800], Train Loss: 0.9560, time: 888.76
INFO:master_logger:----- Epoch[007/800], Train Loss: 0.9559, time: 885.04
INFO:local_logger:Now training epoch 8. LR=0.000031
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-7-Loss-0.9557424400537671.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-7-Loss-0.9557424400537671.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-7-Loss-0.9557424400537671.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-7-Loss-0.9557424400537671.pdopt
INFO:local_logger:Now training epoch 8. LR=0.000031
INFO:master_logger:Now training epoch 8. LR=0.000031
INFO:local_logger:Epoch[008/800], Step[0000/0626], Avg Loss: 0.9562
INFO:master_logger:Epoch[008/800], Step[0000/0626], Avg Loss: 0.9506
INFO:local_logger:Epoch[008/800], Step[0000/0626], Avg Loss: 0.9529
INFO:local_logger:Epoch[008/800], Step[0000/0626], Avg Loss: 0.9443
INFO:local_logger:Epoch[008/800], Step[0000/0626], Avg Loss: 0.9491
INFO:local_logger:Epoch[008/800], Step[0000/0626], Avg Loss: 0.9499
INFO:local_logger:Epoch[008/800], Step[0000/0626], Avg Loss: 0.9539
INFO:local_logger:Epoch[008/800], Step[0000/0626], Avg Loss: 0.9524
INFO:local_logger:Epoch[008/800], Step[0000/0626], Avg Loss: 0.9463
INFO:local_logger:Epoch[008/800], Step[0100/0626], Avg Loss: 0.9530
INFO:local_logger:Epoch[008/800], Step[0100/0626], Avg Loss: 0.9530
INFO:local_logger:Epoch[008/800], Step[0100/0626], Avg Loss: 0.9531
INFO:local_logger:Epoch[008/800], Step[0100/0626], Avg Loss: 0.9531
INFO:master_logger:Epoch[008/800], Step[0100/0626], Avg Loss: 0.9532
INFO:local_logger:Epoch[008/800], Step[0100/0626], Avg Loss: 0.9528
INFO:local_logger:Epoch[008/800], Step[0100/0626], Avg Loss: 0.9532
INFO:local_logger:Epoch[008/800], Step[0100/0626], Avg Loss: 0.9535
INFO:local_logger:Epoch[008/800], Step[0100/0626], Avg Loss: 0.9540
INFO:local_logger:Epoch[008/800], Step[0200/0626], Avg Loss: 0.9527
INFO:local_logger:Epoch[008/800], Step[0200/0626], Avg Loss: 0.9531
INFO:local_logger:Epoch[008/800], Step[0200/0626], Avg Loss: 0.9526
INFO:local_logger:Epoch[008/800], Step[0200/0626], Avg Loss: 0.9526
INFO:local_logger:Epoch[008/800], Step[0200/0626], Avg Loss: 0.9525
INFO:master_logger:Epoch[008/800], Step[0200/0626], Avg Loss: 0.9527
INFO:local_logger:Epoch[008/800], Step[0200/0626], Avg Loss: 0.9528
INFO:local_logger:Epoch[008/800], Step[0200/0626], Avg Loss: 0.9529
INFO:local_logger:Epoch[008/800], Step[0200/0626], Avg Loss: 0.9524
INFO:local_logger:Epoch[008/800], Step[0300/0626], Avg Loss: 0.9524
INFO:local_logger:Epoch[008/800], Step[0300/0626], Avg Loss: 0.9520
INFO:local_logger:Epoch[008/800], Step[0300/0626], Avg Loss: 0.9521
INFO:master_logger:Epoch[008/800], Step[0300/0626], Avg Loss: 0.9523
INFO:local_logger:Epoch[008/800], Step[0300/0626], Avg Loss: 0.9524
INFO:local_logger:Epoch[008/800], Step[0300/0626], Avg Loss: 0.9526
INFO:local_logger:Epoch[008/800], Step[0300/0626], Avg Loss: 0.9520
INFO:local_logger:Epoch[008/800], Step[0300/0626], Avg Loss: 0.9521
INFO:local_logger:Epoch[008/800], Step[0300/0626], Avg Loss: 0.9525
INFO:local_logger:Epoch[008/800], Step[0400/0626], Avg Loss: 0.9517
INFO:local_logger:Epoch[008/800], Step[0400/0626], Avg Loss: 0.9518
INFO:local_logger:Epoch[008/800], Step[0400/0626], Avg Loss: 0.9516
INFO:local_logger:Epoch[008/800], Step[0400/0626], Avg Loss: 0.9519
INFO:local_logger:Epoch[008/800], Step[0400/0626], Avg Loss: 0.9516
INFO:local_logger:Epoch[008/800], Step[0400/0626], Avg Loss: 0.9515
INFO:local_logger:Epoch[008/800], Step[0400/0626], Avg Loss: 0.9518
INFO:master_logger:Epoch[008/800], Step[0400/0626], Avg Loss: 0.9517
INFO:local_logger:Epoch[008/800], Step[0400/0626], Avg Loss: 0.9518
INFO:local_logger:Epoch[008/800], Step[0500/0626], Avg Loss: 0.9511
INFO:local_logger:Epoch[008/800], Step[0500/0626], Avg Loss: 0.9511
INFO:local_logger:Epoch[008/800], Step[0500/0626], Avg Loss: 0.9513
INFO:local_logger:Epoch[008/800], Step[0500/0626], Avg Loss: 0.9511
INFO:master_logger:Epoch[008/800], Step[0500/0626], Avg Loss: 0.9512
INFO:local_logger:Epoch[008/800], Step[0500/0626], Avg Loss: 0.9512
INFO:local_logger:Epoch[008/800], Step[0500/0626], Avg Loss: 0.9513
INFO:local_logger:Epoch[008/800], Step[0500/0626], Avg Loss: 0.9511
INFO:local_logger:Epoch[008/800], Step[0500/0626], Avg Loss: 0.9512
INFO:local_logger:Epoch[008/800], Step[0600/0626], Avg Loss: 0.9506
INFO:local_logger:Epoch[008/800], Step[0600/0626], Avg Loss: 0.9506
INFO:local_logger:Epoch[008/800], Step[0600/0626], Avg Loss: 0.9506
INFO:local_logger:Epoch[008/800], Step[0600/0626], Avg Loss: 0.9508
INFO:local_logger:Epoch[008/800], Step[0600/0626], Avg Loss: 0.9505
INFO:local_logger:Epoch[008/800], Step[0600/0626], Avg Loss: 0.9506
INFO:local_logger:Epoch[008/800], Step[0600/0626], Avg Loss: 0.9509
INFO:master_logger:Epoch[008/800], Step[0600/0626], Avg Loss: 0.9506
INFO:local_logger:Epoch[008/800], Step[0600/0626], Avg Loss: 0.9506
INFO:local_logger:----- Epoch[008/800], Train Loss: 0.9505, time: 854.97
INFO:local_logger:Now training epoch 9. LR=0.000035
INFO:local_logger:----- Epoch[008/800], Train Loss: 0.9507, time: 855.87
INFO:local_logger:Now training epoch 9. LR=0.000035
INFO:local_logger:----- Epoch[008/800], Train Loss: 0.9506, time: 855.95
INFO:local_logger:Now training epoch 9. LR=0.000035
INFO:local_logger:----- Epoch[008/800], Train Loss: 0.9504, time: 855.93
INFO:local_logger:Now training epoch 9. LR=0.000035
INFO:local_logger:----- Epoch[008/800], Train Loss: 0.9504, time: 852.20
INFO:master_logger:----- Epoch[008/800], Train Loss: 0.9505, time: 852.20
INFO:local_logger:----- Epoch[008/800], Train Loss: 0.9504, time: 855.94
INFO:local_logger:----- Epoch[008/800], Train Loss: 0.9504, time: 855.86
INFO:local_logger:Now training epoch 9. LR=0.000035
INFO:local_logger:Now training epoch 9. LR=0.000035
INFO:local_logger:----- Epoch[008/800], Train Loss: 0.9506, time: 855.86
INFO:local_logger:Now training epoch 9. LR=0.000035
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-8-Loss-0.950418085337367.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-8-Loss-0.950418085337367.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-8-Loss-0.950418085337367.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-8-Loss-0.950418085337367.pdopt
INFO:local_logger:Now training epoch 9. LR=0.000035
INFO:master_logger:Now training epoch 9. LR=0.000035
INFO:local_logger:Epoch[009/800], Step[0000/0626], Avg Loss: 0.9532
INFO:master_logger:Epoch[009/800], Step[0000/0626], Avg Loss: 0.9494
INFO:local_logger:Epoch[009/800], Step[0000/0626], Avg Loss: 0.9472
INFO:local_logger:Epoch[009/800], Step[0000/0626], Avg Loss: 0.9535
INFO:local_logger:Epoch[009/800], Step[0000/0626], Avg Loss: 0.9457
INFO:local_logger:Epoch[009/800], Step[0000/0626], Avg Loss: 0.9521
INFO:local_logger:Epoch[009/800], Step[0000/0626], Avg Loss: 0.9484
INFO:local_logger:Epoch[009/800], Step[0000/0626], Avg Loss: 0.9460
INFO:local_logger:Epoch[009/800], Step[0000/0626], Avg Loss: 0.9495
INFO:local_logger:Epoch[009/800], Step[0100/0626], Avg Loss: 0.9469
INFO:local_logger:Epoch[009/800], Step[0100/0626], Avg Loss: 0.9469
INFO:local_logger:Epoch[009/800], Step[0100/0626], Avg Loss: 0.9473
INFO:local_logger:Epoch[009/800], Step[0100/0626], Avg Loss: 0.9469
INFO:local_logger:Epoch[009/800], Step[0100/0626], Avg Loss: 0.9459
INFO:master_logger:Epoch[009/800], Step[0100/0626], Avg Loss: 0.9466
INFO:local_logger:Epoch[009/800], Step[0100/0626], Avg Loss: 0.9459
INFO:local_logger:Epoch[009/800], Step[0100/0626], Avg Loss: 0.9465
INFO:local_logger:Epoch[009/800], Step[0100/0626], Avg Loss: 0.9465
INFO:local_logger:Epoch[009/800], Step[0200/0626], Avg Loss: 0.9466
INFO:local_logger:Epoch[009/800], Step[0200/0626], Avg Loss: 0.9460
INFO:local_logger:Epoch[009/800], Step[0200/0626], Avg Loss: 0.9461
INFO:local_logger:Epoch[009/800], Step[0200/0626], Avg Loss: 0.9462
INFO:local_logger:Epoch[009/800], Step[0200/0626], Avg Loss: 0.9455
INFO:local_logger:Epoch[009/800], Step[0200/0626], Avg Loss: 0.9466
INFO:local_logger:Epoch[009/800], Step[0200/0626], Avg Loss: 0.9460
INFO:local_logger:Epoch[009/800], Step[0200/0626], Avg Loss: 0.9465
INFO:master_logger:Epoch[009/800], Step[0200/0626], Avg Loss: 0.9462
INFO:local_logger:Epoch[009/800], Step[0300/0626], Avg Loss: 0.9455
INFO:local_logger:Epoch[009/800], Step[0300/0626], Avg Loss: 0.9455
INFO:local_logger:Epoch[009/800], Step[0300/0626], Avg Loss: 0.9450
INFO:local_logger:Epoch[009/800], Step[0300/0626], Avg Loss: 0.9451
INFO:local_logger:Epoch[009/800], Step[0300/0626], Avg Loss: 0.9449
INFO:local_logger:Epoch[009/800], Step[0300/0626], Avg Loss: 0.9455
INFO:local_logger:Epoch[009/800], Step[0300/0626], Avg Loss: 0.9452
INFO:local_logger:Epoch[009/800], Step[0300/0626], Avg Loss: 0.9448
INFO:master_logger:Epoch[009/800], Step[0300/0626], Avg Loss: 0.9452
INFO:local_logger:Epoch[009/800], Step[0400/0626], Avg Loss: 0.9441
INFO:local_logger:Epoch[009/800], Step[0400/0626], Avg Loss: 0.9447
INFO:local_logger:Epoch[009/800], Step[0400/0626], Avg Loss: 0.9441
INFO:local_logger:Epoch[009/800], Step[0400/0626], Avg Loss: 0.9444
INFO:local_logger:Epoch[009/800], Step[0400/0626], Avg Loss: 0.9444
INFO:master_logger:Epoch[009/800], Step[0400/0626], Avg Loss: 0.9444
INFO:local_logger:Epoch[009/800], Step[0400/0626], Avg Loss: 0.9445
INFO:local_logger:Epoch[009/800], Step[0400/0626], Avg Loss: 0.9446
INFO:local_logger:Epoch[009/800], Step[0400/0626], Avg Loss: 0.9442
INFO:local_logger:Epoch[009/800], Step[0500/0626], Avg Loss: 0.9437
INFO:local_logger:Epoch[009/800], Step[0500/0626], Avg Loss: 0.9432
INFO:local_logger:Epoch[009/800], Step[0500/0626], Avg Loss: 0.9436
INFO:local_logger:Epoch[009/800], Step[0500/0626], Avg Loss: 0.9435
INFO:local_logger:Epoch[009/800], Step[0500/0626], Avg Loss: 0.9434
INFO:local_logger:Epoch[009/800], Step[0500/0626], Avg Loss: 0.9434
INFO:local_logger:Epoch[009/800], Step[0500/0626], Avg Loss: 0.9434
INFO:master_logger:Epoch[009/800], Step[0500/0626], Avg Loss: 0.9435
INFO:local_logger:Epoch[009/800], Step[0500/0626], Avg Loss: 0.9437
INFO:local_logger:Epoch[009/800], Step[0600/0626], Avg Loss: 0.9426
INFO:local_logger:Epoch[009/800], Step[0600/0626], Avg Loss: 0.9427
INFO:local_logger:Epoch[009/800], Step[0600/0626], Avg Loss: 0.9428
INFO:local_logger:Epoch[009/800], Step[0600/0626], Avg Loss: 0.9423
INFO:local_logger:Epoch[009/800], Step[0600/0626], Avg Loss: 0.9427
INFO:local_logger:Epoch[009/800], Step[0600/0626], Avg Loss: 0.9421
INFO:local_logger:Epoch[009/800], Step[0600/0626], Avg Loss: 0.9426
INFO:local_logger:Epoch[009/800], Step[0600/0626], Avg Loss: 0.9427
INFO:master_logger:Epoch[009/800], Step[0600/0626], Avg Loss: 0.9426
INFO:local_logger:----- Epoch[009/800], Train Loss: 0.9425, time: 891.29
INFO:local_logger:Now training epoch 10. LR=0.000038
INFO:local_logger:----- Epoch[009/800], Train Loss: 0.9425, time: 886.67
INFO:master_logger:----- Epoch[009/800], Train Loss: 0.9424, time: 886.67
INFO:local_logger:----- Epoch[009/800], Train Loss: 0.9420, time: 891.03
INFO:local_logger:Now training epoch 10. LR=0.000038
INFO:local_logger:----- Epoch[009/800], Train Loss: 0.9425, time: 891.03
INFO:local_logger:Now training epoch 10. LR=0.000038
INFO:local_logger:----- Epoch[009/800], Train Loss: 0.9421, time: 891.05
INFO:local_logger:Now training epoch 10. LR=0.000038
INFO:local_logger:----- Epoch[009/800], Train Loss: 0.9424, time: 891.03
INFO:local_logger:Now training epoch 10. LR=0.000038
INFO:local_logger:----- Epoch[009/800], Train Loss: 0.9426, time: 891.05
INFO:local_logger:Now training epoch 10. LR=0.000038
INFO:local_logger:----- Epoch[009/800], Train Loss: 0.9425, time: 891.04
INFO:local_logger:Now training epoch 10. LR=0.000038
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-9-Loss-0.9425096387053156.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-9-Loss-0.9425096387053156.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-9-Loss-0.9425096387053156.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-9-Loss-0.9425096387053156.pdopt
INFO:local_logger:Now training epoch 10. LR=0.000038
INFO:master_logger:Now training epoch 10. LR=0.000038
INFO:local_logger:Epoch[010/800], Step[0000/0626], Avg Loss: 0.9389
INFO:local_logger:Epoch[010/800], Step[0000/0626], Avg Loss: 0.9403
INFO:master_logger:Epoch[010/800], Step[0000/0626], Avg Loss: 0.9385
INFO:local_logger:Epoch[010/800], Step[0000/0626], Avg Loss: 0.9304
INFO:local_logger:Epoch[010/800], Step[0000/0626], Avg Loss: 0.9343
INFO:local_logger:Epoch[010/800], Step[0000/0626], Avg Loss: 0.9450
INFO:local_logger:Epoch[010/800], Step[0000/0626], Avg Loss: 0.9331
INFO:local_logger:Epoch[010/800], Step[0000/0626], Avg Loss: 0.9421
INFO:local_logger:Epoch[010/800], Step[0000/0626], Avg Loss: 0.9438
INFO:local_logger:Epoch[010/800], Step[0100/0626], Avg Loss: 0.9362
INFO:local_logger:Epoch[010/800], Step[0100/0626], Avg Loss: 0.9361
INFO:local_logger:Epoch[010/800], Step[0100/0626], Avg Loss: 0.9356
INFO:local_logger:Epoch[010/800], Step[0100/0626], Avg Loss: 0.9360
INFO:master_logger:Epoch[010/800], Step[0100/0626], Avg Loss: 0.9362
INFO:local_logger:Epoch[010/800], Step[0100/0626], Avg Loss: 0.9362
INFO:local_logger:Epoch[010/800], Step[0100/0626], Avg Loss: 0.9361
INFO:local_logger:Epoch[010/800], Step[0100/0626], Avg Loss: 0.9361
INFO:local_logger:Epoch[010/800], Step[0100/0626], Avg Loss: 0.9371
INFO:local_logger:Epoch[010/800], Step[0200/0626], Avg Loss: 0.9354
INFO:local_logger:Epoch[010/800], Step[0200/0626], Avg Loss: 0.9358
INFO:local_logger:Epoch[010/800], Step[0200/0626], Avg Loss: 0.9355
INFO:local_logger:Epoch[010/800], Step[0200/0626], Avg Loss: 0.9355
INFO:master_logger:Epoch[010/800], Step[0200/0626], Avg Loss: 0.9357
INFO:local_logger:Epoch[010/800], Step[0200/0626], Avg Loss: 0.9360
INFO:local_logger:Epoch[010/800], Step[0200/0626], Avg Loss: 0.9355
INFO:local_logger:Epoch[010/800], Step[0200/0626], Avg Loss: 0.9359
INFO:local_logger:Epoch[010/800], Step[0200/0626], Avg Loss: 0.9358
INFO:local_logger:Epoch[010/800], Step[0300/0626], Avg Loss: 0.9345
INFO:local_logger:Epoch[010/800], Step[0300/0626], Avg Loss: 0.9350
INFO:local_logger:Epoch[010/800], Step[0300/0626], Avg Loss: 0.9346
INFO:local_logger:Epoch[010/800], Step[0300/0626], Avg Loss: 0.9345
INFO:local_logger:Epoch[010/800], Step[0300/0626], Avg Loss: 0.9348
INFO:local_logger:Epoch[010/800], Step[0300/0626], Avg Loss: 0.9348
INFO:master_logger:Epoch[010/800], Step[0300/0626], Avg Loss: 0.9348
INFO:local_logger:Epoch[010/800], Step[0300/0626], Avg Loss: 0.9348
INFO:local_logger:Epoch[010/800], Step[0300/0626], Avg Loss: 0.9350
INFO:local_logger:Epoch[010/800], Step[0400/0626], Avg Loss: 0.9337
INFO:local_logger:Epoch[010/800], Step[0400/0626], Avg Loss: 0.9337
INFO:local_logger:Epoch[010/800], Step[0400/0626], Avg Loss: 0.9336
INFO:local_logger:Epoch[010/800], Step[0400/0626], Avg Loss: 0.9339
INFO:local_logger:Epoch[010/800], Step[0400/0626], Avg Loss: 0.9340
INFO:local_logger:Epoch[010/800], Step[0400/0626], Avg Loss: 0.9340
INFO:local_logger:Epoch[010/800], Step[0400/0626], Avg Loss: 0.9340
INFO:master_logger:Epoch[010/800], Step[0400/0626], Avg Loss: 0.9338
INFO:local_logger:Epoch[010/800], Step[0400/0626], Avg Loss: 0.9337
INFO:local_logger:Epoch[010/800], Step[0500/0626], Avg Loss: 0.9330
INFO:local_logger:Epoch[010/800], Step[0500/0626], Avg Loss: 0.9327
INFO:local_logger:Epoch[010/800], Step[0500/0626], Avg Loss: 0.9328
INFO:local_logger:Epoch[010/800], Step[0500/0626], Avg Loss: 0.9328
INFO:local_logger:Epoch[010/800], Step[0500/0626], Avg Loss: 0.9330
INFO:master_logger:Epoch[010/800], Step[0500/0626], Avg Loss: 0.9329
INFO:local_logger:Epoch[010/800], Step[0500/0626], Avg Loss: 0.9326
INFO:local_logger:Epoch[010/800], Step[0500/0626], Avg Loss: 0.9329
INFO:local_logger:Epoch[010/800], Step[0500/0626], Avg Loss: 0.9330
INFO:local_logger:Epoch[010/800], Step[0600/0626], Avg Loss: 0.9320
INFO:local_logger:Epoch[010/800], Step[0600/0626], Avg Loss: 0.9320
INFO:local_logger:Epoch[010/800], Step[0600/0626], Avg Loss: 0.9323
INFO:local_logger:Epoch[010/800], Step[0600/0626], Avg Loss: 0.9321
INFO:local_logger:Epoch[010/800], Step[0600/0626], Avg Loss: 0.9322
INFO:local_logger:Epoch[010/800], Step[0600/0626], Avg Loss: 0.9321
INFO:local_logger:Epoch[010/800], Step[0600/0626], Avg Loss: 0.9320
INFO:local_logger:Epoch[010/800], Step[0600/0626], Avg Loss: 0.9321
INFO:master_logger:Epoch[010/800], Step[0600/0626], Avg Loss: 0.9321
INFO:local_logger:----- Epoch[010/800], Train Loss: 0.9317, time: 857.40
INFO:local_logger:Now training epoch 11. LR=0.000042
INFO:local_logger:----- Epoch[010/800], Train Loss: 0.9320, time: 857.41
INFO:local_logger:Now training epoch 11. LR=0.000042
INFO:local_logger:----- Epoch[010/800], Train Loss: 0.9318, time: 854.67
INFO:master_logger:----- Epoch[010/800], Train Loss: 0.9318, time: 854.67
INFO:local_logger:----- Epoch[010/800], Train Loss: 0.9318, time: 858.49
INFO:local_logger:Now training epoch 11. LR=0.000042
INFO:local_logger:----- Epoch[010/800], Train Loss: 0.9319, time: 857.80
INFO:local_logger:Now training epoch 11. LR=0.000042
INFO:local_logger:----- Epoch[010/800], Train Loss: 0.9319, time: 857.83
INFO:local_logger:Now training epoch 11. LR=0.000042
INFO:local_logger:----- Epoch[010/800], Train Loss: 0.9319, time: 857.81
INFO:local_logger:Now training epoch 11. LR=0.000042
INFO:local_logger:----- Epoch[010/800], Train Loss: 0.9318, time: 857.82
INFO:local_logger:Now training epoch 11. LR=0.000042
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-10-Loss-0.9318290638491608.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-10-Loss-0.9318290638491608.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-10-Loss-0.9318290638491608.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-10-Loss-0.9318290638491608.pdopt
INFO:local_logger:Now training epoch 11. LR=0.000042
INFO:master_logger:Now training epoch 11. LR=0.000042
INFO:local_logger:Epoch[011/800], Step[0000/0626], Avg Loss: 0.9246
INFO:local_logger:Epoch[011/800], Step[0000/0626], Avg Loss: 0.9293
INFO:master_logger:Epoch[011/800], Step[0000/0626], Avg Loss: 0.9253
INFO:local_logger:Epoch[011/800], Step[0000/0626], Avg Loss: 0.9166
INFO:local_logger:Epoch[011/800], Step[0000/0626], Avg Loss: 0.9227
INFO:local_logger:Epoch[011/800], Step[0000/0626], Avg Loss: 0.9325
INFO:local_logger:Epoch[011/800], Step[0000/0626], Avg Loss: 0.9194
INFO:local_logger:Epoch[011/800], Step[0000/0626], Avg Loss: 0.9280
INFO:local_logger:Epoch[011/800], Step[0000/0626], Avg Loss: 0.9296
INFO:local_logger:Epoch[011/800], Step[0100/0626], Avg Loss: 0.9257
INFO:local_logger:Epoch[011/800], Step[0100/0626], Avg Loss: 0.9242
INFO:local_logger:Epoch[011/800], Step[0100/0626], Avg Loss: 0.9247
INFO:local_logger:Epoch[011/800], Step[0100/0626], Avg Loss: 0.9260
INFO:local_logger:Epoch[011/800], Step[0100/0626], Avg Loss: 0.9256
INFO:master_logger:Epoch[011/800], Step[0100/0626], Avg Loss: 0.9254
INFO:local_logger:Epoch[011/800], Step[0100/0626], Avg Loss: 0.9255
INFO:local_logger:Epoch[011/800], Step[0100/0626], Avg Loss: 0.9251
INFO:local_logger:Epoch[011/800], Step[0100/0626], Avg Loss: 0.9261
INFO:local_logger:Epoch[011/800], Step[0200/0626], Avg Loss: 0.9257
INFO:local_logger:Epoch[011/800], Step[0200/0626], Avg Loss: 0.9247
INFO:local_logger:Epoch[011/800], Step[0200/0626], Avg Loss: 0.9255
INFO:local_logger:Epoch[011/800], Step[0200/0626], Avg Loss: 0.9252
INFO:local_logger:Epoch[011/800], Step[0200/0626], Avg Loss: 0.9255
INFO:master_logger:Epoch[011/800], Step[0200/0626], Avg Loss: 0.9252
INFO:local_logger:Epoch[011/800], Step[0200/0626], Avg Loss: 0.9245
INFO:local_logger:Epoch[011/800], Step[0200/0626], Avg Loss: 0.9253
INFO:local_logger:Epoch[011/800], Step[0200/0626], Avg Loss: 0.9256
INFO:local_logger:Epoch[011/800], Step[0300/0626], Avg Loss: 0.9241
INFO:local_logger:Epoch[011/800], Step[0300/0626], Avg Loss: 0.9237
INFO:local_logger:Epoch[011/800], Step[0300/0626], Avg Loss: 0.9244
INFO:local_logger:Epoch[011/800], Step[0300/0626], Avg Loss: 0.9244
INFO:local_logger:Epoch[011/800], Step[0300/0626], Avg Loss: 0.9246
INFO:local_logger:Epoch[011/800], Step[0300/0626], Avg Loss: 0.9238
INFO:local_logger:Epoch[011/800], Step[0300/0626], Avg Loss: 0.9242
INFO:local_logger:Epoch[011/800], Step[0300/0626], Avg Loss: 0.9241
INFO:master_logger:Epoch[011/800], Step[0300/0626], Avg Loss: 0.9242
INFO:local_logger:Epoch[011/800], Step[0400/0626], Avg Loss: 0.9234
INFO:local_logger:Epoch[011/800], Step[0400/0626], Avg Loss: 0.9229
INFO:local_logger:Epoch[011/800], Step[0400/0626], Avg Loss: 0.9234
INFO:local_logger:Epoch[011/800], Step[0400/0626], Avg Loss: 0.9229
INFO:local_logger:Epoch[011/800], Step[0400/0626], Avg Loss: 0.9234
INFO:local_logger:Epoch[011/800], Step[0400/0626], Avg Loss: 0.9233
INFO:local_logger:Epoch[011/800], Step[0400/0626], Avg Loss: 0.9233
INFO:master_logger:Epoch[011/800], Step[0400/0626], Avg Loss: 0.9232
INFO:local_logger:Epoch[011/800], Step[0400/0626], Avg Loss: 0.9235
INFO:local_logger:Epoch[011/800], Step[0500/0626], Avg Loss: 0.9224
INFO:local_logger:Epoch[011/800], Step[0500/0626], Avg Loss: 0.9222
INFO:local_logger:Epoch[011/800], Step[0500/0626], Avg Loss: 0.9225
INFO:local_logger:Epoch[011/800], Step[0500/0626], Avg Loss: 0.9217
INFO:local_logger:Epoch[011/800], Step[0500/0626], Avg Loss: 0.9223
INFO:local_logger:Epoch[011/800], Step[0500/0626], Avg Loss: 0.9222
INFO:local_logger:Epoch[011/800], Step[0500/0626], Avg Loss: 0.9219
INFO:master_logger:Epoch[011/800], Step[0500/0626], Avg Loss: 0.9222
INFO:local_logger:Epoch[011/800], Step[0500/0626], Avg Loss: 0.9223
INFO:local_logger:Epoch[011/800], Step[0600/0626], Avg Loss: 0.9214
INFO:local_logger:Epoch[011/800], Step[0600/0626], Avg Loss: 0.9207
INFO:local_logger:Epoch[011/800], Step[0600/0626], Avg Loss: 0.9211
INFO:local_logger:Epoch[011/800], Step[0600/0626], Avg Loss: 0.9211
INFO:local_logger:Epoch[011/800], Step[0600/0626], Avg Loss: 0.9210
INFO:local_logger:Epoch[011/800], Step[0600/0626], Avg Loss: 0.9212
INFO:master_logger:Epoch[011/800], Step[0600/0626], Avg Loss: 0.9211
INFO:local_logger:Epoch[011/800], Step[0600/0626], Avg Loss: 0.9212
INFO:local_logger:Epoch[011/800], Step[0600/0626], Avg Loss: 0.9213
INFO:local_logger:----- Epoch[011/800], Train Loss: 0.9209, time: 888.60
INFO:local_logger:----- Epoch[011/800], Train Loss: 0.9210, time: 888.60
INFO:local_logger:----- Epoch[011/800], Train Loss: 0.9208, time: 889.00
INFO:local_logger:Now training epoch 12. LR=0.000046
INFO:local_logger:Now training epoch 12. LR=0.000046
INFO:local_logger:Now training epoch 12. LR=0.000046
INFO:local_logger:----- Epoch[011/800], Train Loss: 0.9209, time: 888.65
INFO:local_logger:Now training epoch 12. LR=0.000046
INFO:local_logger:----- Epoch[011/800], Train Loss: 0.9209, time: 884.67
INFO:master_logger:----- Epoch[011/800], Train Loss: 0.9209, time: 884.67
INFO:local_logger:----- Epoch[011/800], Train Loss: 0.9205, time: 888.70
INFO:local_logger:----- Epoch[011/800], Train Loss: 0.9211, time: 889.09
INFO:local_logger:Now training epoch 12. LR=0.000046
INFO:local_logger:Now training epoch 12. LR=0.000046
INFO:local_logger:----- Epoch[011/800], Train Loss: 0.9209, time: 888.68
INFO:local_logger:Now training epoch 12. LR=0.000046
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-11-Loss-0.9209032249693648.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-11-Loss-0.9209032249693648.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-11-Loss-0.9209032249693648.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-11-Loss-0.9209032249693648.pdopt
INFO:local_logger:Now training epoch 12. LR=0.000046
INFO:master_logger:Now training epoch 12. LR=0.000046
INFO:local_logger:Epoch[012/800], Step[0000/0626], Avg Loss: 0.9086
INFO:local_logger:Epoch[012/800], Step[0000/0626], Avg Loss: 0.9130
INFO:master_logger:Epoch[012/800], Step[0000/0626], Avg Loss: 0.9125
INFO:local_logger:Epoch[012/800], Step[0000/0626], Avg Loss: 0.9171
INFO:local_logger:Epoch[012/800], Step[0000/0626], Avg Loss: 0.9156
INFO:local_logger:Epoch[012/800], Step[0000/0626], Avg Loss: 0.9156
INFO:local_logger:Epoch[012/800], Step[0000/0626], Avg Loss: 0.9171
INFO:local_logger:Epoch[012/800], Step[0000/0626], Avg Loss: 0.9090
INFO:local_logger:Epoch[012/800], Step[0000/0626], Avg Loss: 0.9038
INFO:local_logger:Epoch[012/800], Step[0100/0626], Avg Loss: 0.9149
INFO:local_logger:Epoch[012/800], Step[0100/0626], Avg Loss: 0.9150
INFO:local_logger:Epoch[012/800], Step[0100/0626], Avg Loss: 0.9146
INFO:local_logger:Epoch[012/800], Step[0100/0626], Avg Loss: 0.9152
INFO:local_logger:Epoch[012/800], Step[0100/0626], Avg Loss: 0.9145
INFO:local_logger:Epoch[012/800], Step[0100/0626], Avg Loss: 0.9153
INFO:master_logger:Epoch[012/800], Step[0100/0626], Avg Loss: 0.9149
INFO:local_logger:Epoch[012/800], Step[0100/0626], Avg Loss: 0.9149
INFO:local_logger:Epoch[012/800], Step[0100/0626], Avg Loss: 0.9149
INFO:local_logger:Epoch[012/800], Step[0200/0626], Avg Loss: 0.9144
INFO:local_logger:Epoch[012/800], Step[0200/0626], Avg Loss: 0.9141
INFO:local_logger:Epoch[012/800], Step[0200/0626], Avg Loss: 0.9138
INFO:local_logger:Epoch[012/800], Step[0200/0626], Avg Loss: 0.9139
INFO:local_logger:Epoch[012/800], Step[0200/0626], Avg Loss: 0.9143
INFO:local_logger:Epoch[012/800], Step[0200/0626], Avg Loss: 0.9141
INFO:local_logger:Epoch[012/800], Step[0200/0626], Avg Loss: 0.9142
INFO:master_logger:Epoch[012/800], Step[0200/0626], Avg Loss: 0.9142
INFO:local_logger:Epoch[012/800], Step[0200/0626], Avg Loss: 0.9145
INFO:local_logger:Epoch[012/800], Step[0300/0626], Avg Loss: 0.9128
INFO:local_logger:Epoch[012/800], Step[0300/0626], Avg Loss: 0.9132
INFO:local_logger:Epoch[012/800], Step[0300/0626], Avg Loss: 0.9126
INFO:local_logger:Epoch[012/800], Step[0300/0626], Avg Loss: 0.9129
INFO:local_logger:Epoch[012/800], Step[0300/0626], Avg Loss: 0.9133
INFO:local_logger:Epoch[012/800], Step[0300/0626], Avg Loss: 0.9131
INFO:master_logger:Epoch[012/800], Step[0300/0626], Avg Loss: 0.9130
INFO:local_logger:Epoch[012/800], Step[0300/0626], Avg Loss: 0.9127
INFO:local_logger:Epoch[012/800], Step[0300/0626], Avg Loss: 0.9132
INFO:local_logger:Epoch[012/800], Step[0400/0626], Avg Loss: 0.9121
INFO:local_logger:Epoch[012/800], Step[0400/0626], Avg Loss: 0.9115
INFO:local_logger:Epoch[012/800], Step[0400/0626], Avg Loss: 0.9118
INFO:local_logger:Epoch[012/800], Step[0400/0626], Avg Loss: 0.9120
INFO:master_logger:Epoch[012/800], Step[0400/0626], Avg Loss: 0.9119
INFO:local_logger:Epoch[012/800], Step[0400/0626], Avg Loss: 0.9117
INFO:local_logger:Epoch[012/800], Step[0400/0626], Avg Loss: 0.9118
INFO:local_logger:Epoch[012/800], Step[0400/0626], Avg Loss: 0.9119
INFO:local_logger:Epoch[012/800], Step[0400/0626], Avg Loss: 0.9121
INFO:local_logger:Epoch[012/800], Step[0500/0626], Avg Loss: 0.9113
INFO:local_logger:Epoch[012/800], Step[0500/0626], Avg Loss: 0.9111
INFO:local_logger:Epoch[012/800], Step[0500/0626], Avg Loss: 0.9111
INFO:local_logger:Epoch[012/800], Step[0500/0626], Avg Loss: 0.9108
INFO:local_logger:Epoch[012/800], Step[0500/0626], Avg Loss: 0.9108
INFO:master_logger:Epoch[012/800], Step[0500/0626], Avg Loss: 0.9111
INFO:local_logger:Epoch[012/800], Step[0500/0626], Avg Loss: 0.9111
INFO:local_logger:Epoch[012/800], Step[0500/0626], Avg Loss: 0.9113
INFO:local_logger:Epoch[012/800], Step[0500/0626], Avg Loss: 0.9112
INFO:local_logger:Epoch[012/800], Step[0600/0626], Avg Loss: 0.9103
INFO:local_logger:Epoch[012/800], Step[0600/0626], Avg Loss: 0.9101
INFO:local_logger:Epoch[012/800], Step[0600/0626], Avg Loss: 0.9102
INFO:local_logger:Epoch[012/800], Step[0600/0626], Avg Loss: 0.9105
INFO:local_logger:Epoch[012/800], Step[0600/0626], Avg Loss: 0.9103
INFO:local_logger:Epoch[012/800], Step[0600/0626], Avg Loss: 0.9103
INFO:local_logger:Epoch[012/800], Step[0600/0626], Avg Loss: 0.9101
INFO:local_logger:Epoch[012/800], Step[0600/0626], Avg Loss: 0.9099
INFO:master_logger:Epoch[012/800], Step[0600/0626], Avg Loss: 0.9102
INFO:local_logger:----- Epoch[012/800], Train Loss: 0.9102, time: 850.59
INFO:local_logger:Now training epoch 13. LR=0.000049
INFO:local_logger:----- Epoch[012/800], Train Loss: 0.9099, time: 850.55
INFO:local_logger:Now training epoch 13. LR=0.000049
INFO:local_logger:----- Epoch[012/800], Train Loss: 0.9104, time: 851.02
INFO:local_logger:Now training epoch 13. LR=0.000049
INFO:local_logger:----- Epoch[012/800], Train Loss: 0.9099, time: 851.10
INFO:local_logger:Now training epoch 13. LR=0.000049
INFO:local_logger:----- Epoch[012/800], Train Loss: 0.9100, time: 851.10
INFO:local_logger:Now training epoch 13. LR=0.000049
INFO:local_logger:----- Epoch[012/800], Train Loss: 0.9097, time: 847.34
INFO:master_logger:----- Epoch[012/800], Train Loss: 0.9101, time: 847.34
INFO:local_logger:----- Epoch[012/800], Train Loss: 0.9101, time: 851.03
INFO:local_logger:----- Epoch[012/800], Train Loss: 0.9101, time: 851.05
INFO:local_logger:Now training epoch 13. LR=0.000049
INFO:local_logger:Now training epoch 13. LR=0.000049
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-12-Loss-0.9097320030754859.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-12-Loss-0.9097320030754859.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-12-Loss-0.9097320030754859.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-12-Loss-0.9097320030754859.pdopt
INFO:local_logger:Now training epoch 13. LR=0.000049
INFO:master_logger:Now training epoch 13. LR=0.000049
INFO:local_logger:Epoch[013/800], Step[0000/0626], Avg Loss: 0.9093
INFO:local_logger:Epoch[013/800], Step[0000/0626], Avg Loss: 0.9118
INFO:master_logger:Epoch[013/800], Step[0000/0626], Avg Loss: 0.9072
INFO:local_logger:Epoch[013/800], Step[0000/0626], Avg Loss: 0.9034
INFO:local_logger:Epoch[013/800], Step[0000/0626], Avg Loss: 0.9130
INFO:local_logger:Epoch[013/800], Step[0000/0626], Avg Loss: 0.9112
INFO:local_logger:Epoch[013/800], Step[0000/0626], Avg Loss: 0.9077
INFO:local_logger:Epoch[013/800], Step[0000/0626], Avg Loss: 0.9036
INFO:local_logger:Epoch[013/800], Step[0000/0626], Avg Loss: 0.8972
INFO:local_logger:Epoch[013/800], Step[0100/0626], Avg Loss: 0.9039
INFO:local_logger:Epoch[013/800], Step[0100/0626], Avg Loss: 0.9045
INFO:local_logger:Epoch[013/800], Step[0100/0626], Avg Loss: 0.9040
INFO:local_logger:Epoch[013/800], Step[0100/0626], Avg Loss: 0.9050
INFO:local_logger:Epoch[013/800], Step[0100/0626], Avg Loss: 0.9046
INFO:master_logger:Epoch[013/800], Step[0100/0626], Avg Loss: 0.9044
INFO:local_logger:Epoch[013/800], Step[0100/0626], Avg Loss: 0.9047
INFO:local_logger:Epoch[013/800], Step[0100/0626], Avg Loss: 0.9041
INFO:local_logger:Epoch[013/800], Step[0100/0626], Avg Loss: 0.9043
INFO:local_logger:Epoch[013/800], Step[0200/0626], Avg Loss: 0.9035
INFO:local_logger:Epoch[013/800], Step[0200/0626], Avg Loss: 0.9038
INFO:local_logger:Epoch[013/800], Step[0200/0626], Avg Loss: 0.9041
INFO:local_logger:Epoch[013/800], Step[0200/0626], Avg Loss: 0.9039
INFO:local_logger:Epoch[013/800], Step[0200/0626], Avg Loss: 0.9040
INFO:master_logger:Epoch[013/800], Step[0200/0626], Avg Loss: 0.9039
INFO:local_logger:Epoch[013/800], Step[0200/0626], Avg Loss: 0.9040
INFO:local_logger:Epoch[013/800], Step[0200/0626], Avg Loss: 0.9039
INFO:local_logger:Epoch[013/800], Step[0200/0626], Avg Loss: 0.9040
INFO:local_logger:Epoch[013/800], Step[0300/0626], Avg Loss: 0.9027
INFO:local_logger:Epoch[013/800], Step[0300/0626], Avg Loss: 0.9024
INFO:local_logger:Epoch[013/800], Step[0300/0626], Avg Loss: 0.9030
INFO:local_logger:Epoch[013/800], Step[0300/0626], Avg Loss: 0.9027
INFO:local_logger:Epoch[013/800], Step[0300/0626], Avg Loss: 0.9029
INFO:local_logger:Epoch[013/800], Step[0300/0626], Avg Loss: 0.9032
INFO:local_logger:Epoch[013/800], Step[0300/0626], Avg Loss: 0.9029
INFO:master_logger:Epoch[013/800], Step[0300/0626], Avg Loss: 0.9028
INFO:local_logger:Epoch[013/800], Step[0300/0626], Avg Loss: 0.9029
INFO:local_logger:Epoch[013/800], Step[0400/0626], Avg Loss: 0.9021
INFO:local_logger:Epoch[013/800], Step[0400/0626], Avg Loss: 0.9018
INFO:local_logger:Epoch[013/800], Step[0400/0626], Avg Loss: 0.9023
INFO:local_logger:Epoch[013/800], Step[0400/0626], Avg Loss: 0.9018
INFO:local_logger:Epoch[013/800], Step[0400/0626], Avg Loss: 0.9019
INFO:master_logger:Epoch[013/800], Step[0400/0626], Avg Loss: 0.9019
INFO:local_logger:Epoch[013/800], Step[0400/0626], Avg Loss: 0.9015
INFO:local_logger:Epoch[013/800], Step[0400/0626], Avg Loss: 0.9020
INFO:local_logger:Epoch[013/800], Step[0400/0626], Avg Loss: 0.9016
INFO:local_logger:Epoch[013/800], Step[0500/0626], Avg Loss: 0.9012
INFO:local_logger:Epoch[013/800], Step[0500/0626], Avg Loss: 0.9014
INFO:local_logger:Epoch[013/800], Step[0500/0626], Avg Loss: 0.9018
INFO:local_logger:Epoch[013/800], Step[0500/0626], Avg Loss: 0.9013
INFO:master_logger:Epoch[013/800], Step[0500/0626], Avg Loss: 0.9013
INFO:local_logger:Epoch[013/800], Step[0500/0626], Avg Loss: 0.9014
INFO:local_logger:Epoch[013/800], Step[0500/0626], Avg Loss: 0.9011
INFO:local_logger:Epoch[013/800], Step[0500/0626], Avg Loss: 0.9008
INFO:local_logger:Epoch[013/800], Step[0500/0626], Avg Loss: 0.9010
INFO:local_logger:Epoch[013/800], Step[0600/0626], Avg Loss: 0.9002
INFO:local_logger:Epoch[013/800], Step[0600/0626], Avg Loss: 0.9003
INFO:local_logger:Epoch[013/800], Step[0600/0626], Avg Loss: 0.9009
INFO:master_logger:Epoch[013/800], Step[0600/0626], Avg Loss: 0.9003
INFO:local_logger:Epoch[013/800], Step[0600/0626], Avg Loss: 0.8999
INFO:local_logger:Epoch[013/800], Step[0600/0626], Avg Loss: 0.9002
INFO:local_logger:Epoch[013/800], Step[0600/0626], Avg Loss: 0.9001
INFO:local_logger:Epoch[013/800], Step[0600/0626], Avg Loss: 0.9003
INFO:local_logger:Epoch[013/800], Step[0600/0626], Avg Loss: 0.9006
INFO:local_logger:----- Epoch[013/800], Train Loss: 0.9000, time: 883.21
INFO:local_logger:Now training epoch 14. LR=0.000053
INFO:local_logger:----- Epoch[013/800], Train Loss: 0.8998, time: 883.55
INFO:local_logger:Now training epoch 14. LR=0.000053
INFO:local_logger:----- Epoch[013/800], Train Loss: 0.9000, time: 879.83
INFO:master_logger:----- Epoch[013/800], Train Loss: 0.9000, time: 879.83
INFO:local_logger:----- Epoch[013/800], Train Loss: 0.8996, time: 883.81
INFO:local_logger:Now training epoch 14. LR=0.000053
INFO:local_logger:----- Epoch[013/800], Train Loss: 0.9003, time: 884.67
INFO:local_logger:Now training epoch 14. LR=0.000053
INFO:local_logger:----- Epoch[013/800], Train Loss: 0.8999, time: 884.16
INFO:local_logger:Now training epoch 14. LR=0.000053
INFO:local_logger:----- Epoch[013/800], Train Loss: 0.9005, time: 884.17
INFO:local_logger:Now training epoch 14. LR=0.000053
INFO:local_logger:----- Epoch[013/800], Train Loss: 0.8999, time: 884.64
INFO:local_logger:Now training epoch 14. LR=0.000053
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-13-Loss-0.9000374903566999.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-13-Loss-0.9000374903566999.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-13-Loss-0.9000374903566999.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-13-Loss-0.9000374903566999.pdopt
INFO:local_logger:Now training epoch 14. LR=0.000053
INFO:master_logger:Now training epoch 14. LR=0.000053
INFO:local_logger:Epoch[014/800], Step[0000/0626], Avg Loss: 0.8904
INFO:master_logger:Epoch[014/800], Step[0000/0626], Avg Loss: 0.8921
INFO:local_logger:Epoch[014/800], Step[0000/0626], Avg Loss: 0.8961
INFO:local_logger:Epoch[014/800], Step[0000/0626], Avg Loss: 0.8964
INFO:local_logger:Epoch[014/800], Step[0000/0626], Avg Loss: 0.8893
INFO:local_logger:Epoch[014/800], Step[0000/0626], Avg Loss: 0.8854
INFO:local_logger:Epoch[014/800], Step[0000/0626], Avg Loss: 0.8944
INFO:local_logger:Epoch[014/800], Step[0000/0626], Avg Loss: 0.8877
INFO:local_logger:Epoch[014/800], Step[0000/0626], Avg Loss: 0.8971
INFO:local_logger:Epoch[014/800], Step[0100/0626], Avg Loss: 0.8953
INFO:local_logger:Epoch[014/800], Step[0100/0626], Avg Loss: 0.8955
INFO:local_logger:Epoch[014/800], Step[0100/0626], Avg Loss: 0.8938
INFO:local_logger:Epoch[014/800], Step[0100/0626], Avg Loss: 0.8954
INFO:master_logger:Epoch[014/800], Step[0100/0626], Avg Loss: 0.8951
INFO:local_logger:Epoch[014/800], Step[0100/0626], Avg Loss: 0.8947
INFO:local_logger:Epoch[014/800], Step[0100/0626], Avg Loss: 0.8947
INFO:local_logger:Epoch[014/800], Step[0100/0626], Avg Loss: 0.8950
INFO:local_logger:Epoch[014/800], Step[0100/0626], Avg Loss: 0.8962
INFO:local_logger:Epoch[014/800], Step[0200/0626], Avg Loss: 0.8930
INFO:local_logger:Epoch[014/800], Step[0200/0626], Avg Loss: 0.8934
INFO:local_logger:Epoch[014/800], Step[0200/0626], Avg Loss: 0.8928
INFO:local_logger:Epoch[014/800], Step[0200/0626], Avg Loss: 0.8927
INFO:local_logger:Epoch[014/800], Step[0200/0626], Avg Loss: 0.8928
INFO:local_logger:Epoch[014/800], Step[0200/0626], Avg Loss: 0.8934
INFO:master_logger:Epoch[014/800], Step[0200/0626], Avg Loss: 0.8931
INFO:local_logger:Epoch[014/800], Step[0200/0626], Avg Loss: 0.8934
INFO:local_logger:Epoch[014/800], Step[0200/0626], Avg Loss: 0.8931
INFO:local_logger:Epoch[014/800], Step[0300/0626], Avg Loss: 0.8927
INFO:local_logger:Epoch[014/800], Step[0300/0626], Avg Loss: 0.8919
INFO:local_logger:Epoch[014/800], Step[0300/0626], Avg Loss: 0.8921
INFO:local_logger:Epoch[014/800], Step[0300/0626], Avg Loss: 0.8920
INFO:local_logger:Epoch[014/800], Step[0300/0626], Avg Loss: 0.8926
INFO:local_logger:Epoch[014/800], Step[0300/0626], Avg Loss: 0.8924
INFO:local_logger:Epoch[014/800], Step[0300/0626], Avg Loss: 0.8919
INFO:master_logger:Epoch[014/800], Step[0300/0626], Avg Loss: 0.8922
INFO:local_logger:Epoch[014/800], Step[0300/0626], Avg Loss: 0.8920
INFO:local_logger:Epoch[014/800], Step[0400/0626], Avg Loss: 0.8909
INFO:local_logger:Epoch[014/800], Step[0400/0626], Avg Loss: 0.8914
INFO:local_logger:Epoch[014/800], Step[0400/0626], Avg Loss: 0.8908
INFO:local_logger:Epoch[014/800], Step[0400/0626], Avg Loss: 0.8909
INFO:local_logger:Epoch[014/800], Step[0400/0626], Avg Loss: 0.8912
INFO:local_logger:Epoch[014/800], Step[0400/0626], Avg Loss: 0.8910
INFO:master_logger:Epoch[014/800], Step[0400/0626], Avg Loss: 0.8910
INFO:local_logger:Epoch[014/800], Step[0400/0626], Avg Loss: 0.8906
INFO:local_logger:Epoch[014/800], Step[0400/0626], Avg Loss: 0.8914
INFO:local_logger:Epoch[014/800], Step[0500/0626], Avg Loss: 0.8903
INFO:local_logger:Epoch[014/800], Step[0500/0626], Avg Loss: 0.8904
INFO:local_logger:Epoch[014/800], Step[0500/0626], Avg Loss: 0.8906
INFO:local_logger:Epoch[014/800], Step[0500/0626], Avg Loss: 0.8900
INFO:local_logger:Epoch[014/800], Step[0500/0626], Avg Loss: 0.8906
INFO:local_logger:Epoch[014/800], Step[0500/0626], Avg Loss: 0.8902
INFO:local_logger:Epoch[014/800], Step[0500/0626], Avg Loss: 0.8901
INFO:local_logger:Epoch[014/800], Step[0500/0626], Avg Loss: 0.8902
INFO:master_logger:Epoch[014/800], Step[0500/0626], Avg Loss: 0.8903
INFO:local_logger:Epoch[014/800], Step[0600/0626], Avg Loss: 0.8896
INFO:local_logger:Epoch[014/800], Step[0600/0626], Avg Loss: 0.8898
INFO:local_logger:Epoch[014/800], Step[0600/0626], Avg Loss: 0.8896
INFO:master_logger:Epoch[014/800], Step[0600/0626], Avg Loss: 0.8896
INFO:local_logger:Epoch[014/800], Step[0600/0626], Avg Loss: 0.8897
INFO:local_logger:Epoch[014/800], Step[0600/0626], Avg Loss: 0.8893
INFO:local_logger:Epoch[014/800], Step[0600/0626], Avg Loss: 0.8897
INFO:local_logger:Epoch[014/800], Step[0600/0626], Avg Loss: 0.8893
INFO:local_logger:Epoch[014/800], Step[0600/0626], Avg Loss: 0.8894
INFO:local_logger:----- Epoch[014/800], Train Loss: 0.8895, time: 845.39
INFO:local_logger:Now training epoch 15. LR=0.000057
INFO:local_logger:----- Epoch[014/800], Train Loss: 0.8895, time: 845.75
INFO:local_logger:Now training epoch 15. LR=0.000057
INFO:local_logger:----- Epoch[014/800], Train Loss: 0.8892, time: 846.69
INFO:local_logger:----- Epoch[014/800], Train Loss: 0.8894, time: 846.08
INFO:local_logger:Now training epoch 15. LR=0.000057
INFO:local_logger:----- Epoch[014/800], Train Loss: 0.8891, time: 847.03
INFO:local_logger:Now training epoch 15. LR=0.000057
INFO:local_logger:Now training epoch 15. LR=0.000057
INFO:local_logger:----- Epoch[014/800], Train Loss: 0.8890, time: 846.07
INFO:local_logger:Now training epoch 15. LR=0.000057
INFO:local_logger:----- Epoch[014/800], Train Loss: 0.8893, time: 842.90
INFO:master_logger:----- Epoch[014/800], Train Loss: 0.8893, time: 842.90
INFO:local_logger:----- Epoch[014/800], Train Loss: 0.8894, time: 846.07
INFO:local_logger:Now training epoch 15. LR=0.000057
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-14-Loss-0.8892871914493445.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-14-Loss-0.8892871914493445.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-14-Loss-0.8892871914493445.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-14-Loss-0.8892871914493445.pdopt
INFO:local_logger:Now training epoch 15. LR=0.000057
INFO:master_logger:Now training epoch 15. LR=0.000057
INFO:local_logger:Epoch[015/800], Step[0000/0626], Avg Loss: 0.8662
INFO:local_logger:Epoch[015/800], Step[0000/0626], Avg Loss: 0.8831
INFO:local_logger:Epoch[015/800], Step[0000/0626], Avg Loss: 0.8842
INFO:local_logger:Epoch[015/800], Step[0000/0626], Avg Loss: 0.8880
INFO:local_logger:Epoch[015/800], Step[0000/0626], Avg Loss: 0.8864
INFO:local_logger:Epoch[015/800], Step[0000/0626], Avg Loss: 0.8846
INFO:master_logger:Epoch[015/800], Step[0000/0626], Avg Loss: 0.8812
INFO:local_logger:Epoch[015/800], Step[0000/0626], Avg Loss: 0.8773
INFO:local_logger:Epoch[015/800], Step[0000/0626], Avg Loss: 0.8795
INFO:local_logger:Epoch[015/800], Step[0100/0626], Avg Loss: 0.8853
INFO:local_logger:Epoch[015/800], Step[0100/0626], Avg Loss: 0.8863
INFO:local_logger:Epoch[015/800], Step[0100/0626], Avg Loss: 0.8867
INFO:local_logger:Epoch[015/800], Step[0100/0626], Avg Loss: 0.8861
INFO:local_logger:Epoch[015/800], Step[0100/0626], Avg Loss: 0.8861
INFO:local_logger:Epoch[015/800], Step[0100/0626], Avg Loss: 0.8862
INFO:local_logger:Epoch[015/800], Step[0100/0626], Avg Loss: 0.8860
INFO:master_logger:Epoch[015/800], Step[0100/0626], Avg Loss: 0.8860
INFO:local_logger:Epoch[015/800], Step[0100/0626], Avg Loss: 0.8851
INFO:local_logger:Epoch[015/800], Step[0200/0626], Avg Loss: 0.8851
INFO:local_logger:Epoch[015/800], Step[0200/0626], Avg Loss: 0.8849
INFO:local_logger:Epoch[015/800], Step[0200/0626], Avg Loss: 0.8845
INFO:local_logger:Epoch[015/800], Step[0200/0626], Avg Loss: 0.8847
INFO:master_logger:Epoch[015/800], Step[0200/0626], Avg Loss: 0.8846
INFO:local_logger:Epoch[015/800], Step[0200/0626], Avg Loss: 0.8841
INFO:local_logger:Epoch[015/800], Step[0200/0626], Avg Loss: 0.8841
INFO:local_logger:Epoch[015/800], Step[0200/0626], Avg Loss: 0.8847
INFO:local_logger:Epoch[015/800], Step[0200/0626], Avg Loss: 0.8845
INFO:local_logger:Epoch[015/800], Step[0300/0626], Avg Loss: 0.8832
INFO:local_logger:Epoch[015/800], Step[0300/0626], Avg Loss: 0.8831
INFO:local_logger:Epoch[015/800], Step[0300/0626], Avg Loss: 0.8833
INFO:local_logger:Epoch[015/800], Step[0300/0626], Avg Loss: 0.8827
INFO:local_logger:Epoch[015/800], Step[0300/0626], Avg Loss: 0.8833
INFO:local_logger:Epoch[015/800], Step[0300/0626], Avg Loss: 0.8833
INFO:local_logger:Epoch[015/800], Step[0300/0626], Avg Loss: 0.8830
INFO:master_logger:Epoch[015/800], Step[0300/0626], Avg Loss: 0.8831
INFO:local_logger:Epoch[015/800], Step[0300/0626], Avg Loss: 0.8826
INFO:local_logger:Epoch[015/800], Step[0400/0626], Avg Loss: 0.8827
INFO:local_logger:Epoch[015/800], Step[0400/0626], Avg Loss: 0.8824
INFO:local_logger:Epoch[015/800], Step[0400/0626], Avg Loss: 0.8825
INFO:local_logger:Epoch[015/800], Step[0400/0626], Avg Loss: 0.8824
INFO:local_logger:Epoch[015/800], Step[0400/0626], Avg Loss: 0.8827
INFO:local_logger:Epoch[015/800], Step[0400/0626], Avg Loss: 0.8820
INFO:local_logger:Epoch[015/800], Step[0400/0626], Avg Loss: 0.8828
INFO:local_logger:Epoch[015/800], Step[0400/0626], Avg Loss: 0.8819
INFO:master_logger:Epoch[015/800], Step[0400/0626], Avg Loss: 0.8824
INFO:local_logger:Epoch[015/800], Step[0500/0626], Avg Loss: 0.8819
INFO:local_logger:Epoch[015/800], Step[0500/0626], Avg Loss: 0.8813
INFO:local_logger:Epoch[015/800], Step[0500/0626], Avg Loss: 0.8812
INFO:local_logger:Epoch[015/800], Step[0500/0626], Avg Loss: 0.8818
INFO:local_logger:Epoch[015/800], Step[0500/0626], Avg Loss: 0.8816
INFO:local_logger:Epoch[015/800], Step[0500/0626], Avg Loss: 0.8820
INFO:local_logger:Epoch[015/800], Step[0500/0626], Avg Loss: 0.8815
INFO:master_logger:Epoch[015/800], Step[0500/0626], Avg Loss: 0.8816
INFO:local_logger:Epoch[015/800], Step[0500/0626], Avg Loss: 0.8818
INFO:local_logger:Epoch[015/800], Step[0600/0626], Avg Loss: 0.8808
INFO:local_logger:Epoch[015/800], Step[0600/0626], Avg Loss: 0.8805
INFO:local_logger:Epoch[015/800], Step[0600/0626], Avg Loss: 0.8807
INFO:local_logger:Epoch[015/800], Step[0600/0626], Avg Loss: 0.8807
INFO:local_logger:Epoch[015/800], Step[0600/0626], Avg Loss: 0.8804
INFO:master_logger:Epoch[015/800], Step[0600/0626], Avg Loss: 0.8806
INFO:local_logger:Epoch[015/800], Step[0600/0626], Avg Loss: 0.8808
INFO:local_logger:Epoch[015/800], Step[0600/0626], Avg Loss: 0.8804
INFO:local_logger:Epoch[015/800], Step[0600/0626], Avg Loss: 0.8809
INFO:local_logger:----- Epoch[015/800], Train Loss: 0.8805, time: 897.37
INFO:local_logger:Now training epoch 16. LR=0.000061
INFO:local_logger:----- Epoch[015/800], Train Loss: 0.8805, time: 893.90
INFO:master_logger:----- Epoch[015/800], Train Loss: 0.8804, time: 893.90
INFO:local_logger:----- Epoch[015/800], Train Loss: 0.8805, time: 898.09
INFO:local_logger:----- Epoch[015/800], Train Loss: 0.8803, time: 898.09
INFO:local_logger:Now training epoch 16. LR=0.000061
INFO:local_logger:Now training epoch 16. LR=0.000061
INFO:local_logger:----- Epoch[015/800], Train Loss: 0.8802, time: 898.08
INFO:local_logger:Now training epoch 16. LR=0.000061
INFO:local_logger:----- Epoch[015/800], Train Loss: 0.8802, time: 898.09
INFO:local_logger:Now training epoch 16. LR=0.000061
INFO:local_logger:----- Epoch[015/800], Train Loss: 0.8807, time: 898.77
INFO:local_logger:Now training epoch 16. LR=0.000061
INFO:local_logger:----- Epoch[015/800], Train Loss: 0.8806, time: 898.79
INFO:local_logger:Now training epoch 16. LR=0.000061
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-15-Loss-0.8804958925234925.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-15-Loss-0.8804958925234925.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-15-Loss-0.8804958925234925.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-15-Loss-0.8804958925234925.pdopt
INFO:local_logger:Now training epoch 16. LR=0.000061
INFO:master_logger:Now training epoch 16. LR=0.000061
INFO:local_logger:Epoch[016/800], Step[0000/0626], Avg Loss: 0.8772
INFO:local_logger:Epoch[016/800], Step[0000/0626], Avg Loss: 0.8776
INFO:local_logger:Epoch[016/800], Step[0000/0626], Avg Loss: 0.8818
INFO:local_logger:Epoch[016/800], Step[0000/0626], Avg Loss: 0.8756
INFO:master_logger:Epoch[016/800], Step[0000/0626], Avg Loss: 0.8774
INFO:local_logger:Epoch[016/800], Step[0000/0626], Avg Loss: 0.8834
INFO:local_logger:Epoch[016/800], Step[0000/0626], Avg Loss: 0.8802
INFO:local_logger:Epoch[016/800], Step[0000/0626], Avg Loss: 0.8772
INFO:local_logger:Epoch[016/800], Step[0000/0626], Avg Loss: 0.8659
INFO:local_logger:Epoch[016/800], Step[0100/0626], Avg Loss: 0.8729
INFO:local_logger:Epoch[016/800], Step[0100/0626], Avg Loss: 0.8735
INFO:local_logger:Epoch[016/800], Step[0100/0626], Avg Loss: 0.8742
INFO:local_logger:Epoch[016/800], Step[0100/0626], Avg Loss: 0.8743
INFO:local_logger:Epoch[016/800], Step[0100/0626], Avg Loss: 0.8738
INFO:master_logger:Epoch[016/800], Step[0100/0626], Avg Loss: 0.8738
INFO:local_logger:Epoch[016/800], Step[0100/0626], Avg Loss: 0.8741
INFO:local_logger:Epoch[016/800], Step[0100/0626], Avg Loss: 0.8747
INFO:local_logger:Epoch[016/800], Step[0100/0626], Avg Loss: 0.8731
INFO:local_logger:Epoch[016/800], Step[0200/0626], Avg Loss: 0.8724
INFO:local_logger:Epoch[016/800], Step[0200/0626], Avg Loss: 0.8723
INFO:local_logger:Epoch[016/800], Step[0200/0626], Avg Loss: 0.8727
INFO:local_logger:Epoch[016/800], Step[0200/0626], Avg Loss: 0.8726
INFO:local_logger:Epoch[016/800], Step[0200/0626], Avg Loss: 0.8731
INFO:local_logger:Epoch[016/800], Step[0200/0626], Avg Loss: 0.8732
INFO:local_logger:Epoch[016/800], Step[0200/0626], Avg Loss: 0.8728
INFO:master_logger:Epoch[016/800], Step[0200/0626], Avg Loss: 0.8728
INFO:local_logger:Epoch[016/800], Step[0200/0626], Avg Loss: 0.8732
INFO:local_logger:Epoch[016/800], Step[0300/0626], Avg Loss: 0.8718
INFO:local_logger:Epoch[016/800], Step[0300/0626], Avg Loss: 0.8723
INFO:local_logger:Epoch[016/800], Step[0300/0626], Avg Loss: 0.8717
INFO:local_logger:Epoch[016/800], Step[0300/0626], Avg Loss: 0.8718
INFO:local_logger:Epoch[016/800], Step[0300/0626], Avg Loss: 0.8718
INFO:local_logger:Epoch[016/800], Step[0300/0626], Avg Loss: 0.8720
INFO:master_logger:Epoch[016/800], Step[0300/0626], Avg Loss: 0.8719
INFO:local_logger:Epoch[016/800], Step[0300/0626], Avg Loss: 0.8722
INFO:local_logger:Epoch[016/800], Step[0300/0626], Avg Loss: 0.8717
INFO:local_logger:Epoch[016/800], Step[0400/0626], Avg Loss: 0.8710
INFO:local_logger:Epoch[016/800], Step[0400/0626], Avg Loss: 0.8710
INFO:local_logger:Epoch[016/800], Step[0400/0626], Avg Loss: 0.8710
INFO:local_logger:Epoch[016/800], Step[0400/0626], Avg Loss: 0.8713
INFO:local_logger:Epoch[016/800], Step[0400/0626], Avg Loss: 0.8712
INFO:local_logger:Epoch[016/800], Step[0400/0626], Avg Loss: 0.8717
INFO:master_logger:Epoch[016/800], Step[0400/0626], Avg Loss: 0.8712
INFO:local_logger:Epoch[016/800], Step[0400/0626], Avg Loss: 0.8713
INFO:local_logger:Epoch[016/800], Step[0400/0626], Avg Loss: 0.8713
INFO:local_logger:Epoch[016/800], Step[0500/0626], Avg Loss: 0.8707
INFO:local_logger:Epoch[016/800], Step[0500/0626], Avg Loss: 0.8706
INFO:local_logger:Epoch[016/800], Step[0500/0626], Avg Loss: 0.8710
INFO:local_logger:Epoch[016/800], Step[0500/0626], Avg Loss: 0.8707
INFO:local_logger:Epoch[016/800], Step[0500/0626], Avg Loss: 0.8709
INFO:local_logger:Epoch[016/800], Step[0500/0626], Avg Loss: 0.8706
INFO:master_logger:Epoch[016/800], Step[0500/0626], Avg Loss: 0.8707
INFO:local_logger:Epoch[016/800], Step[0500/0626], Avg Loss: 0.8705
INFO:local_logger:Epoch[016/800], Step[0500/0626], Avg Loss: 0.8709
INFO:local_logger:Epoch[016/800], Step[0600/0626], Avg Loss: 0.8696
INFO:local_logger:Epoch[016/800], Step[0600/0626], Avg Loss: 0.8697
INFO:local_logger:Epoch[016/800], Step[0600/0626], Avg Loss: 0.8697
INFO:local_logger:Epoch[016/800], Step[0600/0626], Avg Loss: 0.8697
INFO:local_logger:Epoch[016/800], Step[0600/0626], Avg Loss: 0.8696
INFO:local_logger:Epoch[016/800], Step[0600/0626], Avg Loss: 0.8700
INFO:local_logger:Epoch[016/800], Step[0600/0626], Avg Loss: 0.8701
INFO:master_logger:Epoch[016/800], Step[0600/0626], Avg Loss: 0.8698
INFO:local_logger:Epoch[016/800], Step[0600/0626], Avg Loss: 0.8700
INFO:local_logger:----- Epoch[016/800], Train Loss: 0.8695, time: 861.71
INFO:local_logger:Now training epoch 17. LR=0.000064
INFO:local_logger:----- Epoch[016/800], Train Loss: 0.8693, time: 862.54
INFO:local_logger:Now training epoch 17. LR=0.000064
INFO:local_logger:----- Epoch[016/800], Train Loss: 0.8699, time: 862.86
INFO:local_logger:Now training epoch 17. LR=0.000064
INFO:local_logger:----- Epoch[016/800], Train Loss: 0.8695, time: 863.58
INFO:local_logger:Now training epoch 17. LR=0.000064
INFO:local_logger:----- Epoch[016/800], Train Loss: 0.8695, time: 862.86
INFO:local_logger:----- Epoch[016/800], Train Loss: 0.8698, time: 862.85
INFO:local_logger:Now training epoch 17. LR=0.000064
INFO:local_logger:Now training epoch 17. LR=0.000064
INFO:local_logger:----- Epoch[016/800], Train Loss: 0.8698, time: 862.87
INFO:local_logger:Now training epoch 17. LR=0.000064
INFO:local_logger:----- Epoch[016/800], Train Loss: 0.8694, time: 859.59
INFO:master_logger:----- Epoch[016/800], Train Loss: 0.8696, time: 859.59
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-16-Loss-0.8694310493630203.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-16-Loss-0.8694310493630203.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-16-Loss-0.8694310493630203.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-16-Loss-0.8694310493630203.pdopt
INFO:local_logger:Now training epoch 17. LR=0.000064
INFO:master_logger:Now training epoch 17. LR=0.000064
INFO:local_logger:Epoch[017/800], Step[0000/0626], Avg Loss: 0.8663
INFO:local_logger:Epoch[017/800], Step[0000/0626], Avg Loss: 0.8709
INFO:local_logger:Epoch[017/800], Step[0000/0626], Avg Loss: 0.8677
INFO:local_logger:Epoch[017/800], Step[0000/0626], Avg Loss: 0.8526
INFO:local_logger:Epoch[017/800], Step[0000/0626], Avg Loss: 0.8679
INFO:local_logger:Epoch[017/800], Step[0000/0626], Avg Loss: 0.8632
INFO:master_logger:Epoch[017/800], Step[0000/0626], Avg Loss: 0.8658
INFO:local_logger:Epoch[017/800], Step[0000/0626], Avg Loss: 0.8670
INFO:local_logger:Epoch[017/800], Step[0000/0626], Avg Loss: 0.8705
INFO:local_logger:Epoch[017/800], Step[0100/0626], Avg Loss: 0.8666
INFO:local_logger:Epoch[017/800], Step[0100/0626], Avg Loss: 0.8669
INFO:local_logger:Epoch[017/800], Step[0100/0626], Avg Loss: 0.8674
INFO:local_logger:Epoch[017/800], Step[0100/0626], Avg Loss: 0.8667
INFO:master_logger:Epoch[017/800], Step[0100/0626], Avg Loss: 0.8668
INFO:local_logger:Epoch[017/800], Step[0100/0626], Avg Loss: 0.8662
INFO:local_logger:Epoch[017/800], Step[0100/0626], Avg Loss: 0.8672
INFO:local_logger:Epoch[017/800], Step[0100/0626], Avg Loss: 0.8665
INFO:local_logger:Epoch[017/800], Step[0100/0626], Avg Loss: 0.8673
INFO:local_logger:Epoch[017/800], Step[0200/0626], Avg Loss: 0.8654
INFO:local_logger:Epoch[017/800], Step[0200/0626], Avg Loss: 0.8660
INFO:local_logger:Epoch[017/800], Step[0200/0626], Avg Loss: 0.8658
INFO:local_logger:Epoch[017/800], Step[0200/0626], Avg Loss: 0.8659
INFO:local_logger:Epoch[017/800], Step[0200/0626], Avg Loss: 0.8654
INFO:local_logger:Epoch[017/800], Step[0200/0626], Avg Loss: 0.8658
INFO:master_logger:Epoch[017/800], Step[0200/0626], Avg Loss: 0.8657
INFO:local_logger:Epoch[017/800], Step[0200/0626], Avg Loss: 0.8656
INFO:local_logger:Epoch[017/800], Step[0200/0626], Avg Loss: 0.8654
INFO:local_logger:Epoch[017/800], Step[0300/0626], Avg Loss: 0.8647
INFO:local_logger:Epoch[017/800], Step[0300/0626], Avg Loss: 0.8648
INFO:local_logger:Epoch[017/800], Step[0300/0626], Avg Loss: 0.8646
INFO:local_logger:Epoch[017/800], Step[0300/0626], Avg Loss: 0.8649
INFO:master_logger:Epoch[017/800], Step[0300/0626], Avg Loss: 0.8647
INFO:local_logger:Epoch[017/800], Step[0300/0626], Avg Loss: 0.8652
INFO:local_logger:Epoch[017/800], Step[0300/0626], Avg Loss: 0.8646
INFO:local_logger:Epoch[017/800], Step[0300/0626], Avg Loss: 0.8642
INFO:local_logger:Epoch[017/800], Step[0300/0626], Avg Loss: 0.8648
INFO:local_logger:Epoch[017/800], Step[0400/0626], Avg Loss: 0.8629
INFO:local_logger:Epoch[017/800], Step[0400/0626], Avg Loss: 0.8639
INFO:local_logger:Epoch[017/800], Step[0400/0626], Avg Loss: 0.8636
INFO:local_logger:Epoch[017/800], Step[0400/0626], Avg Loss: 0.8635
INFO:local_logger:Epoch[017/800], Step[0400/0626], Avg Loss: 0.8634
INFO:local_logger:Epoch[017/800], Step[0400/0626], Avg Loss: 0.8634
INFO:local_logger:Epoch[017/800], Step[0400/0626], Avg Loss: 0.8635
INFO:local_logger:Epoch[017/800], Step[0400/0626], Avg Loss: 0.8633
INFO:master_logger:Epoch[017/800], Step[0400/0626], Avg Loss: 0.8634
INFO:local_logger:Epoch[017/800], Step[0500/0626], Avg Loss: 0.8619
INFO:local_logger:Epoch[017/800], Step[0500/0626], Avg Loss: 0.8628
INFO:local_logger:Epoch[017/800], Step[0500/0626], Avg Loss: 0.8626
INFO:local_logger:Epoch[017/800], Step[0500/0626], Avg Loss: 0.8624
INFO:local_logger:Epoch[017/800], Step[0500/0626], Avg Loss: 0.8622
INFO:local_logger:Epoch[017/800], Step[0500/0626], Avg Loss: 0.8624
INFO:master_logger:Epoch[017/800], Step[0500/0626], Avg Loss: 0.8624
INFO:local_logger:Epoch[017/800], Step[0500/0626], Avg Loss: 0.8625
INFO:local_logger:Epoch[017/800], Step[0500/0626], Avg Loss: 0.8625
INFO:local_logger:Epoch[017/800], Step[0600/0626], Avg Loss: 0.8615
INFO:local_logger:Epoch[017/800], Step[0600/0626], Avg Loss: 0.8619
INFO:local_logger:Epoch[017/800], Step[0600/0626], Avg Loss: 0.8620
INFO:local_logger:Epoch[017/800], Step[0600/0626], Avg Loss: 0.8613
INFO:local_logger:Epoch[017/800], Step[0600/0626], Avg Loss: 0.8618
INFO:local_logger:Epoch[017/800], Step[0600/0626], Avg Loss: 0.8618
INFO:local_logger:Epoch[017/800], Step[0600/0626], Avg Loss: 0.8616
INFO:master_logger:Epoch[017/800], Step[0600/0626], Avg Loss: 0.8617
INFO:local_logger:Epoch[017/800], Step[0600/0626], Avg Loss: 0.8619
INFO:local_logger:----- Epoch[017/800], Train Loss: 0.8617, time: 890.30
INFO:local_logger:Now training epoch 18. LR=0.000068
INFO:local_logger:----- Epoch[017/800], Train Loss: 0.8616, time: 890.30
INFO:local_logger:Now training epoch 18. LR=0.000068
INFO:local_logger:----- Epoch[017/800], Train Loss: 0.8617, time: 890.31
INFO:local_logger:Now training epoch 18. LR=0.000068
INFO:local_logger:----- Epoch[017/800], Train Loss: 0.8610, time: 890.96
INFO:local_logger:Now training epoch 18. LR=0.000068
INFO:local_logger:----- Epoch[017/800], Train Loss: 0.8614, time: 887.14
INFO:master_logger:----- Epoch[017/800], Train Loss: 0.8615, time: 887.14
INFO:local_logger:----- Epoch[017/800], Train Loss: 0.8616, time: 891.29
INFO:local_logger:Now training epoch 18. LR=0.000068
INFO:local_logger:----- Epoch[017/800], Train Loss: 0.8617, time: 890.99
INFO:local_logger:Now training epoch 18. LR=0.000068
INFO:local_logger:----- Epoch[017/800], Train Loss: 0.8614, time: 892.15
INFO:local_logger:Now training epoch 18. LR=0.000068
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-17-Loss-0.8613511298173326.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-17-Loss-0.8613511298173326.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-17-Loss-0.8613511298173326.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-17-Loss-0.8613511298173326.pdopt
INFO:local_logger:Now training epoch 18. LR=0.000068
INFO:master_logger:Now training epoch 18. LR=0.000068
INFO:local_logger:Epoch[018/800], Step[0000/0626], Avg Loss: 0.8610
INFO:master_logger:Epoch[018/800], Step[0000/0626], Avg Loss: 0.8573
INFO:local_logger:Epoch[018/800], Step[0000/0626], Avg Loss: 0.8499
INFO:local_logger:Epoch[018/800], Step[0000/0626], Avg Loss: 0.8529
INFO:local_logger:Epoch[018/800], Step[0000/0626], Avg Loss: 0.8567
INFO:local_logger:Epoch[018/800], Step[0000/0626], Avg Loss: 0.8551
INFO:local_logger:Epoch[018/800], Step[0000/0626], Avg Loss: 0.8589
INFO:local_logger:Epoch[018/800], Step[0000/0626], Avg Loss: 0.8601
INFO:local_logger:Epoch[018/800], Step[0000/0626], Avg Loss: 0.8641
INFO:local_logger:Epoch[018/800], Step[0100/0626], Avg Loss: 0.8555
INFO:local_logger:Epoch[018/800], Step[0100/0626], Avg Loss: 0.8553
INFO:local_logger:Epoch[018/800], Step[0100/0626], Avg Loss: 0.8547
INFO:local_logger:Epoch[018/800], Step[0100/0626], Avg Loss: 0.8552
INFO:local_logger:Epoch[018/800], Step[0100/0626], Avg Loss: 0.8543
INFO:local_logger:Epoch[018/800], Step[0100/0626], Avg Loss: 0.8543
INFO:master_logger:Epoch[018/800], Step[0100/0626], Avg Loss: 0.8547
INFO:local_logger:Epoch[018/800], Step[0100/0626], Avg Loss: 0.8542
INFO:local_logger:Epoch[018/800], Step[0100/0626], Avg Loss: 0.8543
INFO:local_logger:Epoch[018/800], Step[0200/0626], Avg Loss: 0.8544
INFO:local_logger:Epoch[018/800], Step[0200/0626], Avg Loss: 0.8543
INFO:local_logger:Epoch[018/800], Step[0200/0626], Avg Loss: 0.8543
INFO:local_logger:Epoch[018/800], Step[0200/0626], Avg Loss: 0.8541
INFO:local_logger:Epoch[018/800], Step[0200/0626], Avg Loss: 0.8537
INFO:master_logger:Epoch[018/800], Step[0200/0626], Avg Loss: 0.8541
INFO:local_logger:Epoch[018/800], Step[0200/0626], Avg Loss: 0.8544
INFO:local_logger:Epoch[018/800], Step[0200/0626], Avg Loss: 0.8539
INFO:local_logger:Epoch[018/800], Step[0200/0626], Avg Loss: 0.8541
INFO:local_logger:Epoch[018/800], Step[0300/0626], Avg Loss: 0.8534
INFO:local_logger:Epoch[018/800], Step[0300/0626], Avg Loss: 0.8534
INFO:local_logger:Epoch[018/800], Step[0300/0626], Avg Loss: 0.8536
INFO:local_logger:Epoch[018/800], Step[0300/0626], Avg Loss: 0.8535
INFO:local_logger:Epoch[018/800], Step[0300/0626], Avg Loss: 0.8540
INFO:local_logger:Epoch[018/800], Step[0300/0626], Avg Loss: 0.8535
INFO:master_logger:Epoch[018/800], Step[0300/0626], Avg Loss: 0.8537
INFO:local_logger:Epoch[018/800], Step[0300/0626], Avg Loss: 0.8544
INFO:local_logger:Epoch[018/800], Step[0300/0626], Avg Loss: 0.8538
INFO:local_logger:Epoch[018/800], Step[0400/0626], Avg Loss: 0.8536
INFO:local_logger:Epoch[018/800], Step[0400/0626], Avg Loss: 0.8534
INFO:local_logger:Epoch[018/800], Step[0400/0626], Avg Loss: 0.8534
INFO:local_logger:Epoch[018/800], Step[0400/0626], Avg Loss: 0.8532
INFO:local_logger:Epoch[018/800], Step[0400/0626], Avg Loss: 0.8542
INFO:local_logger:Epoch[018/800], Step[0400/0626], Avg Loss: 0.8532
INFO:local_logger:Epoch[018/800], Step[0400/0626], Avg Loss: 0.8537
INFO:master_logger:Epoch[018/800], Step[0400/0626], Avg Loss: 0.8535
INFO:local_logger:Epoch[018/800], Step[0400/0626], Avg Loss: 0.8530
INFO:local_logger:Epoch[018/800], Step[0500/0626], Avg Loss: 0.8533
INFO:local_logger:Epoch[018/800], Step[0500/0626], Avg Loss: 0.8536
INFO:local_logger:Epoch[018/800], Step[0500/0626], Avg Loss: 0.8529
INFO:local_logger:Epoch[018/800], Step[0500/0626], Avg Loss: 0.8531
INFO:local_logger:Epoch[018/800], Step[0500/0626], Avg Loss: 0.8534
INFO:local_logger:Epoch[018/800], Step[0500/0626], Avg Loss: 0.8529
INFO:local_logger:Epoch[018/800], Step[0500/0626], Avg Loss: 0.8534
INFO:master_logger:Epoch[018/800], Step[0500/0626], Avg Loss: 0.8533
INFO:local_logger:Epoch[018/800], Step[0500/0626], Avg Loss: 0.8541
INFO:local_logger:Epoch[018/800], Step[0600/0626], Avg Loss: 0.8533
INFO:local_logger:Epoch[018/800], Step[0600/0626], Avg Loss: 0.8525
INFO:local_logger:Epoch[018/800], Step[0600/0626], Avg Loss: 0.8531
INFO:local_logger:Epoch[018/800], Step[0600/0626], Avg Loss: 0.8529
INFO:local_logger:Epoch[018/800], Step[0600/0626], Avg Loss: 0.8528
INFO:local_logger:Epoch[018/800], Step[0600/0626], Avg Loss: 0.8525
INFO:local_logger:Epoch[018/800], Step[0600/0626], Avg Loss: 0.8536
INFO:local_logger:Epoch[018/800], Step[0600/0626], Avg Loss: 0.8528
INFO:master_logger:Epoch[018/800], Step[0600/0626], Avg Loss: 0.8529
INFO:local_logger:----- Epoch[018/800], Train Loss: 0.8532, time: 859.28
INFO:local_logger:Now training epoch 19. LR=0.000072
INFO:local_logger:----- Epoch[018/800], Train Loss: 0.8524, time: 859.95
INFO:local_logger:Now training epoch 19. LR=0.000072
INFO:local_logger:----- Epoch[018/800], Train Loss: 0.8527, time: 855.56
INFO:local_logger:----- Epoch[018/800], Train Loss: 0.8523, time: 859.27
INFO:local_logger:----- Epoch[018/800], Train Loss: 0.8527, time: 859.94
INFO:master_logger:----- Epoch[018/800], Train Loss: 0.8528, time: 855.56
INFO:local_logger:Now training epoch 19. LR=0.000072
INFO:local_logger:Now training epoch 19. LR=0.000072
INFO:local_logger:----- Epoch[018/800], Train Loss: 0.8530, time: 859.29
INFO:local_logger:Now training epoch 19. LR=0.000072
INFO:local_logger:----- Epoch[018/800], Train Loss: 0.8528, time: 859.96
INFO:local_logger:Now training epoch 19. LR=0.000072
INFO:local_logger:----- Epoch[018/800], Train Loss: 0.8534, time: 859.27
INFO:local_logger:Now training epoch 19. LR=0.000072
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-18-Loss-0.8526818839083388.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-18-Loss-0.8526818839083388.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-18-Loss-0.8526818839083388.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-18-Loss-0.8526818839083388.pdopt
INFO:local_logger:Now training epoch 19. LR=0.000072
INFO:master_logger:Now training epoch 19. LR=0.000072
INFO:local_logger:Epoch[019/800], Step[0000/0626], Avg Loss: 0.8466
INFO:local_logger:Epoch[019/800], Step[0000/0626], Avg Loss: 0.8442
INFO:local_logger:Epoch[019/800], Step[0000/0626], Avg Loss: 0.8470
INFO:local_logger:Epoch[019/800], Step[0000/0626], Avg Loss: 0.8424
INFO:local_logger:Epoch[019/800], Step[0000/0626], Avg Loss: 0.8531
INFO:local_logger:Epoch[019/800], Step[0000/0626], Avg Loss: 0.8522
INFO:master_logger:Epoch[019/800], Step[0000/0626], Avg Loss: 0.8474
INFO:local_logger:Epoch[019/800], Step[0000/0626], Avg Loss: 0.8452
INFO:local_logger:Epoch[019/800], Step[0000/0626], Avg Loss: 0.8487
INFO:local_logger:Epoch[019/800], Step[0100/0626], Avg Loss: 0.8481
INFO:local_logger:Epoch[019/800], Step[0100/0626], Avg Loss: 0.8485
INFO:local_logger:Epoch[019/800], Step[0100/0626], Avg Loss: 0.8477
INFO:local_logger:Epoch[019/800], Step[0100/0626], Avg Loss: 0.8475
INFO:local_logger:Epoch[019/800], Step[0100/0626], Avg Loss: 0.8477
INFO:local_logger:Epoch[019/800], Step[0100/0626], Avg Loss: 0.8473
INFO:local_logger:Epoch[019/800], Step[0100/0626], Avg Loss: 0.8491
INFO:master_logger:Epoch[019/800], Step[0100/0626], Avg Loss: 0.8480
INFO:local_logger:Epoch[019/800], Step[0100/0626], Avg Loss: 0.8481
INFO:local_logger:Epoch[019/800], Step[0200/0626], Avg Loss: 0.8476
INFO:local_logger:Epoch[019/800], Step[0200/0626], Avg Loss: 0.8477
INFO:local_logger:Epoch[019/800], Step[0200/0626], Avg Loss: 0.8476
INFO:local_logger:Epoch[019/800], Step[0200/0626], Avg Loss: 0.8481
INFO:local_logger:Epoch[019/800], Step[0200/0626], Avg Loss: 0.8480
INFO:master_logger:Epoch[019/800], Step[0200/0626], Avg Loss: 0.8478
INFO:local_logger:Epoch[019/800], Step[0200/0626], Avg Loss: 0.8483
INFO:local_logger:Epoch[019/800], Step[0200/0626], Avg Loss: 0.8476
INFO:local_logger:Epoch[019/800], Step[0200/0626], Avg Loss: 0.8478
INFO:local_logger:Epoch[019/800], Step[0300/0626], Avg Loss: 0.8470
INFO:local_logger:Epoch[019/800], Step[0300/0626], Avg Loss: 0.8465
INFO:master_logger:Epoch[019/800], Step[0300/0626], Avg Loss: 0.8468
INFO:local_logger:Epoch[019/800], Step[0300/0626], Avg Loss: 0.8469
INFO:local_logger:Epoch[019/800], Step[0300/0626], Avg Loss: 0.8470
INFO:local_logger:Epoch[019/800], Step[0300/0626], Avg Loss: 0.8470
INFO:local_logger:Epoch[019/800], Step[0300/0626], Avg Loss: 0.8465
INFO:local_logger:Epoch[019/800], Step[0300/0626], Avg Loss: 0.8468
INFO:local_logger:Epoch[019/800], Step[0300/0626], Avg Loss: 0.8467
INFO:local_logger:Epoch[019/800], Step[0400/0626], Avg Loss: 0.8458
INFO:local_logger:Epoch[019/800], Step[0400/0626], Avg Loss: 0.8460
INFO:local_logger:Epoch[019/800], Step[0400/0626], Avg Loss: 0.8460
INFO:local_logger:Epoch[019/800], Step[0400/0626], Avg Loss: 0.8464
INFO:local_logger:Epoch[019/800], Step[0400/0626], Avg Loss: 0.8461
INFO:local_logger:Epoch[019/800], Step[0400/0626], Avg Loss: 0.8460
INFO:local_logger:Epoch[019/800], Step[0400/0626], Avg Loss: 0.8465
INFO:local_logger:Epoch[019/800], Step[0400/0626], Avg Loss: 0.8463
INFO:master_logger:Epoch[019/800], Step[0400/0626], Avg Loss: 0.8461
INFO:local_logger:Epoch[019/800], Step[0500/0626], Avg Loss: 0.8452
INFO:local_logger:Epoch[019/800], Step[0500/0626], Avg Loss: 0.8454
INFO:local_logger:Epoch[019/800], Step[0500/0626], Avg Loss: 0.8450
INFO:local_logger:Epoch[019/800], Step[0500/0626], Avg Loss: 0.8455
INFO:local_logger:Epoch[019/800], Step[0500/0626], Avg Loss: 0.8451
INFO:local_logger:Epoch[019/800], Step[0500/0626], Avg Loss: 0.8452
INFO:local_logger:Epoch[019/800], Step[0500/0626], Avg Loss: 0.8452
INFO:local_logger:Epoch[019/800], Step[0500/0626], Avg Loss: 0.8455
INFO:master_logger:Epoch[019/800], Step[0500/0626], Avg Loss: 0.8452
INFO:local_logger:Epoch[019/800], Step[0600/0626], Avg Loss: 0.8445
INFO:local_logger:Epoch[019/800], Step[0600/0626], Avg Loss: 0.8442
INFO:local_logger:Epoch[019/800], Step[0600/0626], Avg Loss: 0.8446
INFO:local_logger:Epoch[019/800], Step[0600/0626], Avg Loss: 0.8445
INFO:local_logger:Epoch[019/800], Step[0600/0626], Avg Loss: 0.8447
INFO:local_logger:Epoch[019/800], Step[0600/0626], Avg Loss: 0.8445
INFO:master_logger:Epoch[019/800], Step[0600/0626], Avg Loss: 0.8445
INFO:local_logger:Epoch[019/800], Step[0600/0626], Avg Loss: 0.8443
INFO:local_logger:Epoch[019/800], Step[0600/0626], Avg Loss: 0.8445
INFO:local_logger:----- Epoch[019/800], Train Loss: 0.8446, time: 880.98
INFO:master_logger:----- Epoch[019/800], Train Loss: 0.8443, time: 880.98
INFO:local_logger:----- Epoch[019/800], Train Loss: 0.8443, time: 885.40
INFO:local_logger:Now training epoch 20. LR=0.000075
INFO:local_logger:----- Epoch[019/800], Train Loss: 0.8443, time: 885.43
INFO:local_logger:Now training epoch 20. LR=0.000075
INFO:local_logger:----- Epoch[019/800], Train Loss: 0.8444, time: 885.46
INFO:local_logger:Now training epoch 20. LR=0.000075
INFO:local_logger:----- Epoch[019/800], Train Loss: 0.8441, time: 885.49
INFO:local_logger:Now training epoch 20. LR=0.000075
INFO:local_logger:----- Epoch[019/800], Train Loss: 0.8441, time: 885.53
INFO:local_logger:Now training epoch 20. LR=0.000075
INFO:local_logger:----- Epoch[019/800], Train Loss: 0.8443, time: 885.54
INFO:local_logger:Now training epoch 20. LR=0.000075
INFO:local_logger:----- Epoch[019/800], Train Loss: 0.8446, time: 885.53
INFO:local_logger:Now training epoch 20. LR=0.000075
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-19-Loss-0.8445631699389794.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-19-Loss-0.8445631699389794.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-19-Loss-0.8445631699389794.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-19-Loss-0.8445631699389794.pdopt
INFO:local_logger:Now training epoch 20. LR=0.000075
INFO:master_logger:Now training epoch 20. LR=0.000075
INFO:local_logger:Epoch[020/800], Step[0000/0626], Avg Loss: 0.8395
INFO:local_logger:Epoch[020/800], Step[0000/0626], Avg Loss: 0.8579
INFO:local_logger:Epoch[020/800], Step[0000/0626], Avg Loss: 0.8337
INFO:master_logger:Epoch[020/800], Step[0000/0626], Avg Loss: 0.8394
INFO:local_logger:Epoch[020/800], Step[0000/0626], Avg Loss: 0.8377
INFO:local_logger:Epoch[020/800], Step[0000/0626], Avg Loss: 0.8425
INFO:local_logger:Epoch[020/800], Step[0000/0626], Avg Loss: 0.8297
INFO:local_logger:Epoch[020/800], Step[0000/0626], Avg Loss: 0.8371
INFO:local_logger:Epoch[020/800], Step[0000/0626], Avg Loss: 0.8374
INFO:local_logger:Epoch[020/800], Step[0100/0626], Avg Loss: 0.8389
INFO:local_logger:Epoch[020/800], Step[0100/0626], Avg Loss: 0.8399
INFO:local_logger:Epoch[020/800], Step[0100/0626], Avg Loss: 0.8385
INFO:local_logger:Epoch[020/800], Step[0100/0626], Avg Loss: 0.8402
INFO:local_logger:Epoch[020/800], Step[0100/0626], Avg Loss: 0.8398
INFO:local_logger:Epoch[020/800], Step[0100/0626], Avg Loss: 0.8399
INFO:master_logger:Epoch[020/800], Step[0100/0626], Avg Loss: 0.8396
INFO:local_logger:Epoch[020/800], Step[0100/0626], Avg Loss: 0.8398
INFO:local_logger:Epoch[020/800], Step[0100/0626], Avg Loss: 0.8400
INFO:local_logger:Epoch[020/800], Step[0200/0626], Avg Loss: 0.8402
INFO:local_logger:Epoch[020/800], Step[0200/0626], Avg Loss: 0.8410
INFO:local_logger:Epoch[020/800], Step[0200/0626], Avg Loss: 0.8400
INFO:local_logger:Epoch[020/800], Step[0200/0626], Avg Loss: 0.8406
INFO:local_logger:Epoch[020/800], Step[0200/0626], Avg Loss: 0.8409
INFO:master_logger:Epoch[020/800], Step[0200/0626], Avg Loss: 0.8408
INFO:local_logger:Epoch[020/800], Step[0200/0626], Avg Loss: 0.8403
INFO:local_logger:Epoch[020/800], Step[0200/0626], Avg Loss: 0.8416
INFO:local_logger:Epoch[020/800], Step[0200/0626], Avg Loss: 0.8415
INFO:local_logger:Epoch[020/800], Step[0300/0626], Avg Loss: 0.8399
INFO:local_logger:Epoch[020/800], Step[0300/0626], Avg Loss: 0.8411
INFO:local_logger:Epoch[020/800], Step[0300/0626], Avg Loss: 0.8404
INFO:local_logger:Epoch[020/800], Step[0300/0626], Avg Loss: 0.8406
INFO:local_logger:Epoch[020/800], Step[0300/0626], Avg Loss: 0.8406
INFO:master_logger:Epoch[020/800], Step[0300/0626], Avg Loss: 0.8406
INFO:local_logger:Epoch[020/800], Step[0300/0626], Avg Loss: 0.8403
INFO:local_logger:Epoch[020/800], Step[0300/0626], Avg Loss: 0.8415
INFO:local_logger:Epoch[020/800], Step[0300/0626], Avg Loss: 0.8403
INFO:local_logger:Epoch[020/800], Step[0400/0626], Avg Loss: 0.8397
INFO:local_logger:Epoch[020/800], Step[0400/0626], Avg Loss: 0.8397
INFO:local_logger:Epoch[020/800], Step[0400/0626], Avg Loss: 0.8393
INFO:local_logger:Epoch[020/800], Step[0400/0626], Avg Loss: 0.8400
INFO:local_logger:Epoch[020/800], Step[0400/0626], Avg Loss: 0.8395
INFO:local_logger:Epoch[020/800], Step[0400/0626], Avg Loss: 0.8400
INFO:local_logger:Epoch[020/800], Step[0400/0626], Avg Loss: 0.8397
INFO:master_logger:Epoch[020/800], Step[0400/0626], Avg Loss: 0.8398
INFO:local_logger:Epoch[020/800], Step[0400/0626], Avg Loss: 0.8404
INFO:local_logger:Epoch[020/800], Step[0500/0626], Avg Loss: 0.8384
INFO:local_logger:Epoch[020/800], Step[0500/0626], Avg Loss: 0.8386
INFO:local_logger:Epoch[020/800], Step[0500/0626], Avg Loss: 0.8384
INFO:local_logger:Epoch[020/800], Step[0500/0626], Avg Loss: 0.8388
INFO:local_logger:Epoch[020/800], Step[0500/0626], Avg Loss: 0.8383
INFO:local_logger:Epoch[020/800], Step[0500/0626], Avg Loss: 0.8387
INFO:local_logger:Epoch[020/800], Step[0500/0626], Avg Loss: 0.8384
INFO:master_logger:Epoch[020/800], Step[0500/0626], Avg Loss: 0.8386
INFO:local_logger:Epoch[020/800], Step[0500/0626], Avg Loss: 0.8391
INFO:local_logger:Epoch[020/800], Step[0600/0626], Avg Loss: 0.8387
INFO:local_logger:Epoch[020/800], Step[0600/0626], Avg Loss: 0.8378
INFO:local_logger:Epoch[020/800], Step[0600/0626], Avg Loss: 0.8380
INFO:local_logger:Epoch[020/800], Step[0600/0626], Avg Loss: 0.8382
INFO:local_logger:Epoch[020/800], Step[0600/0626], Avg Loss: 0.8383
INFO:local_logger:Epoch[020/800], Step[0600/0626], Avg Loss: 0.8381
INFO:master_logger:Epoch[020/800], Step[0600/0626], Avg Loss: 0.8382
INFO:local_logger:Epoch[020/800], Step[0600/0626], Avg Loss: 0.8380
INFO:local_logger:Epoch[020/800], Step[0600/0626], Avg Loss: 0.8383
INFO:local_logger:----- Epoch[020/800], Train Loss: 0.8379, time: 856.21
INFO:local_logger:Now training epoch 21. LR=0.000079
INFO:local_logger:----- Epoch[020/800], Train Loss: 0.8378, time: 856.64
INFO:local_logger:----- Epoch[020/800], Train Loss: 0.8381, time: 856.54
INFO:local_logger:Now training epoch 21. LR=0.000079
INFO:local_logger:Now training epoch 21. LR=0.000079
INFO:local_logger:----- Epoch[020/800], Train Loss: 0.8378, time: 856.55
INFO:local_logger:Now training epoch 21. LR=0.000079
INFO:local_logger:----- Epoch[020/800], Train Loss: 0.8378, time: 856.54
INFO:local_logger:Now training epoch 21. LR=0.000079
INFO:local_logger:----- Epoch[020/800], Train Loss: 0.8385, time: 856.62
INFO:local_logger:----- Epoch[020/800], Train Loss: 0.8379, time: 856.58
INFO:local_logger:Now training epoch 21. LR=0.000079
INFO:local_logger:Now training epoch 21. LR=0.000079
INFO:local_logger:----- Epoch[020/800], Train Loss: 0.8377, time: 853.74
INFO:master_logger:----- Epoch[020/800], Train Loss: 0.8379, time: 853.74
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-20-Loss-0.837697342612629.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-20-Loss-0.837697342612629.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-20-Loss-0.837697342612629.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-20-Loss-0.837697342612629.pdopt
INFO:local_logger:Now training epoch 21. LR=0.000079
INFO:master_logger:Now training epoch 21. LR=0.000079
INFO:local_logger:Epoch[021/800], Step[0000/0626], Avg Loss: 0.8247
INFO:local_logger:Epoch[021/800], Step[0000/0626], Avg Loss: 0.8468
INFO:master_logger:Epoch[021/800], Step[0000/0626], Avg Loss: 0.8311
INFO:local_logger:Epoch[021/800], Step[0000/0626], Avg Loss: 0.8307
INFO:local_logger:Epoch[021/800], Step[0000/0626], Avg Loss: 0.8301
INFO:local_logger:Epoch[021/800], Step[0000/0626], Avg Loss: 0.8220
INFO:local_logger:Epoch[021/800], Step[0000/0626], Avg Loss: 0.8352
INFO:local_logger:Epoch[021/800], Step[0000/0626], Avg Loss: 0.8284
INFO:local_logger:Epoch[021/800], Step[0000/0626], Avg Loss: 0.8313
INFO:local_logger:Epoch[021/800], Step[0100/0626], Avg Loss: 0.8345
INFO:local_logger:Epoch[021/800], Step[0100/0626], Avg Loss: 0.8327
INFO:local_logger:Epoch[021/800], Step[0100/0626], Avg Loss: 0.8336
INFO:local_logger:Epoch[021/800], Step[0100/0626], Avg Loss: 0.8344
INFO:local_logger:Epoch[021/800], Step[0100/0626], Avg Loss: 0.8331
INFO:local_logger:Epoch[021/800], Step[0100/0626], Avg Loss: 0.8343
INFO:master_logger:Epoch[021/800], Step[0100/0626], Avg Loss: 0.8338
INFO:local_logger:Epoch[021/800], Step[0100/0626], Avg Loss: 0.8339
INFO:local_logger:Epoch[021/800], Step[0100/0626], Avg Loss: 0.8339
INFO:local_logger:Epoch[021/800], Step[0200/0626], Avg Loss: 0.8343
INFO:local_logger:Epoch[021/800], Step[0200/0626], Avg Loss: 0.8340
INFO:local_logger:Epoch[021/800], Step[0200/0626], Avg Loss: 0.8344
INFO:local_logger:Epoch[021/800], Step[0200/0626], Avg Loss: 0.8339
INFO:master_logger:Epoch[021/800], Step[0200/0626], Avg Loss: 0.8338
INFO:local_logger:Epoch[021/800], Step[0200/0626], Avg Loss: 0.8333
INFO:local_logger:Epoch[021/800], Step[0200/0626], Avg Loss: 0.8332
INFO:local_logger:Epoch[021/800], Step[0200/0626], Avg Loss: 0.8337
INFO:local_logger:Epoch[021/800], Step[0200/0626], Avg Loss: 0.8335
INFO:local_logger:Epoch[021/800], Step[0300/0626], Avg Loss: 0.8328
INFO:local_logger:Epoch[021/800], Step[0300/0626], Avg Loss: 0.8336
INFO:local_logger:Epoch[021/800], Step[0300/0626], Avg Loss: 0.8330
INFO:local_logger:Epoch[021/800], Step[0300/0626], Avg Loss: 0.8331
INFO:local_logger:Epoch[021/800], Step[0300/0626], Avg Loss: 0.8331
INFO:local_logger:Epoch[021/800], Step[0300/0626], Avg Loss: 0.8337
INFO:local_logger:Epoch[021/800], Step[0300/0626], Avg Loss: 0.8328
INFO:local_logger:Epoch[021/800], Step[0300/0626], Avg Loss: 0.8336
INFO:master_logger:Epoch[021/800], Step[0300/0626], Avg Loss: 0.8332
INFO:local_logger:Epoch[021/800], Step[0400/0626], Avg Loss: 0.8324
INFO:local_logger:Epoch[021/800], Step[0400/0626], Avg Loss: 0.8322
INFO:local_logger:Epoch[021/800], Step[0400/0626], Avg Loss: 0.8329
INFO:local_logger:Epoch[021/800], Step[0400/0626], Avg Loss: 0.8326
INFO:local_logger:Epoch[021/800], Step[0400/0626], Avg Loss: 0.8333
INFO:local_logger:Epoch[021/800], Step[0400/0626], Avg Loss: 0.8324
INFO:local_logger:Epoch[021/800], Step[0400/0626], Avg Loss: 0.8332
INFO:master_logger:Epoch[021/800], Step[0400/0626], Avg Loss: 0.8327
INFO:local_logger:Epoch[021/800], Step[0400/0626], Avg Loss: 0.8328
INFO:local_logger:Epoch[021/800], Step[0500/0626], Avg Loss: 0.8323
INFO:local_logger:Epoch[021/800], Step[0500/0626], Avg Loss: 0.8322
INFO:master_logger:Epoch[021/800], Step[0500/0626], Avg Loss: 0.8321
INFO:local_logger:Epoch[021/800], Step[0500/0626], Avg Loss: 0.8325
INFO:local_logger:Epoch[021/800], Step[0500/0626], Avg Loss: 0.8319
INFO:local_logger:Epoch[021/800], Step[0500/0626], Avg Loss: 0.8317
INFO:local_logger:Epoch[021/800], Step[0500/0626], Avg Loss: 0.8323
INFO:local_logger:Epoch[021/800], Step[0500/0626], Avg Loss: 0.8319
INFO:local_logger:Epoch[021/800], Step[0500/0626], Avg Loss: 0.8320
INFO:local_logger:Epoch[021/800], Step[0600/0626], Avg Loss: 0.8319
INFO:local_logger:Epoch[021/800], Step[0600/0626], Avg Loss: 0.8316
INFO:local_logger:Epoch[021/800], Step[0600/0626], Avg Loss: 0.8318
INFO:local_logger:Epoch[021/800], Step[0600/0626], Avg Loss: 0.8314
INFO:local_logger:Epoch[021/800], Step[0600/0626], Avg Loss: 0.8317
INFO:local_logger:Epoch[021/800], Step[0600/0626], Avg Loss: 0.8314
INFO:local_logger:Epoch[021/800], Step[0600/0626], Avg Loss: 0.8315
INFO:master_logger:Epoch[021/800], Step[0600/0626], Avg Loss: 0.8316
INFO:local_logger:Epoch[021/800], Step[0600/0626], Avg Loss: 0.8317
INFO:local_logger:----- Epoch[021/800], Train Loss: 0.8314, time: 903.73
INFO:master_logger:----- Epoch[021/800], Train Loss: 0.8313, time: 903.73
INFO:local_logger:----- Epoch[021/800], Train Loss: 0.8311, time: 908.09
INFO:local_logger:Now training epoch 22. LR=0.000083
INFO:local_logger:----- Epoch[021/800], Train Loss: 0.8312, time: 908.50
INFO:local_logger:Now training epoch 22. LR=0.000083
INFO:local_logger:----- Epoch[021/800], Train Loss: 0.8312, time: 908.98
INFO:local_logger:Now training epoch 22. LR=0.000083
INFO:local_logger:----- Epoch[021/800], Train Loss: 0.8314, time: 908.52
INFO:local_logger:Now training epoch 22. LR=0.000083
INFO:local_logger:----- Epoch[021/800], Train Loss: 0.8314, time: 908.52
INFO:local_logger:Now training epoch 22. LR=0.000083
INFO:local_logger:----- Epoch[021/800], Train Loss: 0.8311, time: 908.52
INFO:local_logger:Now training epoch 22. LR=0.000083
INFO:local_logger:----- Epoch[021/800], Train Loss: 0.8317, time: 908.52
INFO:local_logger:Now training epoch 22. LR=0.000083
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-21-Loss-0.8314437446567381.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-21-Loss-0.8314437446567381.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-21-Loss-0.8314437446567381.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-21-Loss-0.8314437446567381.pdopt
INFO:local_logger:Now training epoch 22. LR=0.000083
INFO:master_logger:Now training epoch 22. LR=0.000083
INFO:local_logger:Epoch[022/800], Step[0000/0626], Avg Loss: 0.8238
INFO:local_logger:Epoch[022/800], Step[0000/0626], Avg Loss: 0.8287
INFO:master_logger:Epoch[022/800], Step[0000/0626], Avg Loss: 0.8236
INFO:local_logger:Epoch[022/800], Step[0000/0626], Avg Loss: 0.8314
INFO:local_logger:Epoch[022/800], Step[0000/0626], Avg Loss: 0.8120
INFO:local_logger:Epoch[022/800], Step[0000/0626], Avg Loss: 0.8206
INFO:local_logger:Epoch[022/800], Step[0000/0626], Avg Loss: 0.8245
INFO:local_logger:Epoch[022/800], Step[0000/0626], Avg Loss: 0.8214
INFO:local_logger:Epoch[022/800], Step[0000/0626], Avg Loss: 0.8266
INFO:local_logger:Epoch[022/800], Step[0100/0626], Avg Loss: 0.8256
INFO:local_logger:Epoch[022/800], Step[0100/0626], Avg Loss: 0.8255
INFO:local_logger:Epoch[022/800], Step[0100/0626], Avg Loss: 0.8256
INFO:local_logger:Epoch[022/800], Step[0100/0626], Avg Loss: 0.8256
INFO:local_logger:Epoch[022/800], Step[0100/0626], Avg Loss: 0.8274
INFO:local_logger:Epoch[022/800], Step[0100/0626], Avg Loss: 0.8262
INFO:local_logger:Epoch[022/800], Step[0100/0626], Avg Loss: 0.8270
INFO:master_logger:Epoch[022/800], Step[0100/0626], Avg Loss: 0.8262
INFO:local_logger:Epoch[022/800], Step[0100/0626], Avg Loss: 0.8269
INFO:local_logger:Epoch[022/800], Step[0200/0626], Avg Loss: 0.8246
INFO:local_logger:Epoch[022/800], Step[0200/0626], Avg Loss: 0.8258
INFO:local_logger:Epoch[022/800], Step[0200/0626], Avg Loss: 0.8262
INFO:local_logger:Epoch[022/800], Step[0200/0626], Avg Loss: 0.8250
INFO:master_logger:Epoch[022/800], Step[0200/0626], Avg Loss: 0.8252
INFO:local_logger:Epoch[022/800], Step[0200/0626], Avg Loss: 0.8250
INFO:local_logger:Epoch[022/800], Step[0200/0626], Avg Loss: 0.8256
INFO:local_logger:Epoch[022/800], Step[0200/0626], Avg Loss: 0.8245
INFO:local_logger:Epoch[022/800], Step[0200/0626], Avg Loss: 0.8253
INFO:local_logger:Epoch[022/800], Step[0300/0626], Avg Loss: 0.8236
INFO:local_logger:Epoch[022/800], Step[0300/0626], Avg Loss: 0.8237
INFO:local_logger:Epoch[022/800], Step[0300/0626], Avg Loss: 0.8235
INFO:local_logger:Epoch[022/800], Step[0300/0626], Avg Loss: 0.8239
INFO:local_logger:Epoch[022/800], Step[0300/0626], Avg Loss: 0.8245
INFO:local_logger:Epoch[022/800], Step[0300/0626], Avg Loss: 0.8242
INFO:local_logger:Epoch[022/800], Step[0300/0626], Avg Loss: 0.8232
INFO:local_logger:Epoch[022/800], Step[0300/0626], Avg Loss: 0.8245
INFO:master_logger:Epoch[022/800], Step[0300/0626], Avg Loss: 0.8239
INFO:local_logger:Epoch[022/800], Step[0400/0626], Avg Loss: 0.8234
INFO:local_logger:Epoch[022/800], Step[0400/0626], Avg Loss: 0.8226
INFO:local_logger:Epoch[022/800], Step[0400/0626], Avg Loss: 0.8230
INFO:local_logger:Epoch[022/800], Step[0400/0626], Avg Loss: 0.8239
INFO:local_logger:Epoch[022/800], Step[0400/0626], Avg Loss: 0.8240
INFO:local_logger:Epoch[022/800], Step[0400/0626], Avg Loss: 0.8231
INFO:local_logger:Epoch[022/800], Step[0400/0626], Avg Loss: 0.8231
INFO:local_logger:Epoch[022/800], Step[0400/0626], Avg Loss: 0.8235
INFO:master_logger:Epoch[022/800], Step[0400/0626], Avg Loss: 0.8233
INFO:local_logger:Epoch[022/800], Step[0500/0626], Avg Loss: 0.8231
INFO:local_logger:Epoch[022/800], Step[0500/0626], Avg Loss: 0.8231
INFO:local_logger:Epoch[022/800], Step[0500/0626], Avg Loss: 0.8222
INFO:local_logger:Epoch[022/800], Step[0500/0626], Avg Loss: 0.8226
INFO:master_logger:Epoch[022/800], Step[0500/0626], Avg Loss: 0.8225
INFO:local_logger:Epoch[022/800], Step[0500/0626], Avg Loss: 0.8222
INFO:local_logger:Epoch[022/800], Step[0500/0626], Avg Loss: 0.8223
INFO:local_logger:Epoch[022/800], Step[0500/0626], Avg Loss: 0.8226
INFO:local_logger:Epoch[022/800], Step[0500/0626], Avg Loss: 0.8222
INFO:local_logger:Epoch[022/800], Step[0600/0626], Avg Loss: 0.8221
INFO:local_logger:Epoch[022/800], Step[0600/0626], Avg Loss: 0.8219
INFO:local_logger:Epoch[022/800], Step[0600/0626], Avg Loss: 0.8219
INFO:local_logger:Epoch[022/800], Step[0600/0626], Avg Loss: 0.8220
INFO:local_logger:Epoch[022/800], Step[0600/0626], Avg Loss: 0.8223
INFO:local_logger:Epoch[022/800], Step[0600/0626], Avg Loss: 0.8226
INFO:master_logger:Epoch[022/800], Step[0600/0626], Avg Loss: 0.8222
INFO:local_logger:Epoch[022/800], Step[0600/0626], Avg Loss: 0.8228
INFO:local_logger:Epoch[022/800], Step[0600/0626], Avg Loss: 0.8222
INFO:local_logger:----- Epoch[022/800], Train Loss: 0.8221, time: 859.97
INFO:master_logger:----- Epoch[022/800], Train Loss: 0.8221, time: 859.97
INFO:local_logger:----- Epoch[022/800], Train Loss: 0.8221, time: 863.27
INFO:local_logger:Now training epoch 23. LR=0.000087
INFO:local_logger:----- Epoch[022/800], Train Loss: 0.8219, time: 864.02
INFO:local_logger:Now training epoch 23. LR=0.000087
INFO:local_logger:----- Epoch[022/800], Train Loss: 0.8219, time: 863.88
INFO:local_logger:Now training epoch 23. LR=0.000087
INFO:local_logger:----- Epoch[022/800], Train Loss: 0.8227, time: 863.94
INFO:local_logger:Now training epoch 23. LR=0.000087
INFO:local_logger:----- Epoch[022/800], Train Loss: 0.8220, time: 863.94
INFO:local_logger:Now training epoch 23. LR=0.000087
INFO:local_logger:----- Epoch[022/800], Train Loss: 0.8223, time: 863.94
INFO:local_logger:Now training epoch 23. LR=0.000087
INFO:local_logger:----- Epoch[022/800], Train Loss: 0.8219, time: 863.94
INFO:local_logger:Now training epoch 23. LR=0.000087
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-22-Loss-0.8221496500572387.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-22-Loss-0.8221496500572387.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-22-Loss-0.8221496500572387.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-22-Loss-0.8221496500572387.pdopt
INFO:local_logger:Now training epoch 23. LR=0.000087
INFO:master_logger:Now training epoch 23. LR=0.000087
INFO:local_logger:Epoch[023/800], Step[0000/0626], Avg Loss: 0.8185
INFO:local_logger:Epoch[023/800], Step[0000/0626], Avg Loss: 0.8079
INFO:local_logger:Epoch[023/800], Step[0000/0626], Avg Loss: 0.8203
INFO:local_logger:Epoch[023/800], Step[0000/0626], Avg Loss: 0.8216
INFO:master_logger:Epoch[023/800], Step[0000/0626], Avg Loss: 0.8178
INFO:local_logger:Epoch[023/800], Step[0000/0626], Avg Loss: 0.8182
INFO:local_logger:Epoch[023/800], Step[0000/0626], Avg Loss: 0.8154
INFO:local_logger:Epoch[023/800], Step[0000/0626], Avg Loss: 0.8235
INFO:local_logger:Epoch[023/800], Step[0000/0626], Avg Loss: 0.8168
INFO:local_logger:Epoch[023/800], Step[0100/0626], Avg Loss: 0.8184
INFO:local_logger:Epoch[023/800], Step[0100/0626], Avg Loss: 0.8174
INFO:local_logger:Epoch[023/800], Step[0100/0626], Avg Loss: 0.8173
INFO:local_logger:Epoch[023/800], Step[0100/0626], Avg Loss: 0.8176
INFO:local_logger:Epoch[023/800], Step[0100/0626], Avg Loss: 0.8182
INFO:master_logger:Epoch[023/800], Step[0100/0626], Avg Loss: 0.8177
INFO:local_logger:Epoch[023/800], Step[0100/0626], Avg Loss: 0.8171
INFO:local_logger:Epoch[023/800], Step[0100/0626], Avg Loss: 0.8178
INFO:local_logger:Epoch[023/800], Step[0100/0626], Avg Loss: 0.8173
INFO:local_logger:Epoch[023/800], Step[0200/0626], Avg Loss: 0.8173
INFO:local_logger:Epoch[023/800], Step[0200/0626], Avg Loss: 0.8172
INFO:local_logger:Epoch[023/800], Step[0200/0626], Avg Loss: 0.8169
INFO:local_logger:Epoch[023/800], Step[0200/0626], Avg Loss: 0.8168
INFO:local_logger:Epoch[023/800], Step[0200/0626], Avg Loss: 0.8171
INFO:local_logger:Epoch[023/800], Step[0200/0626], Avg Loss: 0.8171
INFO:local_logger:Epoch[023/800], Step[0200/0626], Avg Loss: 0.8172
INFO:local_logger:Epoch[023/800], Step[0200/0626], Avg Loss: 0.8171
INFO:master_logger:Epoch[023/800], Step[0200/0626], Avg Loss: 0.8171
INFO:local_logger:Epoch[023/800], Step[0300/0626], Avg Loss: 0.8166
INFO:local_logger:Epoch[023/800], Step[0300/0626], Avg Loss: 0.8163
INFO:local_logger:Epoch[023/800], Step[0300/0626], Avg Loss: 0.8166
INFO:local_logger:Epoch[023/800], Step[0300/0626], Avg Loss: 0.8167
INFO:local_logger:Epoch[023/800], Step[0300/0626], Avg Loss: 0.8164
INFO:master_logger:Epoch[023/800], Step[0300/0626], Avg Loss: 0.8166
INFO:local_logger:Epoch[023/800], Step[0300/0626], Avg Loss: 0.8170
INFO:local_logger:Epoch[023/800], Step[0300/0626], Avg Loss: 0.8170
INFO:local_logger:Epoch[023/800], Step[0300/0626], Avg Loss: 0.8166
INFO:local_logger:Epoch[023/800], Step[0400/0626], Avg Loss: 0.8161
INFO:local_logger:Epoch[023/800], Step[0400/0626], Avg Loss: 0.8159
INFO:local_logger:Epoch[023/800], Step[0400/0626], Avg Loss: 0.8161
INFO:local_logger:Epoch[023/800], Step[0400/0626], Avg Loss: 0.8159
INFO:local_logger:Epoch[023/800], Step[0400/0626], Avg Loss: 0.8158
INFO:local_logger:Epoch[023/800], Step[0400/0626], Avg Loss: 0.8165
INFO:master_logger:Epoch[023/800], Step[0400/0626], Avg Loss: 0.8161
INFO:local_logger:Epoch[023/800], Step[0400/0626], Avg Loss: 0.8163
INFO:local_logger:Epoch[023/800], Step[0400/0626], Avg Loss: 0.8165
INFO:local_logger:Epoch[023/800], Step[0500/0626], Avg Loss: 0.8157
INFO:local_logger:Epoch[023/800], Step[0500/0626], Avg Loss: 0.8160
INFO:local_logger:Epoch[023/800], Step[0500/0626], Avg Loss: 0.8161
INFO:local_logger:Epoch[023/800], Step[0500/0626], Avg Loss: 0.8155
INFO:master_logger:Epoch[023/800], Step[0500/0626], Avg Loss: 0.8157
INFO:local_logger:Epoch[023/800], Step[0500/0626], Avg Loss: 0.8154
INFO:local_logger:Epoch[023/800], Step[0500/0626], Avg Loss: 0.8155
INFO:local_logger:Epoch[023/800], Step[0500/0626], Avg Loss: 0.8156
INFO:local_logger:Epoch[023/800], Step[0500/0626], Avg Loss: 0.8155
INFO:local_logger:Epoch[023/800], Step[0600/0626], Avg Loss: 0.8151
INFO:local_logger:Epoch[023/800], Step[0600/0626], Avg Loss: 0.8149
INFO:local_logger:Epoch[023/800], Step[0600/0626], Avg Loss: 0.8154
INFO:local_logger:Epoch[023/800], Step[0600/0626], Avg Loss: 0.8151
INFO:local_logger:Epoch[023/800], Step[0600/0626], Avg Loss: 0.8152
INFO:local_logger:Epoch[023/800], Step[0600/0626], Avg Loss: 0.8149
INFO:master_logger:Epoch[023/800], Step[0600/0626], Avg Loss: 0.8151
INFO:local_logger:Epoch[023/800], Step[0600/0626], Avg Loss: 0.8151
INFO:local_logger:Epoch[023/800], Step[0600/0626], Avg Loss: 0.8153
INFO:local_logger:----- Epoch[023/800], Train Loss: 0.8150, time: 884.65
INFO:master_logger:----- Epoch[023/800], Train Loss: 0.8150, time: 884.65
INFO:local_logger:----- Epoch[023/800], Train Loss: 0.8151, time: 888.50
INFO:local_logger:Now training epoch 24. LR=0.000090
INFO:local_logger:----- Epoch[023/800], Train Loss: 0.8152, time: 889.17
INFO:local_logger:Now training epoch 24. LR=0.000090
INFO:local_logger:----- Epoch[023/800], Train Loss: 0.8152, time: 888.59
INFO:local_logger:Now training epoch 24. LR=0.000090
INFO:local_logger:----- Epoch[023/800], Train Loss: 0.8148, time: 888.85
INFO:local_logger:Now training epoch 24. LR=0.000090
INFO:local_logger:----- Epoch[023/800], Train Loss: 0.8149, time: 888.51
INFO:local_logger:Now training epoch 24. LR=0.000090
INFO:local_logger:----- Epoch[023/800], Train Loss: 0.8149, time: 888.52
INFO:local_logger:Now training epoch 24. LR=0.000090
INFO:local_logger:----- Epoch[023/800], Train Loss: 0.8146, time: 888.53
INFO:local_logger:Now training epoch 24. LR=0.000090
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-23-Loss-0.8150022067021212.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-23-Loss-0.8150022067021212.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-23-Loss-0.8150022067021212.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-23-Loss-0.8150022067021212.pdopt
INFO:local_logger:Now training epoch 24. LR=0.000090
INFO:master_logger:Now training epoch 24. LR=0.000090
INFO:local_logger:Epoch[024/800], Step[0000/0626], Avg Loss: 0.8150
INFO:local_logger:Epoch[024/800], Step[0000/0626], Avg Loss: 0.8170
INFO:local_logger:Epoch[024/800], Step[0000/0626], Avg Loss: 0.8108
INFO:local_logger:Epoch[024/800], Step[0000/0626], Avg Loss: 0.8043
INFO:local_logger:Epoch[024/800], Step[0000/0626], Avg Loss: 0.8090
INFO:local_logger:Epoch[024/800], Step[0000/0626], Avg Loss: 0.8224
INFO:local_logger:Epoch[024/800], Step[0000/0626], Avg Loss: 0.8215
INFO:master_logger:Epoch[024/800], Step[0000/0626], Avg Loss: 0.8135
INFO:local_logger:Epoch[024/800], Step[0000/0626], Avg Loss: 0.8082
INFO:local_logger:Epoch[024/800], Step[0100/0626], Avg Loss: 0.8115
INFO:local_logger:Epoch[024/800], Step[0100/0626], Avg Loss: 0.8115
INFO:local_logger:Epoch[024/800], Step[0100/0626], Avg Loss: 0.8110
INFO:local_logger:Epoch[024/800], Step[0100/0626], Avg Loss: 0.8112
INFO:local_logger:Epoch[024/800], Step[0100/0626], Avg Loss: 0.8117
INFO:local_logger:Epoch[024/800], Step[0100/0626], Avg Loss: 0.8124
INFO:local_logger:Epoch[024/800], Step[0100/0626], Avg Loss: 0.8113
INFO:master_logger:Epoch[024/800], Step[0100/0626], Avg Loss: 0.8114
INFO:local_logger:Epoch[024/800], Step[0100/0626], Avg Loss: 0.8105
INFO:local_logger:Epoch[024/800], Step[0200/0626], Avg Loss: 0.8104
INFO:local_logger:Epoch[024/800], Step[0200/0626], Avg Loss: 0.8110
INFO:local_logger:Epoch[024/800], Step[0200/0626], Avg Loss: 0.8111
INFO:local_logger:Epoch[024/800], Step[0200/0626], Avg Loss: 0.8115
INFO:local_logger:Epoch[024/800], Step[0200/0626], Avg Loss: 0.8114
INFO:local_logger:Epoch[024/800], Step[0200/0626], Avg Loss: 0.8115
INFO:local_logger:Epoch[024/800], Step[0200/0626], Avg Loss: 0.8108
INFO:local_logger:Epoch[024/800], Step[0200/0626], Avg Loss: 0.8112
INFO:master_logger:Epoch[024/800], Step[0200/0626], Avg Loss: 0.8111
INFO:local_logger:Epoch[024/800], Step[0300/0626], Avg Loss: 0.8101
INFO:local_logger:Epoch[024/800], Step[0300/0626], Avg Loss: 0.8100
INFO:local_logger:Epoch[024/800], Step[0300/0626], Avg Loss: 0.8106
INFO:local_logger:Epoch[024/800], Step[0300/0626], Avg Loss: 0.8106
INFO:local_logger:Epoch[024/800], Step[0300/0626], Avg Loss: 0.8099
INFO:local_logger:Epoch[024/800], Step[0300/0626], Avg Loss: 0.8100
INFO:local_logger:Epoch[024/800], Step[0300/0626], Avg Loss: 0.8101
INFO:local_logger:Epoch[024/800], Step[0300/0626], Avg Loss: 0.8107
INFO:master_logger:Epoch[024/800], Step[0300/0626], Avg Loss: 0.8103
INFO:local_logger:Epoch[024/800], Step[0400/0626], Avg Loss: 0.8096
INFO:local_logger:Epoch[024/800], Step[0400/0626], Avg Loss: 0.8096
INFO:local_logger:Epoch[024/800], Step[0400/0626], Avg Loss: 0.8096
INFO:local_logger:Epoch[024/800], Step[0400/0626], Avg Loss: 0.8099
INFO:local_logger:Epoch[024/800], Step[0400/0626], Avg Loss: 0.8098
INFO:local_logger:Epoch[024/800], Step[0400/0626], Avg Loss: 0.8097
INFO:local_logger:Epoch[024/800], Step[0400/0626], Avg Loss: 0.8101
INFO:master_logger:Epoch[024/800], Step[0400/0626], Avg Loss: 0.8098
INFO:local_logger:Epoch[024/800], Step[0400/0626], Avg Loss: 0.8103
INFO:local_logger:Epoch[024/800], Step[0500/0626], Avg Loss: 0.8089
INFO:local_logger:Epoch[024/800], Step[0500/0626], Avg Loss: 0.8095
INFO:local_logger:Epoch[024/800], Step[0500/0626], Avg Loss: 0.8094
INFO:local_logger:Epoch[024/800], Step[0500/0626], Avg Loss: 0.8090
INFO:local_logger:Epoch[024/800], Step[0500/0626], Avg Loss: 0.8094
INFO:local_logger:Epoch[024/800], Step[0500/0626], Avg Loss: 0.8090
INFO:local_logger:Epoch[024/800], Step[0500/0626], Avg Loss: 0.8091
INFO:master_logger:Epoch[024/800], Step[0500/0626], Avg Loss: 0.8092
INFO:local_logger:Epoch[024/800], Step[0500/0626], Avg Loss: 0.8090
INFO:local_logger:Epoch[024/800], Step[0600/0626], Avg Loss: 0.8088
INFO:local_logger:Epoch[024/800], Step[0600/0626], Avg Loss: 0.8088
INFO:local_logger:Epoch[024/800], Step[0600/0626], Avg Loss: 0.8093
INFO:local_logger:Epoch[024/800], Step[0600/0626], Avg Loss: 0.8088
INFO:local_logger:Epoch[024/800], Step[0600/0626], Avg Loss: 0.8087
INFO:local_logger:Epoch[024/800], Step[0600/0626], Avg Loss: 0.8091
INFO:local_logger:Epoch[024/800], Step[0600/0626], Avg Loss: 0.8091
INFO:master_logger:Epoch[024/800], Step[0600/0626], Avg Loss: 0.8089
INFO:local_logger:Epoch[024/800], Step[0600/0626], Avg Loss: 0.8088
INFO:local_logger:----- Epoch[024/800], Train Loss: 0.8087, time: 870.26
INFO:local_logger:Now training epoch 25. LR=0.000094
INFO:local_logger:----- Epoch[024/800], Train Loss: 0.8084, time: 870.28
INFO:local_logger:Now training epoch 25. LR=0.000094
INFO:local_logger:----- Epoch[024/800], Train Loss: 0.8092, time: 867.64
INFO:master_logger:----- Epoch[024/800], Train Loss: 0.8088, time: 867.64
INFO:local_logger:----- Epoch[024/800], Train Loss: 0.8090, time: 870.78
INFO:local_logger:Now training epoch 25. LR=0.000094
INFO:local_logger:----- Epoch[024/800], Train Loss: 0.8089, time: 870.77
INFO:local_logger:Now training epoch 25. LR=0.000094
INFO:local_logger:----- Epoch[024/800], Train Loss: 0.8086, time: 870.75
INFO:local_logger:Now training epoch 25. LR=0.000094
INFO:local_logger:----- Epoch[024/800], Train Loss: 0.8086, time: 870.78
INFO:local_logger:----- Epoch[024/800], Train Loss: 0.8087, time: 870.75
INFO:local_logger:Now training epoch 25. LR=0.000094
INFO:local_logger:Now training epoch 25. LR=0.000094
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-24-Loss-0.8091736378081739.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-24-Loss-0.8091736378081739.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-24-Loss-0.8091736378081739.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-24-Loss-0.8091736378081739.pdopt
INFO:local_logger:Now training epoch 25. LR=0.000094
INFO:master_logger:Now training epoch 25. LR=0.000094
INFO:local_logger:Epoch[025/800], Step[0000/0626], Avg Loss: 0.8030
INFO:local_logger:Epoch[025/800], Step[0000/0626], Avg Loss: 0.8073
INFO:local_logger:Epoch[025/800], Step[0000/0626], Avg Loss: 0.7969
INFO:master_logger:Epoch[025/800], Step[0000/0626], Avg Loss: 0.8051
INFO:local_logger:Epoch[025/800], Step[0000/0626], Avg Loss: 0.8085
INFO:local_logger:Epoch[025/800], Step[0000/0626], Avg Loss: 0.8000
INFO:local_logger:Epoch[025/800], Step[0000/0626], Avg Loss: 0.8034
INFO:local_logger:Epoch[025/800], Step[0000/0626], Avg Loss: 0.8158
INFO:local_logger:Epoch[025/800], Step[0000/0626], Avg Loss: 0.8061
INFO:local_logger:Epoch[025/800], Step[0100/0626], Avg Loss: 0.8055
INFO:local_logger:Epoch[025/800], Step[0100/0626], Avg Loss: 0.8046
INFO:local_logger:Epoch[025/800], Step[0100/0626], Avg Loss: 0.8059
INFO:local_logger:Epoch[025/800], Step[0100/0626], Avg Loss: 0.8052
INFO:local_logger:Epoch[025/800], Step[0100/0626], Avg Loss: 0.8053
INFO:local_logger:Epoch[025/800], Step[0100/0626], Avg Loss: 0.8054
INFO:local_logger:Epoch[025/800], Step[0100/0626], Avg Loss: 0.8052
INFO:local_logger:Epoch[025/800], Step[0100/0626], Avg Loss: 0.8057
INFO:master_logger:Epoch[025/800], Step[0100/0626], Avg Loss: 0.8053
INFO:local_logger:Epoch[025/800], Step[0200/0626], Avg Loss: 0.8049
INFO:local_logger:Epoch[025/800], Step[0200/0626], Avg Loss: 0.8050
INFO:local_logger:Epoch[025/800], Step[0200/0626], Avg Loss: 0.8051
INFO:local_logger:Epoch[025/800], Step[0200/0626], Avg Loss: 0.8047
INFO:local_logger:Epoch[025/800], Step[0200/0626], Avg Loss: 0.8054
INFO:local_logger:Epoch[025/800], Step[0200/0626], Avg Loss: 0.8050
INFO:local_logger:Epoch[025/800], Step[0200/0626], Avg Loss: 0.8057
INFO:master_logger:Epoch[025/800], Step[0200/0626], Avg Loss: 0.8051
INFO:local_logger:Epoch[025/800], Step[0200/0626], Avg Loss: 0.8051
INFO:local_logger:Epoch[025/800], Step[0300/0626], Avg Loss: 0.8052
INFO:local_logger:Epoch[025/800], Step[0300/0626], Avg Loss: 0.8049
INFO:local_logger:Epoch[025/800], Step[0300/0626], Avg Loss: 0.8046
INFO:local_logger:Epoch[025/800], Step[0300/0626], Avg Loss: 0.8044
INFO:local_logger:Epoch[025/800], Step[0300/0626], Avg Loss: 0.8048
INFO:local_logger:Epoch[025/800], Step[0300/0626], Avg Loss: 0.8046
INFO:master_logger:Epoch[025/800], Step[0300/0626], Avg Loss: 0.8047
INFO:local_logger:Epoch[025/800], Step[0300/0626], Avg Loss: 0.8044
INFO:local_logger:Epoch[025/800], Step[0300/0626], Avg Loss: 0.8044
INFO:local_logger:Epoch[025/800], Step[0400/0626], Avg Loss: 0.8043
INFO:master_logger:Epoch[025/800], Step[0400/0626], Avg Loss: 0.8042
INFO:local_logger:Epoch[025/800], Step[0400/0626], Avg Loss: 0.8042
INFO:local_logger:Epoch[025/800], Step[0400/0626], Avg Loss: 0.8043
INFO:local_logger:Epoch[025/800], Step[0400/0626], Avg Loss: 0.8040
INFO:local_logger:Epoch[025/800], Step[0400/0626], Avg Loss: 0.8045
INFO:local_logger:Epoch[025/800], Step[0400/0626], Avg Loss: 0.8038
INFO:local_logger:Epoch[025/800], Step[0400/0626], Avg Loss: 0.8043
INFO:local_logger:Epoch[025/800], Step[0400/0626], Avg Loss: 0.8043
INFO:local_logger:Epoch[025/800], Step[0500/0626], Avg Loss: 0.8040
INFO:local_logger:Epoch[025/800], Step[0500/0626], Avg Loss: 0.8039
INFO:local_logger:Epoch[025/800], Step[0500/0626], Avg Loss: 0.8032
INFO:local_logger:Epoch[025/800], Step[0500/0626], Avg Loss: 0.8034
INFO:master_logger:Epoch[025/800], Step[0500/0626], Avg Loss: 0.8036
INFO:local_logger:Epoch[025/800], Step[0500/0626], Avg Loss: 0.8037
INFO:local_logger:Epoch[025/800], Step[0500/0626], Avg Loss: 0.8037
INFO:local_logger:Epoch[025/800], Step[0500/0626], Avg Loss: 0.8037
INFO:local_logger:Epoch[025/800], Step[0500/0626], Avg Loss: 0.8035
INFO:local_logger:Epoch[025/800], Step[0600/0626], Avg Loss: 0.8036
INFO:local_logger:Epoch[025/800], Step[0600/0626], Avg Loss: 0.8029
INFO:master_logger:Epoch[025/800], Step[0600/0626], Avg Loss: 0.8033
INFO:local_logger:Epoch[025/800], Step[0600/0626], Avg Loss: 0.8032
INFO:local_logger:Epoch[025/800], Step[0600/0626], Avg Loss: 0.8032
INFO:local_logger:Epoch[025/800], Step[0600/0626], Avg Loss: 0.8030
INFO:local_logger:Epoch[025/800], Step[0600/0626], Avg Loss: 0.8035
INFO:local_logger:Epoch[025/800], Step[0600/0626], Avg Loss: 0.8036
INFO:local_logger:Epoch[025/800], Step[0600/0626], Avg Loss: 0.8035
INFO:local_logger:----- Epoch[025/800], Train Loss: 0.8033, time: 888.93
INFO:local_logger:Now training epoch 26. LR=0.000098
INFO:local_logger:----- Epoch[025/800], Train Loss: 0.8035, time: 885.88
INFO:master_logger:----- Epoch[025/800], Train Loss: 0.8032, time: 885.88
INFO:local_logger:----- Epoch[025/800], Train Loss: 0.8029, time: 889.72
INFO:local_logger:Now training epoch 26. LR=0.000098
INFO:local_logger:----- Epoch[025/800], Train Loss: 0.8033, time: 889.81
INFO:local_logger:Now training epoch 26. LR=0.000098
INFO:local_logger:----- Epoch[025/800], Train Loss: 0.8031, time: 889.83
INFO:local_logger:Now training epoch 26. LR=0.000098
INFO:local_logger:----- Epoch[025/800], Train Loss: 0.8031, time: 890.36
INFO:local_logger:Now training epoch 26. LR=0.000098
INFO:local_logger:----- Epoch[025/800], Train Loss: 0.8035, time: 890.36
INFO:local_logger:Now training epoch 26. LR=0.000098
INFO:local_logger:----- Epoch[025/800], Train Loss: 0.8028, time: 889.87
INFO:local_logger:Now training epoch 26. LR=0.000098
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-25-Loss-0.8034991228641365.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-25-Loss-0.8034991228641365.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-25-Loss-0.8034991228641365.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-25-Loss-0.8034991228641365.pdopt
INFO:local_logger:Now training epoch 26. LR=0.000098
INFO:master_logger:Now training epoch 26. LR=0.000098
INFO:local_logger:Epoch[026/800], Step[0000/0626], Avg Loss: 0.7943
INFO:local_logger:Epoch[026/800], Step[0000/0626], Avg Loss: 0.7989
INFO:master_logger:Epoch[026/800], Step[0000/0626], Avg Loss: 0.7988
INFO:local_logger:Epoch[026/800], Step[0000/0626], Avg Loss: 0.7899
INFO:local_logger:Epoch[026/800], Step[0000/0626], Avg Loss: 0.7949
INFO:local_logger:Epoch[026/800], Step[0000/0626], Avg Loss: 0.7996
INFO:local_logger:Epoch[026/800], Step[0000/0626], Avg Loss: 0.8022
INFO:local_logger:Epoch[026/800], Step[0000/0626], Avg Loss: 0.8063
INFO:local_logger:Epoch[026/800], Step[0000/0626], Avg Loss: 0.8043
INFO:local_logger:Epoch[026/800], Step[0100/0626], Avg Loss: 0.7989
INFO:local_logger:Epoch[026/800], Step[0100/0626], Avg Loss: 0.7997
INFO:local_logger:Epoch[026/800], Step[0100/0626], Avg Loss: 0.7993
INFO:local_logger:Epoch[026/800], Step[0100/0626], Avg Loss: 0.7994
INFO:local_logger:Epoch[026/800], Step[0100/0626], Avg Loss: 0.7998
INFO:local_logger:Epoch[026/800], Step[0100/0626], Avg Loss: 0.7976
INFO:master_logger:Epoch[026/800], Step[0100/0626], Avg Loss: 0.7992
INFO:local_logger:Epoch[026/800], Step[0100/0626], Avg Loss: 0.7992
INFO:local_logger:Epoch[026/800], Step[0100/0626], Avg Loss: 0.7992
INFO:local_logger:Epoch[026/800], Step[0200/0626], Avg Loss: 0.7993
INFO:local_logger:Epoch[026/800], Step[0200/0626], Avg Loss: 0.7986
INFO:local_logger:Epoch[026/800], Step[0200/0626], Avg Loss: 0.7985
INFO:local_logger:Epoch[026/800], Step[0200/0626], Avg Loss: 0.7986
INFO:local_logger:Epoch[026/800], Step[0200/0626], Avg Loss: 0.7987
INFO:local_logger:Epoch[026/800], Step[0200/0626], Avg Loss: 0.7988
INFO:local_logger:Epoch[026/800], Step[0200/0626], Avg Loss: 0.7980
INFO:master_logger:Epoch[026/800], Step[0200/0626], Avg Loss: 0.7988
INFO:local_logger:Epoch[026/800], Step[0200/0626], Avg Loss: 0.7999
INFO:local_logger:Epoch[026/800], Step[0300/0626], Avg Loss: 0.7980
INFO:local_logger:Epoch[026/800], Step[0300/0626], Avg Loss: 0.7983
INFO:local_logger:Epoch[026/800], Step[0300/0626], Avg Loss: 0.7983
INFO:local_logger:Epoch[026/800], Step[0300/0626], Avg Loss: 0.7992
INFO:local_logger:Epoch[026/800], Step[0300/0626], Avg Loss: 0.7980
INFO:local_logger:Epoch[026/800], Step[0300/0626], Avg Loss: 0.7983
INFO:local_logger:Epoch[026/800], Step[0300/0626], Avg Loss: 0.7985
INFO:master_logger:Epoch[026/800], Step[0300/0626], Avg Loss: 0.7983
INFO:local_logger:Epoch[026/800], Step[0300/0626], Avg Loss: 0.7979
INFO:local_logger:Epoch[026/800], Step[0400/0626], Avg Loss: 0.7977
INFO:local_logger:Epoch[026/800], Step[0400/0626], Avg Loss: 0.7973
INFO:local_logger:Epoch[026/800], Step[0400/0626], Avg Loss: 0.7976
INFO:local_logger:Epoch[026/800], Step[0400/0626], Avg Loss: 0.7977
INFO:local_logger:Epoch[026/800], Step[0400/0626], Avg Loss: 0.7975
INFO:local_logger:Epoch[026/800], Step[0400/0626], Avg Loss: 0.7976
INFO:local_logger:Epoch[026/800], Step[0400/0626], Avg Loss: 0.7979
INFO:master_logger:Epoch[026/800], Step[0400/0626], Avg Loss: 0.7977
INFO:local_logger:Epoch[026/800], Step[0400/0626], Avg Loss: 0.7984
INFO:local_logger:Epoch[026/800], Step[0500/0626], Avg Loss: 0.7970
INFO:local_logger:Epoch[026/800], Step[0500/0626], Avg Loss: 0.7967
INFO:local_logger:Epoch[026/800], Step[0500/0626], Avg Loss: 0.7967
INFO:local_logger:Epoch[026/800], Step[0500/0626], Avg Loss: 0.7976
INFO:local_logger:Epoch[026/800], Step[0500/0626], Avg Loss: 0.7970
INFO:local_logger:Epoch[026/800], Step[0500/0626], Avg Loss: 0.7965
INFO:master_logger:Epoch[026/800], Step[0500/0626], Avg Loss: 0.7969
INFO:local_logger:Epoch[026/800], Step[0500/0626], Avg Loss: 0.7968
INFO:local_logger:Epoch[026/800], Step[0500/0626], Avg Loss: 0.7969
INFO:local_logger:Epoch[026/800], Step[0600/0626], Avg Loss: 0.7961
INFO:local_logger:Epoch[026/800], Step[0600/0626], Avg Loss: 0.7961
INFO:local_logger:Epoch[026/800], Step[0600/0626], Avg Loss: 0.7959
INFO:local_logger:Epoch[026/800], Step[0600/0626], Avg Loss: 0.7961
INFO:local_logger:Epoch[026/800], Step[0600/0626], Avg Loss: 0.7969
INFO:local_logger:Epoch[026/800], Step[0600/0626], Avg Loss: 0.7961
INFO:master_logger:Epoch[026/800], Step[0600/0626], Avg Loss: 0.7962
INFO:local_logger:Epoch[026/800], Step[0600/0626], Avg Loss: 0.7963
INFO:local_logger:Epoch[026/800], Step[0600/0626], Avg Loss: 0.7964
INFO:local_logger:----- Epoch[026/800], Train Loss: 0.7968, time: 871.34
INFO:local_logger:Now training epoch 27. LR=0.000102
INFO:local_logger:----- Epoch[026/800], Train Loss: 0.7961, time: 867.96
INFO:master_logger:----- Epoch[026/800], Train Loss: 0.7962, time: 867.96
INFO:local_logger:----- Epoch[026/800], Train Loss: 0.7959, time: 871.51
INFO:local_logger:Now training epoch 27. LR=0.000102
INFO:local_logger:----- Epoch[026/800], Train Loss: 0.7960, time: 871.97
INFO:local_logger:Now training epoch 27. LR=0.000102
INFO:local_logger:----- Epoch[026/800], Train Loss: 0.7960, time: 871.99
INFO:local_logger:Now training epoch 27. LR=0.000102
INFO:local_logger:----- Epoch[026/800], Train Loss: 0.7963, time: 872.03
INFO:local_logger:Now training epoch 27. LR=0.000102
INFO:local_logger:----- Epoch[026/800], Train Loss: 0.7962, time: 872.93
INFO:local_logger:Now training epoch 27. LR=0.000102
INFO:local_logger:----- Epoch[026/800], Train Loss: 0.7961, time: 872.05
INFO:local_logger:Now training epoch 27. LR=0.000102
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-26-Loss-0.796081899062454.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-26-Loss-0.796081899062454.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-26-Loss-0.796081899062454.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-26-Loss-0.796081899062454.pdopt
INFO:local_logger:Now training epoch 27. LR=0.000102
INFO:master_logger:Now training epoch 27. LR=0.000102
INFO:local_logger:Epoch[027/800], Step[0000/0626], Avg Loss: 0.7991
INFO:local_logger:Epoch[027/800], Step[0000/0626], Avg Loss: 0.8009
INFO:local_logger:Epoch[027/800], Step[0000/0626], Avg Loss: 0.7909
INFO:master_logger:Epoch[027/800], Step[0000/0626], Avg Loss: 0.7963
INFO:local_logger:Epoch[027/800], Step[0000/0626], Avg Loss: 0.8054
INFO:local_logger:Epoch[027/800], Step[0000/0626], Avg Loss: 0.7875
INFO:local_logger:Epoch[027/800], Step[0000/0626], Avg Loss: 0.7990
INFO:local_logger:Epoch[027/800], Step[0000/0626], Avg Loss: 0.7946
INFO:local_logger:Epoch[027/800], Step[0000/0626], Avg Loss: 0.7932
INFO:local_logger:Epoch[027/800], Step[0100/0626], Avg Loss: 0.7923
INFO:local_logger:Epoch[027/800], Step[0100/0626], Avg Loss: 0.7922
INFO:local_logger:Epoch[027/800], Step[0100/0626], Avg Loss: 0.7923
INFO:master_logger:Epoch[027/800], Step[0100/0626], Avg Loss: 0.7920
INFO:local_logger:Epoch[027/800], Step[0100/0626], Avg Loss: 0.7924
INFO:local_logger:Epoch[027/800], Step[0100/0626], Avg Loss: 0.7913
INFO:local_logger:Epoch[027/800], Step[0100/0626], Avg Loss: 0.7925
INFO:local_logger:Epoch[027/800], Step[0100/0626], Avg Loss: 0.7917
INFO:local_logger:Epoch[027/800], Step[0100/0626], Avg Loss: 0.7914
INFO:local_logger:Epoch[027/800], Step[0200/0626], Avg Loss: 0.7922
INFO:local_logger:Epoch[027/800], Step[0200/0626], Avg Loss: 0.7922
INFO:local_logger:Epoch[027/800], Step[0200/0626], Avg Loss: 0.7924
INFO:local_logger:Epoch[027/800], Step[0200/0626], Avg Loss: 0.7924
INFO:local_logger:Epoch[027/800], Step[0200/0626], Avg Loss: 0.7915
INFO:local_logger:Epoch[027/800], Step[0200/0626], Avg Loss: 0.7919
INFO:master_logger:Epoch[027/800], Step[0200/0626], Avg Loss: 0.7920
INFO:local_logger:Epoch[027/800], Step[0200/0626], Avg Loss: 0.7914
INFO:local_logger:Epoch[027/800], Step[0200/0626], Avg Loss: 0.7921
INFO:local_logger:Epoch[027/800], Step[0300/0626], Avg Loss: 0.7914
INFO:local_logger:Epoch[027/800], Step[0300/0626], Avg Loss: 0.7917
INFO:local_logger:Epoch[027/800], Step[0300/0626], Avg Loss: 0.7905
INFO:local_logger:Epoch[027/800], Step[0300/0626], Avg Loss: 0.7909
INFO:master_logger:Epoch[027/800], Step[0300/0626], Avg Loss: 0.7911
INFO:local_logger:Epoch[027/800], Step[0300/0626], Avg Loss: 0.7907
INFO:local_logger:Epoch[027/800], Step[0300/0626], Avg Loss: 0.7915
INFO:local_logger:Epoch[027/800], Step[0300/0626], Avg Loss: 0.7912
INFO:local_logger:Epoch[027/800], Step[0300/0626], Avg Loss: 0.7909
INFO:local_logger:Epoch[027/800], Step[0400/0626], Avg Loss: 0.7909
INFO:local_logger:Epoch[027/800], Step[0400/0626], Avg Loss: 0.7900
INFO:local_logger:Epoch[027/800], Step[0400/0626], Avg Loss: 0.7909
INFO:local_logger:Epoch[027/800], Step[0400/0626], Avg Loss: 0.7902
INFO:local_logger:Epoch[027/800], Step[0400/0626], Avg Loss: 0.7904
INFO:master_logger:Epoch[027/800], Step[0400/0626], Avg Loss: 0.7906
INFO:local_logger:Epoch[027/800], Step[0400/0626], Avg Loss: 0.7907
INFO:local_logger:Epoch[027/800], Step[0400/0626], Avg Loss: 0.7903
INFO:local_logger:Epoch[027/800], Step[0400/0626], Avg Loss: 0.7911
INFO:local_logger:Epoch[027/800], Step[0500/0626], Avg Loss: 0.7902
INFO:local_logger:Epoch[027/800], Step[0500/0626], Avg Loss: 0.7899
INFO:local_logger:Epoch[027/800], Step[0500/0626], Avg Loss: 0.7898
INFO:local_logger:Epoch[027/800], Step[0500/0626], Avg Loss: 0.7903
INFO:local_logger:Epoch[027/800], Step[0500/0626], Avg Loss: 0.7903
INFO:master_logger:Epoch[027/800], Step[0500/0626], Avg Loss: 0.7900
INFO:local_logger:Epoch[027/800], Step[0500/0626], Avg Loss: 0.7904
INFO:local_logger:Epoch[027/800], Step[0500/0626], Avg Loss: 0.7895
INFO:local_logger:Epoch[027/800], Step[0500/0626], Avg Loss: 0.7898
INFO:local_logger:Epoch[027/800], Step[0600/0626], Avg Loss: 0.7896
INFO:local_logger:Epoch[027/800], Step[0600/0626], Avg Loss: 0.7893
INFO:local_logger:Epoch[027/800], Step[0600/0626], Avg Loss: 0.7898
INFO:local_logger:Epoch[027/800], Step[0600/0626], Avg Loss: 0.7894
INFO:local_logger:Epoch[027/800], Step[0600/0626], Avg Loss: 0.7893
INFO:local_logger:Epoch[027/800], Step[0600/0626], Avg Loss: 0.7899
INFO:local_logger:Epoch[027/800], Step[0600/0626], Avg Loss: 0.7891
INFO:local_logger:Epoch[027/800], Step[0600/0626], Avg Loss: 0.7894
INFO:master_logger:Epoch[027/800], Step[0600/0626], Avg Loss: 0.7895
INFO:local_logger:----- Epoch[027/800], Train Loss: 0.7892, time: 878.78
INFO:local_logger:Now training epoch 28. LR=0.000105
INFO:local_logger:----- Epoch[027/800], Train Loss: 0.7897, time: 878.81
INFO:local_logger:Now training epoch 28. LR=0.000105
INFO:local_logger:----- Epoch[027/800], Train Loss: 0.7894, time: 875.73
INFO:master_logger:----- Epoch[027/800], Train Loss: 0.7893, time: 875.73
INFO:local_logger:----- Epoch[027/800], Train Loss: 0.7890, time: 878.88
INFO:local_logger:Now training epoch 28. LR=0.000105
INFO:local_logger:----- Epoch[027/800], Train Loss: 0.7892, time: 878.97
INFO:local_logger:Now training epoch 28. LR=0.000105
INFO:local_logger:----- Epoch[027/800], Train Loss: 0.7891, time: 879.29
INFO:local_logger:Now training epoch 28. LR=0.000105
INFO:local_logger:----- Epoch[027/800], Train Loss: 0.7895, time: 879.32
INFO:local_logger:Now training epoch 28. LR=0.000105
INFO:local_logger:----- Epoch[027/800], Train Loss: 0.7895, time: 879.32
INFO:local_logger:Now training epoch 28. LR=0.000105
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-27-Loss-0.7894227700390196.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-27-Loss-0.7894227700390196.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-27-Loss-0.7894227700390196.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-27-Loss-0.7894227700390196.pdopt
INFO:local_logger:Now training epoch 28. LR=0.000105
INFO:master_logger:Now training epoch 28. LR=0.000105
INFO:local_logger:Epoch[028/800], Step[0000/0626], Avg Loss: 0.7943
INFO:master_logger:Epoch[028/800], Step[0000/0626], Avg Loss: 0.7880
INFO:local_logger:Epoch[028/800], Step[0000/0626], Avg Loss: 0.7931
INFO:local_logger:Epoch[028/800], Step[0000/0626], Avg Loss: 0.7814
INFO:local_logger:Epoch[028/800], Step[0000/0626], Avg Loss: 0.7872
INFO:local_logger:Epoch[028/800], Step[0000/0626], Avg Loss: 0.7910
INFO:local_logger:Epoch[028/800], Step[0000/0626], Avg Loss: 0.7793
INFO:local_logger:Epoch[028/800], Step[0000/0626], Avg Loss: 0.7897
INFO:local_logger:Epoch[028/800], Step[0000/0626], Avg Loss: 0.7883
INFO:local_logger:Epoch[028/800], Step[0100/0626], Avg Loss: 0.7855
INFO:local_logger:Epoch[028/800], Step[0100/0626], Avg Loss: 0.7868
INFO:local_logger:Epoch[028/800], Step[0100/0626], Avg Loss: 0.7873
INFO:local_logger:Epoch[028/800], Step[0100/0626], Avg Loss: 0.7853
INFO:local_logger:Epoch[028/800], Step[0100/0626], Avg Loss: 0.7862
INFO:local_logger:Epoch[028/800], Step[0100/0626], Avg Loss: 0.7875
INFO:master_logger:Epoch[028/800], Step[0100/0626], Avg Loss: 0.7866
INFO:local_logger:Epoch[028/800], Step[0100/0626], Avg Loss: 0.7874
INFO:local_logger:Epoch[028/800], Step[0100/0626], Avg Loss: 0.7871
INFO:local_logger:Epoch[028/800], Step[0200/0626], Avg Loss: 0.7865
INFO:local_logger:Epoch[028/800], Step[0200/0626], Avg Loss: 0.7866
INFO:local_logger:Epoch[028/800], Step[0200/0626], Avg Loss: 0.7859
INFO:local_logger:Epoch[028/800], Step[0200/0626], Avg Loss: 0.7855
INFO:local_logger:Epoch[028/800], Step[0200/0626], Avg Loss: 0.7855
INFO:local_logger:Epoch[028/800], Step[0200/0626], Avg Loss: 0.7858
INFO:local_logger:Epoch[028/800], Step[0200/0626], Avg Loss: 0.7860
INFO:master_logger:Epoch[028/800], Step[0200/0626], Avg Loss: 0.7861
INFO:local_logger:Epoch[028/800], Step[0200/0626], Avg Loss: 0.7865
INFO:local_logger:Epoch[028/800], Step[0300/0626], Avg Loss: 0.7850
INFO:local_logger:Epoch[028/800], Step[0300/0626], Avg Loss: 0.7850
INFO:local_logger:Epoch[028/800], Step[0300/0626], Avg Loss: 0.7851
INFO:local_logger:Epoch[028/800], Step[0300/0626], Avg Loss: 0.7845
INFO:master_logger:Epoch[028/800], Step[0300/0626], Avg Loss: 0.7851
INFO:local_logger:Epoch[028/800], Step[0300/0626], Avg Loss: 0.7851
INFO:local_logger:Epoch[028/800], Step[0300/0626], Avg Loss: 0.7855
INFO:local_logger:Epoch[028/800], Step[0300/0626], Avg Loss: 0.7846
INFO:local_logger:Epoch[028/800], Step[0300/0626], Avg Loss: 0.7860
INFO:local_logger:Epoch[028/800], Step[0400/0626], Avg Loss: 0.7841
INFO:local_logger:Epoch[028/800], Step[0400/0626], Avg Loss: 0.7844
INFO:local_logger:Epoch[028/800], Step[0400/0626], Avg Loss: 0.7839
INFO:local_logger:Epoch[028/800], Step[0400/0626], Avg Loss: 0.7850
INFO:master_logger:Epoch[028/800], Step[0400/0626], Avg Loss: 0.7843
INFO:local_logger:Epoch[028/800], Step[0400/0626], Avg Loss: 0.7837
INFO:local_logger:Epoch[028/800], Step[0400/0626], Avg Loss: 0.7843
INFO:local_logger:Epoch[028/800], Step[0400/0626], Avg Loss: 0.7843
INFO:local_logger:Epoch[028/800], Step[0400/0626], Avg Loss: 0.7845
INFO:local_logger:Epoch[028/800], Step[0500/0626], Avg Loss: 0.7842
INFO:local_logger:Epoch[028/800], Step[0500/0626], Avg Loss: 0.7838
INFO:local_logger:Epoch[028/800], Step[0500/0626], Avg Loss: 0.7835
INFO:local_logger:Epoch[028/800], Step[0500/0626], Avg Loss: 0.7841
INFO:local_logger:Epoch[028/800], Step[0500/0626], Avg Loss: 0.7840
INFO:local_logger:Epoch[028/800], Step[0500/0626], Avg Loss: 0.7839
INFO:master_logger:Epoch[028/800], Step[0500/0626], Avg Loss: 0.7840
INFO:local_logger:Epoch[028/800], Step[0500/0626], Avg Loss: 0.7846
INFO:local_logger:Epoch[028/800], Step[0500/0626], Avg Loss: 0.7837
INFO:local_logger:Epoch[028/800], Step[0600/0626], Avg Loss: 0.7838
INFO:local_logger:Epoch[028/800], Step[0600/0626], Avg Loss: 0.7831
INFO:local_logger:Epoch[028/800], Step[0600/0626], Avg Loss: 0.7837
INFO:local_logger:Epoch[028/800], Step[0600/0626], Avg Loss: 0.7836
INFO:local_logger:Epoch[028/800], Step[0600/0626], Avg Loss: 0.7836
INFO:local_logger:Epoch[028/800], Step[0600/0626], Avg Loss: 0.7840
INFO:local_logger:Epoch[028/800], Step[0600/0626], Avg Loss: 0.7839
INFO:local_logger:Epoch[028/800], Step[0600/0626], Avg Loss: 0.7833
INFO:master_logger:Epoch[028/800], Step[0600/0626], Avg Loss: 0.7836
INFO:local_logger:----- Epoch[028/800], Train Loss: 0.7837, time: 871.38
INFO:local_logger:Now training epoch 29. LR=0.000109
INFO:local_logger:----- Epoch[028/800], Train Loss: 0.7835, time: 868.22
INFO:master_logger:----- Epoch[028/800], Train Loss: 0.7835, time: 868.22
INFO:local_logger:----- Epoch[028/800], Train Loss: 0.7831, time: 872.18
INFO:local_logger:Now training epoch 29. LR=0.000109
INFO:local_logger:----- Epoch[028/800], Train Loss: 0.7830, time: 873.00
INFO:local_logger:----- Epoch[028/800], Train Loss: 0.7835, time: 871.85
INFO:local_logger:Now training epoch 29. LR=0.000109
INFO:local_logger:Now training epoch 29. LR=0.000109
INFO:local_logger:----- Epoch[028/800], Train Loss: 0.7834, time: 871.89
INFO:local_logger:Now training epoch 29. LR=0.000109
INFO:local_logger:----- Epoch[028/800], Train Loss: 0.7837, time: 873.03
INFO:local_logger:----- Epoch[028/800], Train Loss: 0.7838, time: 872.29
INFO:local_logger:Now training epoch 29. LR=0.000109
INFO:local_logger:Now training epoch 29. LR=0.000109
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-28-Loss-0.783455797444855.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-28-Loss-0.783455797444855.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-28-Loss-0.783455797444855.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-28-Loss-0.783455797444855.pdopt
INFO:local_logger:Now training epoch 29. LR=0.000109
INFO:master_logger:Now training epoch 29. LR=0.000109
INFO:local_logger:Epoch[029/800], Step[0000/0626], Avg Loss: 0.7766
INFO:local_logger:Epoch[029/800], Step[0000/0626], Avg Loss: 0.7720
INFO:master_logger:Epoch[029/800], Step[0000/0626], Avg Loss: 0.7785
INFO:local_logger:Epoch[029/800], Step[0000/0626], Avg Loss: 0.7816
INFO:local_logger:Epoch[029/800], Step[0000/0626], Avg Loss: 0.7862
INFO:local_logger:Epoch[029/800], Step[0000/0626], Avg Loss: 0.7662
INFO:local_logger:Epoch[029/800], Step[0000/0626], Avg Loss: 0.7767
INFO:local_logger:Epoch[029/800], Step[0000/0626], Avg Loss: 0.7916
INFO:local_logger:Epoch[029/800], Step[0000/0626], Avg Loss: 0.7771
INFO:local_logger:Epoch[029/800], Step[0100/0626], Avg Loss: 0.7823
INFO:local_logger:Epoch[029/800], Step[0100/0626], Avg Loss: 0.7822
INFO:local_logger:Epoch[029/800], Step[0100/0626], Avg Loss: 0.7822
INFO:local_logger:Epoch[029/800], Step[0100/0626], Avg Loss: 0.7816
INFO:local_logger:Epoch[029/800], Step[0100/0626], Avg Loss: 0.7818
INFO:local_logger:Epoch[029/800], Step[0100/0626], Avg Loss: 0.7815
INFO:local_logger:Epoch[029/800], Step[0100/0626], Avg Loss: 0.7809
INFO:master_logger:Epoch[029/800], Step[0100/0626], Avg Loss: 0.7818
INFO:local_logger:Epoch[029/800], Step[0100/0626], Avg Loss: 0.7822
INFO:local_logger:Epoch[029/800], Step[0200/0626], Avg Loss: 0.7802
INFO:local_logger:Epoch[029/800], Step[0200/0626], Avg Loss: 0.7802
INFO:local_logger:Epoch[029/800], Step[0200/0626], Avg Loss: 0.7795
INFO:local_logger:Epoch[029/800], Step[0200/0626], Avg Loss: 0.7806
INFO:local_logger:Epoch[029/800], Step[0200/0626], Avg Loss: 0.7806
INFO:local_logger:Epoch[029/800], Step[0200/0626], Avg Loss: 0.7807
INFO:local_logger:Epoch[029/800], Step[0200/0626], Avg Loss: 0.7800
INFO:local_logger:Epoch[029/800], Step[0200/0626], Avg Loss: 0.7800
INFO:master_logger:Epoch[029/800], Step[0200/0626], Avg Loss: 0.7802
INFO:local_logger:Epoch[029/800], Step[0300/0626], Avg Loss: 0.7794
INFO:local_logger:Epoch[029/800], Step[0300/0626], Avg Loss: 0.7794
INFO:local_logger:Epoch[029/800], Step[0300/0626], Avg Loss: 0.7796
INFO:local_logger:Epoch[029/800], Step[0300/0626], Avg Loss: 0.7798
INFO:master_logger:Epoch[029/800], Step[0300/0626], Avg Loss: 0.7795
INFO:local_logger:Epoch[029/800], Step[0300/0626], Avg Loss: 0.7797
INFO:local_logger:Epoch[029/800], Step[0300/0626], Avg Loss: 0.7796
INFO:local_logger:Epoch[029/800], Step[0300/0626], Avg Loss: 0.7797
INFO:local_logger:Epoch[029/800], Step[0300/0626], Avg Loss: 0.7789
INFO:local_logger:Epoch[029/800], Step[0400/0626], Avg Loss: 0.7788
INFO:local_logger:Epoch[029/800], Step[0400/0626], Avg Loss: 0.7781
INFO:local_logger:Epoch[029/800], Step[0400/0626], Avg Loss: 0.7786
INFO:local_logger:Epoch[029/800], Step[0400/0626], Avg Loss: 0.7787
INFO:local_logger:Epoch[029/800], Step[0400/0626], Avg Loss: 0.7785
INFO:local_logger:Epoch[029/800], Step[0400/0626], Avg Loss: 0.7787
INFO:local_logger:Epoch[029/800], Step[0400/0626], Avg Loss: 0.7786
INFO:master_logger:Epoch[029/800], Step[0400/0626], Avg Loss: 0.7786
INFO:local_logger:Epoch[029/800], Step[0400/0626], Avg Loss: 0.7788
INFO:local_logger:Epoch[029/800], Step[0500/0626], Avg Loss: 0.7782
INFO:local_logger:Epoch[029/800], Step[0500/0626], Avg Loss: 0.7776
INFO:local_logger:Epoch[029/800], Step[0500/0626], Avg Loss: 0.7779
INFO:local_logger:Epoch[029/800], Step[0500/0626], Avg Loss: 0.7781
INFO:local_logger:Epoch[029/800], Step[0500/0626], Avg Loss: 0.7782
INFO:local_logger:Epoch[029/800], Step[0500/0626], Avg Loss: 0.7780
INFO:master_logger:Epoch[029/800], Step[0500/0626], Avg Loss: 0.7781
INFO:local_logger:Epoch[029/800], Step[0500/0626], Avg Loss: 0.7781
INFO:local_logger:Epoch[029/800], Step[0500/0626], Avg Loss: 0.7782
INFO:local_logger:Epoch[029/800], Step[0600/0626], Avg Loss: 0.7776
INFO:local_logger:Epoch[029/800], Step[0600/0626], Avg Loss: 0.7777
INFO:local_logger:Epoch[029/800], Step[0600/0626], Avg Loss: 0.7777
INFO:local_logger:Epoch[029/800], Step[0600/0626], Avg Loss: 0.7778
INFO:local_logger:Epoch[029/800], Step[0600/0626], Avg Loss: 0.7776
INFO:local_logger:Epoch[029/800], Step[0600/0626], Avg Loss: 0.7776
INFO:master_logger:Epoch[029/800], Step[0600/0626], Avg Loss: 0.7776
INFO:local_logger:Epoch[029/800], Step[0600/0626], Avg Loss: 0.7772
INFO:local_logger:Epoch[029/800], Step[0600/0626], Avg Loss: 0.7776
INFO:local_logger:----- Epoch[029/800], Train Loss: 0.7778, time: 869.47
INFO:local_logger:Now training epoch 30. LR=0.000113
INFO:local_logger:----- Epoch[029/800], Train Loss: 0.7776, time: 865.73
INFO:local_logger:----- Epoch[029/800], Train Loss: 0.7773, time: 868.98
INFO:master_logger:----- Epoch[029/800], Train Loss: 0.7776, time: 865.73
INFO:local_logger:Now training epoch 30. LR=0.000113
INFO:local_logger:----- Epoch[029/800], Train Loss: 0.7777, time: 869.01
INFO:local_logger:Now training epoch 30. LR=0.000113
INFO:local_logger:----- Epoch[029/800], Train Loss: 0.7777, time: 869.04
INFO:local_logger:Now training epoch 30. LR=0.000113
INFO:local_logger:----- Epoch[029/800], Train Loss: 0.7774, time: 869.04
INFO:local_logger:Now training epoch 30. LR=0.000113
INFO:local_logger:----- Epoch[029/800], Train Loss: 0.7776, time: 869.04
INFO:local_logger:Now training epoch 30. LR=0.000113
INFO:local_logger:----- Epoch[029/800], Train Loss: 0.7776, time: 869.04
INFO:local_logger:Now training epoch 30. LR=0.000113
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-29-Loss-0.77762673581754.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-29-Loss-0.77762673581754.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-29-Loss-0.77762673581754.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-29-Loss-0.77762673581754.pdopt
INFO:local_logger:Now training epoch 30. LR=0.000113
INFO:master_logger:Now training epoch 30. LR=0.000113
INFO:local_logger:Epoch[030/800], Step[0000/0626], Avg Loss: 0.7671
INFO:master_logger:Epoch[030/800], Step[0000/0626], Avg Loss: 0.7736
INFO:local_logger:Epoch[030/800], Step[0000/0626], Avg Loss: 0.7764
INFO:local_logger:Epoch[030/800], Step[0000/0626], Avg Loss: 0.7666
INFO:local_logger:Epoch[030/800], Step[0000/0626], Avg Loss: 0.7815
INFO:local_logger:Epoch[030/800], Step[0000/0626], Avg Loss: 0.7786
INFO:local_logger:Epoch[030/800], Step[0000/0626], Avg Loss: 0.7819
INFO:local_logger:Epoch[030/800], Step[0000/0626], Avg Loss: 0.7683
INFO:local_logger:Epoch[030/800], Step[0000/0626], Avg Loss: 0.7686
INFO:local_logger:Epoch[030/800], Step[0100/0626], Avg Loss: 0.7759
INFO:local_logger:Epoch[030/800], Step[0100/0626], Avg Loss: 0.7765
INFO:local_logger:Epoch[030/800], Step[0100/0626], Avg Loss: 0.7745
INFO:local_logger:Epoch[030/800], Step[0100/0626], Avg Loss: 0.7773
INFO:local_logger:Epoch[030/800], Step[0100/0626], Avg Loss: 0.7759
INFO:local_logger:Epoch[030/800], Step[0100/0626], Avg Loss: 0.7772
INFO:local_logger:Epoch[030/800], Step[0100/0626], Avg Loss: 0.7763
INFO:master_logger:Epoch[030/800], Step[0100/0626], Avg Loss: 0.7762
INFO:local_logger:Epoch[030/800], Step[0100/0626], Avg Loss: 0.7756
INFO:local_logger:Epoch[030/800], Step[0200/0626], Avg Loss: 0.7744
INFO:local_logger:Epoch[030/800], Step[0200/0626], Avg Loss: 0.7748
INFO:local_logger:Epoch[030/800], Step[0200/0626], Avg Loss: 0.7754
INFO:local_logger:Epoch[030/800], Step[0200/0626], Avg Loss: 0.7743
INFO:local_logger:Epoch[030/800], Step[0200/0626], Avg Loss: 0.7742
INFO:local_logger:Epoch[030/800], Step[0200/0626], Avg Loss: 0.7749
INFO:master_logger:Epoch[030/800], Step[0200/0626], Avg Loss: 0.7747
INFO:local_logger:Epoch[030/800], Step[0200/0626], Avg Loss: 0.7747
INFO:local_logger:Epoch[030/800], Step[0200/0626], Avg Loss: 0.7747
INFO:local_logger:Epoch[030/800], Step[0300/0626], Avg Loss: 0.7740
INFO:local_logger:Epoch[030/800], Step[0300/0626], Avg Loss: 0.7748
INFO:local_logger:Epoch[030/800], Step[0300/0626], Avg Loss: 0.7739
INFO:local_logger:Epoch[030/800], Step[0300/0626], Avg Loss: 0.7744
INFO:local_logger:Epoch[030/800], Step[0300/0626], Avg Loss: 0.7741
INFO:local_logger:Epoch[030/800], Step[0300/0626], Avg Loss: 0.7739
INFO:master_logger:Epoch[030/800], Step[0300/0626], Avg Loss: 0.7741
INFO:local_logger:Epoch[030/800], Step[0300/0626], Avg Loss: 0.7744
INFO:local_logger:Epoch[030/800], Step[0300/0626], Avg Loss: 0.7736
INFO:local_logger:Epoch[030/800], Step[0400/0626], Avg Loss: 0.7737
INFO:local_logger:Epoch[030/800], Step[0400/0626], Avg Loss: 0.7736
INFO:local_logger:Epoch[030/800], Step[0400/0626], Avg Loss: 0.7732
INFO:local_logger:Epoch[030/800], Step[0400/0626], Avg Loss: 0.7735
INFO:local_logger:Epoch[030/800], Step[0400/0626], Avg Loss: 0.7740
INFO:local_logger:Epoch[030/800], Step[0400/0626], Avg Loss: 0.7732
INFO:local_logger:Epoch[030/800], Step[0400/0626], Avg Loss: 0.7734
INFO:local_logger:Epoch[030/800], Step[0400/0626], Avg Loss: 0.7732
INFO:master_logger:Epoch[030/800], Step[0400/0626], Avg Loss: 0.7735
INFO:local_logger:Epoch[030/800], Step[0500/0626], Avg Loss: 0.7730
INFO:local_logger:Epoch[030/800], Step[0500/0626], Avg Loss: 0.7729
INFO:local_logger:Epoch[030/800], Step[0500/0626], Avg Loss: 0.7729
INFO:local_logger:Epoch[030/800], Step[0500/0626], Avg Loss: 0.7732
INFO:local_logger:Epoch[030/800], Step[0500/0626], Avg Loss: 0.7731
INFO:local_logger:Epoch[030/800], Step[0500/0626], Avg Loss: 0.7727
INFO:local_logger:Epoch[030/800], Step[0500/0626], Avg Loss: 0.7729
INFO:local_logger:Epoch[030/800], Step[0500/0626], Avg Loss: 0.7725
INFO:master_logger:Epoch[030/800], Step[0500/0626], Avg Loss: 0.7729
INFO:local_logger:Epoch[030/800], Step[0600/0626], Avg Loss: 0.7723
INFO:local_logger:Epoch[030/800], Step[0600/0626], Avg Loss: 0.7723
INFO:local_logger:Epoch[030/800], Step[0600/0626], Avg Loss: 0.7722
INFO:local_logger:Epoch[030/800], Step[0600/0626], Avg Loss: 0.7720
INFO:local_logger:Epoch[030/800], Step[0600/0626], Avg Loss: 0.7726
INFO:local_logger:Epoch[030/800], Step[0600/0626], Avg Loss: 0.7724
INFO:master_logger:Epoch[030/800], Step[0600/0626], Avg Loss: 0.7722
INFO:local_logger:Epoch[030/800], Step[0600/0626], Avg Loss: 0.7723
INFO:local_logger:Epoch[030/800], Step[0600/0626], Avg Loss: 0.7719
INFO:local_logger:----- Epoch[030/800], Train Loss: 0.7722, time: 884.56
INFO:local_logger:Now training epoch 31. LR=0.000116
INFO:local_logger:----- Epoch[030/800], Train Loss: 0.7718, time: 884.68
INFO:local_logger:Now training epoch 31. LR=0.000116
INFO:local_logger:----- Epoch[030/800], Train Loss: 0.7719, time: 881.45
INFO:master_logger:----- Epoch[030/800], Train Loss: 0.7721, time: 881.45
INFO:local_logger:----- Epoch[030/800], Train Loss: 0.7721, time: 885.20
INFO:local_logger:Now training epoch 31. LR=0.000116
INFO:local_logger:----- Epoch[030/800], Train Loss: 0.7721, time: 885.20
INFO:local_logger:----- Epoch[030/800], Train Loss: 0.7722, time: 885.14
INFO:local_logger:Now training epoch 31. LR=0.000116
INFO:local_logger:Now training epoch 31. LR=0.000116
INFO:local_logger:----- Epoch[030/800], Train Loss: 0.7724, time: 885.17
INFO:local_logger:Now training epoch 31. LR=0.000116
INFO:local_logger:----- Epoch[030/800], Train Loss: 0.7723, time: 885.20
INFO:local_logger:Now training epoch 31. LR=0.000116
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-30-Loss-0.7718740027551073.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-30-Loss-0.7718740027551073.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-30-Loss-0.7718740027551073.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-30-Loss-0.7718740027551073.pdopt
INFO:local_logger:Now training epoch 31. LR=0.000116
INFO:master_logger:Now training epoch 31. LR=0.000116
INFO:local_logger:Epoch[031/800], Step[0000/0626], Avg Loss: 0.7753
INFO:local_logger:Epoch[031/800], Step[0000/0626], Avg Loss: 0.7631
INFO:local_logger:Epoch[031/800], Step[0000/0626], Avg Loss: 0.7707
INFO:local_logger:Epoch[031/800], Step[0000/0626], Avg Loss: 0.7713
INFO:local_logger:Epoch[031/800], Step[0000/0626], Avg Loss: 0.7666
INFO:local_logger:Epoch[031/800], Step[0000/0626], Avg Loss: 0.7545
INFO:master_logger:Epoch[031/800], Step[0000/0626], Avg Loss: 0.7651
INFO:local_logger:Epoch[031/800], Step[0000/0626], Avg Loss: 0.7590
INFO:local_logger:Epoch[031/800], Step[0000/0626], Avg Loss: 0.7607
INFO:local_logger:Epoch[031/800], Step[0100/0626], Avg Loss: 0.7683
INFO:master_logger:Epoch[031/800], Step[0100/0626], Avg Loss: 0.7684
INFO:local_logger:Epoch[031/800], Step[0100/0626], Avg Loss: 0.7667
INFO:local_logger:Epoch[031/800], Step[0100/0626], Avg Loss: 0.7685
INFO:local_logger:Epoch[031/800], Step[0100/0626], Avg Loss: 0.7688
INFO:local_logger:Epoch[031/800], Step[0100/0626], Avg Loss: 0.7677
INFO:local_logger:Epoch[031/800], Step[0100/0626], Avg Loss: 0.7694
INFO:local_logger:Epoch[031/800], Step[0100/0626], Avg Loss: 0.7688
INFO:local_logger:Epoch[031/800], Step[0100/0626], Avg Loss: 0.7686
INFO:local_logger:Epoch[031/800], Step[0200/0626], Avg Loss: 0.7679
INFO:local_logger:Epoch[031/800], Step[0200/0626], Avg Loss: 0.7681
INFO:local_logger:Epoch[031/800], Step[0200/0626], Avg Loss: 0.7678
INFO:local_logger:Epoch[031/800], Step[0200/0626], Avg Loss: 0.7680
INFO:master_logger:Epoch[031/800], Step[0200/0626], Avg Loss: 0.7680
INFO:local_logger:Epoch[031/800], Step[0200/0626], Avg Loss: 0.7683
INFO:local_logger:Epoch[031/800], Step[0200/0626], Avg Loss: 0.7685
INFO:local_logger:Epoch[031/800], Step[0200/0626], Avg Loss: 0.7681
INFO:local_logger:Epoch[031/800], Step[0200/0626], Avg Loss: 0.7672
INFO:local_logger:Epoch[031/800], Step[0300/0626], Avg Loss: 0.7678
INFO:local_logger:Epoch[031/800], Step[0300/0626], Avg Loss: 0.7684
INFO:local_logger:Epoch[031/800], Step[0300/0626], Avg Loss: 0.7676
INFO:local_logger:Epoch[031/800], Step[0300/0626], Avg Loss: 0.7680
INFO:local_logger:Epoch[031/800], Step[0300/0626], Avg Loss: 0.7676
INFO:master_logger:Epoch[031/800], Step[0300/0626], Avg Loss: 0.7677
INFO:local_logger:Epoch[031/800], Step[0300/0626], Avg Loss: 0.7668
INFO:local_logger:Epoch[031/800], Step[0300/0626], Avg Loss: 0.7674
INFO:local_logger:Epoch[031/800], Step[0300/0626], Avg Loss: 0.7679
INFO:local_logger:Epoch[031/800], Step[0400/0626], Avg Loss: 0.7667
INFO:local_logger:Epoch[031/800], Step[0400/0626], Avg Loss: 0.7676
INFO:local_logger:Epoch[031/800], Step[0400/0626], Avg Loss: 0.7674
INFO:local_logger:Epoch[031/800], Step[0400/0626], Avg Loss: 0.7676
INFO:local_logger:Epoch[031/800], Step[0400/0626], Avg Loss: 0.7675
INFO:local_logger:Epoch[031/800], Step[0400/0626], Avg Loss: 0.7678
INFO:local_logger:Epoch[031/800], Step[0400/0626], Avg Loss: 0.7683
INFO:local_logger:Epoch[031/800], Step[0400/0626], Avg Loss: 0.7678
INFO:master_logger:Epoch[031/800], Step[0400/0626], Avg Loss: 0.7676
INFO:local_logger:Epoch[031/800], Step[0500/0626], Avg Loss: 0.7669
INFO:local_logger:Epoch[031/800], Step[0500/0626], Avg Loss: 0.7673
INFO:local_logger:Epoch[031/800], Step[0500/0626], Avg Loss: 0.7674
INFO:local_logger:Epoch[031/800], Step[0500/0626], Avg Loss: 0.7670
INFO:local_logger:Epoch[031/800], Step[0500/0626], Avg Loss: 0.7672
INFO:master_logger:Epoch[031/800], Step[0500/0626], Avg Loss: 0.7671
INFO:local_logger:Epoch[031/800], Step[0500/0626], Avg Loss: 0.7663
INFO:local_logger:Epoch[031/800], Step[0500/0626], Avg Loss: 0.7671
INFO:local_logger:Epoch[031/800], Step[0500/0626], Avg Loss: 0.7676
INFO:local_logger:Epoch[031/800], Step[0600/0626], Avg Loss: 0.7669
INFO:local_logger:Epoch[031/800], Step[0600/0626], Avg Loss: 0.7668
INFO:local_logger:Epoch[031/800], Step[0600/0626], Avg Loss: 0.7664
INFO:local_logger:Epoch[031/800], Step[0600/0626], Avg Loss: 0.7667
INFO:local_logger:Epoch[031/800], Step[0600/0626], Avg Loss: 0.7661
INFO:local_logger:Epoch[031/800], Step[0600/0626], Avg Loss: 0.7667
INFO:master_logger:Epoch[031/800], Step[0600/0626], Avg Loss: 0.7667
INFO:local_logger:Epoch[031/800], Step[0600/0626], Avg Loss: 0.7670
INFO:local_logger:Epoch[031/800], Step[0600/0626], Avg Loss: 0.7669
INFO:local_logger:----- Epoch[031/800], Train Loss: 0.7663, time: 868.34
INFO:local_logger:Now training epoch 32. LR=0.000120
INFO:local_logger:----- Epoch[031/800], Train Loss: 0.7660, time: 868.35
INFO:local_logger:Now training epoch 32. LR=0.000120
INFO:local_logger:----- Epoch[031/800], Train Loss: 0.7666, time: 868.93
INFO:local_logger:Now training epoch 32. LR=0.000120
INFO:local_logger:----- Epoch[031/800], Train Loss: 0.7667, time: 868.45
INFO:local_logger:Now training epoch 32. LR=0.000120
INFO:local_logger:----- Epoch[031/800], Train Loss: 0.7668, time: 868.50
INFO:local_logger:Now training epoch 32. LR=0.000120
INFO:local_logger:----- Epoch[031/800], Train Loss: 0.7666, time: 864.87
INFO:master_logger:----- Epoch[031/800], Train Loss: 0.7665, time: 864.87
INFO:local_logger:----- Epoch[031/800], Train Loss: 0.7668, time: 869.11
INFO:local_logger:Now training epoch 32. LR=0.000120
INFO:local_logger:----- Epoch[031/800], Train Loss: 0.7665, time: 868.66
INFO:local_logger:Now training epoch 32. LR=0.000120
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-31-Loss-0.7666413477179265.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-31-Loss-0.7666413477179265.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-31-Loss-0.7666413477179265.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-31-Loss-0.7666413477179265.pdopt
INFO:local_logger:Now training epoch 32. LR=0.000120
INFO:master_logger:Now training epoch 32. LR=0.000120
INFO:local_logger:Epoch[032/800], Step[0000/0626], Avg Loss: 0.7643
INFO:local_logger:Epoch[032/800], Step[0000/0626], Avg Loss: 0.7629
INFO:local_logger:Epoch[032/800], Step[0000/0626], Avg Loss: 0.7742
INFO:master_logger:Epoch[032/800], Step[0000/0626], Avg Loss: 0.7666
INFO:local_logger:Epoch[032/800], Step[0000/0626], Avg Loss: 0.7638
INFO:local_logger:Epoch[032/800], Step[0000/0626], Avg Loss: 0.7575
INFO:local_logger:Epoch[032/800], Step[0000/0626], Avg Loss: 0.7728
INFO:local_logger:Epoch[032/800], Step[0000/0626], Avg Loss: 0.7710
INFO:local_logger:Epoch[032/800], Step[0000/0626], Avg Loss: 0.7662
INFO:local_logger:Epoch[032/800], Step[0100/0626], Avg Loss: 0.7641
INFO:local_logger:Epoch[032/800], Step[0100/0626], Avg Loss: 0.7647
INFO:local_logger:Epoch[032/800], Step[0100/0626], Avg Loss: 0.7646
INFO:master_logger:Epoch[032/800], Step[0100/0626], Avg Loss: 0.7645
INFO:local_logger:Epoch[032/800], Step[0100/0626], Avg Loss: 0.7645
INFO:local_logger:Epoch[032/800], Step[0100/0626], Avg Loss: 0.7638
INFO:local_logger:Epoch[032/800], Step[0100/0626], Avg Loss: 0.7636
INFO:local_logger:Epoch[032/800], Step[0100/0626], Avg Loss: 0.7646
INFO:local_logger:Epoch[032/800], Step[0100/0626], Avg Loss: 0.7657
INFO:local_logger:Epoch[032/800], Step[0200/0626], Avg Loss: 0.7632
INFO:local_logger:Epoch[032/800], Step[0200/0626], Avg Loss: 0.7627
INFO:local_logger:Epoch[032/800], Step[0200/0626], Avg Loss: 0.7635
INFO:local_logger:Epoch[032/800], Step[0200/0626], Avg Loss: 0.7633
INFO:local_logger:Epoch[032/800], Step[0200/0626], Avg Loss: 0.7638
INFO:master_logger:Epoch[032/800], Step[0200/0626], Avg Loss: 0.7635
INFO:local_logger:Epoch[032/800], Step[0200/0626], Avg Loss: 0.7637
INFO:local_logger:Epoch[032/800], Step[0200/0626], Avg Loss: 0.7629
INFO:local_logger:Epoch[032/800], Step[0200/0626], Avg Loss: 0.7644
INFO:local_logger:Epoch[032/800], Step[0300/0626], Avg Loss: 0.7629
INFO:local_logger:Epoch[032/800], Step[0300/0626], Avg Loss: 0.7628
INFO:local_logger:Epoch[032/800], Step[0300/0626], Avg Loss: 0.7626
INFO:local_logger:Epoch[032/800], Step[0300/0626], Avg Loss: 0.7631
INFO:local_logger:Epoch[032/800], Step[0300/0626], Avg Loss: 0.7633
INFO:local_logger:Epoch[032/800], Step[0300/0626], Avg Loss: 0.7634
INFO:master_logger:Epoch[032/800], Step[0300/0626], Avg Loss: 0.7629
INFO:local_logger:Epoch[032/800], Step[0300/0626], Avg Loss: 0.7626
INFO:local_logger:Epoch[032/800], Step[0300/0626], Avg Loss: 0.7628
INFO:local_logger:Epoch[032/800], Step[0400/0626], Avg Loss: 0.7623
INFO:local_logger:Epoch[032/800], Step[0400/0626], Avg Loss: 0.7625
INFO:local_logger:Epoch[032/800], Step[0400/0626], Avg Loss: 0.7620
INFO:local_logger:Epoch[032/800], Step[0400/0626], Avg Loss: 0.7630
INFO:local_logger:Epoch[032/800], Step[0400/0626], Avg Loss: 0.7621
INFO:local_logger:Epoch[032/800], Step[0400/0626], Avg Loss: 0.7628
INFO:master_logger:Epoch[032/800], Step[0400/0626], Avg Loss: 0.7624
INFO:local_logger:Epoch[032/800], Step[0400/0626], Avg Loss: 0.7622
INFO:local_logger:Epoch[032/800], Step[0400/0626], Avg Loss: 0.7621
INFO:local_logger:Epoch[032/800], Step[0500/0626], Avg Loss: 0.7617
INFO:local_logger:Epoch[032/800], Step[0500/0626], Avg Loss: 0.7623
INFO:local_logger:Epoch[032/800], Step[0500/0626], Avg Loss: 0.7623
INFO:master_logger:Epoch[032/800], Step[0500/0626], Avg Loss: 0.7619
INFO:local_logger:Epoch[032/800], Step[0500/0626], Avg Loss: 0.7618
INFO:local_logger:Epoch[032/800], Step[0500/0626], Avg Loss: 0.7617
INFO:local_logger:Epoch[032/800], Step[0500/0626], Avg Loss: 0.7619
INFO:local_logger:Epoch[032/800], Step[0500/0626], Avg Loss: 0.7617
INFO:local_logger:Epoch[032/800], Step[0500/0626], Avg Loss: 0.7617
INFO:local_logger:Epoch[032/800], Step[0600/0626], Avg Loss: 0.7613
INFO:local_logger:Epoch[032/800], Step[0600/0626], Avg Loss: 0.7613
INFO:local_logger:Epoch[032/800], Step[0600/0626], Avg Loss: 0.7616
INFO:local_logger:Epoch[032/800], Step[0600/0626], Avg Loss: 0.7612
INFO:local_logger:Epoch[032/800], Step[0600/0626], Avg Loss: 0.7615
INFO:local_logger:Epoch[032/800], Step[0600/0626], Avg Loss: 0.7612
INFO:master_logger:Epoch[032/800], Step[0600/0626], Avg Loss: 0.7614
INFO:local_logger:Epoch[032/800], Step[0600/0626], Avg Loss: 0.7616
INFO:local_logger:Epoch[032/800], Step[0600/0626], Avg Loss: 0.7611
INFO:local_logger:----- Epoch[032/800], Train Loss: 0.7611, time: 877.39
INFO:local_logger:Now training epoch 33. LR=0.000124
INFO:local_logger:----- Epoch[032/800], Train Loss: 0.7610, time: 878.25
INFO:local_logger:Now training epoch 33. LR=0.000124
INFO:local_logger:----- Epoch[032/800], Train Loss: 0.7609, time: 878.35
INFO:local_logger:Now training epoch 33. LR=0.000124
INFO:local_logger:----- Epoch[032/800], Train Loss: 0.7615, time: 874.33
INFO:master_logger:----- Epoch[032/800], Train Loss: 0.7612, time: 874.33
INFO:local_logger:----- Epoch[032/800], Train Loss: 0.7612, time: 878.35
INFO:local_logger:Now training epoch 33. LR=0.000124
INFO:local_logger:----- Epoch[032/800], Train Loss: 0.7612, time: 878.38
INFO:local_logger:Now training epoch 33. LR=0.000124
INFO:local_logger:----- Epoch[032/800], Train Loss: 0.7612, time: 878.23
INFO:local_logger:Now training epoch 33. LR=0.000124
INFO:local_logger:----- Epoch[032/800], Train Loss: 0.7615, time: 878.10
INFO:local_logger:Now training epoch 33. LR=0.000124
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-32-Loss-0.761453199911086.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-32-Loss-0.761453199911086.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-32-Loss-0.761453199911086.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-32-Loss-0.761453199911086.pdopt
INFO:local_logger:Now training epoch 33. LR=0.000124
INFO:master_logger:Now training epoch 33. LR=0.000124
INFO:local_logger:Epoch[033/800], Step[0000/0626], Avg Loss: 0.7557
INFO:local_logger:Epoch[033/800], Step[0000/0626], Avg Loss: 0.7650
INFO:local_logger:Epoch[033/800], Step[0000/0626], Avg Loss: 0.7551
INFO:master_logger:Epoch[033/800], Step[0000/0626], Avg Loss: 0.7556
INFO:local_logger:Epoch[033/800], Step[0000/0626], Avg Loss: 0.7455
INFO:local_logger:Epoch[033/800], Step[0000/0626], Avg Loss: 0.7599
INFO:local_logger:Epoch[033/800], Step[0000/0626], Avg Loss: 0.7565
INFO:local_logger:Epoch[033/800], Step[0000/0626], Avg Loss: 0.7625
INFO:local_logger:Epoch[033/800], Step[0000/0626], Avg Loss: 0.7448
INFO:local_logger:Epoch[033/800], Step[0100/0626], Avg Loss: 0.7586
INFO:local_logger:Epoch[033/800], Step[0100/0626], Avg Loss: 0.7590
INFO:local_logger:Epoch[033/800], Step[0100/0626], Avg Loss: 0.7584
INFO:local_logger:Epoch[033/800], Step[0100/0626], Avg Loss: 0.7588
INFO:local_logger:Epoch[033/800], Step[0100/0626], Avg Loss: 0.7580
INFO:local_logger:Epoch[033/800], Step[0100/0626], Avg Loss: 0.7570
INFO:local_logger:Epoch[033/800], Step[0100/0626], Avg Loss: 0.7575
INFO:local_logger:Epoch[033/800], Step[0100/0626], Avg Loss: 0.7570
INFO:master_logger:Epoch[033/800], Step[0100/0626], Avg Loss: 0.7580
INFO:local_logger:Epoch[033/800], Step[0200/0626], Avg Loss: 0.7577
INFO:local_logger:Epoch[033/800], Step[0200/0626], Avg Loss: 0.7575
INFO:local_logger:Epoch[033/800], Step[0200/0626], Avg Loss: 0.7584
INFO:local_logger:Epoch[033/800], Step[0200/0626], Avg Loss: 0.7580
INFO:local_logger:Epoch[033/800], Step[0200/0626], Avg Loss: 0.7584
INFO:local_logger:Epoch[033/800], Step[0200/0626], Avg Loss: 0.7571
INFO:local_logger:Epoch[033/800], Step[0200/0626], Avg Loss: 0.7574
INFO:master_logger:Epoch[033/800], Step[0200/0626], Avg Loss: 0.7578
INFO:local_logger:Epoch[033/800], Step[0200/0626], Avg Loss: 0.7576
INFO:local_logger:Epoch[033/800], Step[0300/0626], Avg Loss: 0.7572
INFO:local_logger:Epoch[033/800], Step[0300/0626], Avg Loss: 0.7579
INFO:local_logger:Epoch[033/800], Step[0300/0626], Avg Loss: 0.7576
INFO:local_logger:Epoch[033/800], Step[0300/0626], Avg Loss: 0.7568
INFO:local_logger:Epoch[033/800], Step[0300/0626], Avg Loss: 0.7577
INFO:local_logger:Epoch[033/800], Step[0300/0626], Avg Loss: 0.7579
INFO:master_logger:Epoch[033/800], Step[0300/0626], Avg Loss: 0.7575
INFO:local_logger:Epoch[033/800], Step[0300/0626], Avg Loss: 0.7579
INFO:local_logger:Epoch[033/800], Step[0300/0626], Avg Loss: 0.7572
INFO:local_logger:Epoch[033/800], Step[0400/0626], Avg Loss: 0.7574
INFO:local_logger:Epoch[033/800], Step[0400/0626], Avg Loss: 0.7571
INFO:local_logger:Epoch[033/800], Step[0400/0626], Avg Loss: 0.7568
INFO:local_logger:Epoch[033/800], Step[0400/0626], Avg Loss: 0.7573
INFO:local_logger:Epoch[033/800], Step[0400/0626], Avg Loss: 0.7571
INFO:local_logger:Epoch[033/800], Step[0400/0626], Avg Loss: 0.7573
INFO:local_logger:Epoch[033/800], Step[0400/0626], Avg Loss: 0.7572
INFO:local_logger:Epoch[033/800], Step[0400/0626], Avg Loss: 0.7565
INFO:master_logger:Epoch[033/800], Step[0400/0626], Avg Loss: 0.7571
INFO:local_logger:Epoch[033/800], Step[0500/0626], Avg Loss: 0.7565
INFO:local_logger:Epoch[033/800], Step[0500/0626], Avg Loss: 0.7567
INFO:local_logger:Epoch[033/800], Step[0500/0626], Avg Loss: 0.7570
INFO:local_logger:Epoch[033/800], Step[0500/0626], Avg Loss: 0.7569
INFO:local_logger:Epoch[033/800], Step[0500/0626], Avg Loss: 0.7564
INFO:local_logger:Epoch[033/800], Step[0500/0626], Avg Loss: 0.7570
INFO:local_logger:Epoch[033/800], Step[0500/0626], Avg Loss: 0.7569
INFO:master_logger:Epoch[033/800], Step[0500/0626], Avg Loss: 0.7568
INFO:local_logger:Epoch[033/800], Step[0500/0626], Avg Loss: 0.7569
INFO:local_logger:Epoch[033/800], Step[0600/0626], Avg Loss: 0.7564
INFO:local_logger:Epoch[033/800], Step[0600/0626], Avg Loss: 0.7561
INFO:local_logger:Epoch[033/800], Step[0600/0626], Avg Loss: 0.7567
INFO:local_logger:Epoch[033/800], Step[0600/0626], Avg Loss: 0.7561
INFO:local_logger:Epoch[033/800], Step[0600/0626], Avg Loss: 0.7566
INFO:local_logger:Epoch[033/800], Step[0600/0626], Avg Loss: 0.7567
INFO:master_logger:Epoch[033/800], Step[0600/0626], Avg Loss: 0.7564
INFO:local_logger:Epoch[033/800], Step[0600/0626], Avg Loss: 0.7563
INFO:local_logger:Epoch[033/800], Step[0600/0626], Avg Loss: 0.7566
INFO:local_logger:----- Epoch[033/800], Train Loss: 0.7561, time: 852.50
INFO:master_logger:----- Epoch[033/800], Train Loss: 0.7564, time: 852.50
INFO:local_logger:----- Epoch[033/800], Train Loss: 0.7566, time: 856.70
INFO:local_logger:Now training epoch 34. LR=0.000128
INFO:local_logger:----- Epoch[033/800], Train Loss: 0.7566, time: 856.72
INFO:local_logger:Now training epoch 34. LR=0.000128
INFO:local_logger:----- Epoch[033/800], Train Loss: 0.7560, time: 856.75
INFO:local_logger:Now training epoch 34. LR=0.000128
INFO:local_logger:----- Epoch[033/800], Train Loss: 0.7566, time: 857.55
INFO:local_logger:Now training epoch 34. LR=0.000128
INFO:local_logger:----- Epoch[033/800], Train Loss: 0.7562, time: 856.91
INFO:local_logger:Now training epoch 34. LR=0.000128
INFO:local_logger:----- Epoch[033/800], Train Loss: 0.7565, time: 856.90
INFO:local_logger:Now training epoch 34. LR=0.000128
INFO:local_logger:----- Epoch[033/800], Train Loss: 0.7563, time: 856.92
INFO:local_logger:Now training epoch 34. LR=0.000128
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-33-Loss-0.7561021761674307.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-33-Loss-0.7561021761674307.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-33-Loss-0.7561021761674307.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-33-Loss-0.7561021761674307.pdopt
INFO:local_logger:Now training epoch 34. LR=0.000128
INFO:master_logger:Now training epoch 34. LR=0.000128
INFO:local_logger:Epoch[034/800], Step[0000/0626], Avg Loss: 0.7407
INFO:master_logger:Epoch[034/800], Step[0000/0626], Avg Loss: 0.7518
INFO:local_logger:Epoch[034/800], Step[0000/0626], Avg Loss: 0.7494
INFO:local_logger:Epoch[034/800], Step[0000/0626], Avg Loss: 0.7433
INFO:local_logger:Epoch[034/800], Step[0000/0626], Avg Loss: 0.7598
INFO:local_logger:Epoch[034/800], Step[0000/0626], Avg Loss: 0.7563
INFO:local_logger:Epoch[034/800], Step[0000/0626], Avg Loss: 0.7612
INFO:local_logger:Epoch[034/800], Step[0000/0626], Avg Loss: 0.7477
INFO:local_logger:Epoch[034/800], Step[0000/0626], Avg Loss: 0.7559
INFO:local_logger:Epoch[034/800], Step[0100/0626], Avg Loss: 0.7547
INFO:local_logger:Epoch[034/800], Step[0100/0626], Avg Loss: 0.7548
INFO:local_logger:Epoch[034/800], Step[0100/0626], Avg Loss: 0.7543
INFO:local_logger:Epoch[034/800], Step[0100/0626], Avg Loss: 0.7539
INFO:local_logger:Epoch[034/800], Step[0100/0626], Avg Loss: 0.7534
INFO:local_logger:Epoch[034/800], Step[0100/0626], Avg Loss: 0.7536
INFO:master_logger:Epoch[034/800], Step[0100/0626], Avg Loss: 0.7540
INFO:local_logger:Epoch[034/800], Step[0100/0626], Avg Loss: 0.7534
INFO:local_logger:Epoch[034/800], Step[0100/0626], Avg Loss: 0.7540
INFO:local_logger:Epoch[034/800], Step[0200/0626], Avg Loss: 0.7539
INFO:local_logger:Epoch[034/800], Step[0200/0626], Avg Loss: 0.7529
INFO:local_logger:Epoch[034/800], Step[0200/0626], Avg Loss: 0.7541
INFO:local_logger:Epoch[034/800], Step[0200/0626], Avg Loss: 0.7541
INFO:local_logger:Epoch[034/800], Step[0200/0626], Avg Loss: 0.7531
INFO:master_logger:Epoch[034/800], Step[0200/0626], Avg Loss: 0.7536
INFO:local_logger:Epoch[034/800], Step[0200/0626], Avg Loss: 0.7536
INFO:local_logger:Epoch[034/800], Step[0200/0626], Avg Loss: 0.7534
INFO:local_logger:Epoch[034/800], Step[0200/0626], Avg Loss: 0.7533
INFO:local_logger:Epoch[034/800], Step[0300/0626], Avg Loss: 0.7529
INFO:local_logger:Epoch[034/800], Step[0300/0626], Avg Loss: 0.7533
INFO:local_logger:Epoch[034/800], Step[0300/0626], Avg Loss: 0.7532
INFO:local_logger:Epoch[034/800], Step[0300/0626], Avg Loss: 0.7531
INFO:local_logger:Epoch[034/800], Step[0300/0626], Avg Loss: 0.7528
INFO:local_logger:Epoch[034/800], Step[0300/0626], Avg Loss: 0.7534
INFO:master_logger:Epoch[034/800], Step[0300/0626], Avg Loss: 0.7531
INFO:local_logger:Epoch[034/800], Step[0300/0626], Avg Loss: 0.7539
INFO:local_logger:Epoch[034/800], Step[0300/0626], Avg Loss: 0.7523
INFO:local_logger:Epoch[034/800], Step[0400/0626], Avg Loss: 0.7527
INFO:local_logger:Epoch[034/800], Step[0400/0626], Avg Loss: 0.7527
INFO:local_logger:Epoch[034/800], Step[0400/0626], Avg Loss: 0.7526
INFO:local_logger:Epoch[034/800], Step[0400/0626], Avg Loss: 0.7520
INFO:local_logger:Epoch[034/800], Step[0400/0626], Avg Loss: 0.7536
INFO:local_logger:Epoch[034/800], Step[0400/0626], Avg Loss: 0.7525
INFO:local_logger:Epoch[034/800], Step[0400/0626], Avg Loss: 0.7525
INFO:local_logger:Epoch[034/800], Step[0400/0626], Avg Loss: 0.7528
INFO:master_logger:Epoch[034/800], Step[0400/0626], Avg Loss: 0.7527
INFO:local_logger:Epoch[034/800], Step[0500/0626], Avg Loss: 0.7526
INFO:local_logger:Epoch[034/800], Step[0500/0626], Avg Loss: 0.7520
INFO:local_logger:Epoch[034/800], Step[0500/0626], Avg Loss: 0.7518
INFO:local_logger:Epoch[034/800], Step[0500/0626], Avg Loss: 0.7523
INFO:local_logger:Epoch[034/800], Step[0500/0626], Avg Loss: 0.7525
INFO:local_logger:Epoch[034/800], Step[0500/0626], Avg Loss: 0.7524
INFO:local_logger:Epoch[034/800], Step[0500/0626], Avg Loss: 0.7522
INFO:local_logger:Epoch[034/800], Step[0500/0626], Avg Loss: 0.7532
INFO:master_logger:Epoch[034/800], Step[0500/0626], Avg Loss: 0.7524
INFO:local_logger:Epoch[034/800], Step[0600/0626], Avg Loss: 0.7520
INFO:local_logger:Epoch[034/800], Step[0600/0626], Avg Loss: 0.7521
INFO:local_logger:Epoch[034/800], Step[0600/0626], Avg Loss: 0.7516
INFO:local_logger:Epoch[034/800], Step[0600/0626], Avg Loss: 0.7522
INFO:local_logger:Epoch[034/800], Step[0600/0626], Avg Loss: 0.7516
INFO:master_logger:Epoch[034/800], Step[0600/0626], Avg Loss: 0.7520
INFO:local_logger:Epoch[034/800], Step[0600/0626], Avg Loss: 0.7521
INFO:local_logger:Epoch[034/800], Step[0600/0626], Avg Loss: 0.7526
INFO:local_logger:Epoch[034/800], Step[0600/0626], Avg Loss: 0.7516
INFO:local_logger:----- Epoch[034/800], Train Loss: 0.7519, time: 885.18
INFO:local_logger:Now training epoch 35. LR=0.000131
INFO:local_logger:----- Epoch[034/800], Train Loss: 0.7520, time: 885.17
INFO:local_logger:Now training epoch 35. LR=0.000131
INFO:local_logger:----- Epoch[034/800], Train Loss: 0.7519, time: 885.32
INFO:local_logger:Now training epoch 35. LR=0.000131
INFO:local_logger:----- Epoch[034/800], Train Loss: 0.7516, time: 882.34
INFO:master_logger:----- Epoch[034/800], Train Loss: 0.7519, time: 882.34
INFO:local_logger:----- Epoch[034/800], Train Loss: 0.7522, time: 885.50
INFO:local_logger:Now training epoch 35. LR=0.000131
INFO:local_logger:----- Epoch[034/800], Train Loss: 0.7516, time: 885.48
INFO:local_logger:Now training epoch 35. LR=0.000131
INFO:local_logger:----- Epoch[034/800], Train Loss: 0.7516, time: 885.48
INFO:local_logger:Now training epoch 35. LR=0.000131
INFO:local_logger:----- Epoch[034/800], Train Loss: 0.7524, time: 885.48
INFO:local_logger:Now training epoch 35. LR=0.000131
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-34-Loss-0.7516406552539668.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-34-Loss-0.7516406552539668.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-34-Loss-0.7516406552539668.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-34-Loss-0.7516406552539668.pdopt
INFO:local_logger:Now training epoch 35. LR=0.000131
INFO:master_logger:Now training epoch 35. LR=0.000131
INFO:local_logger:Epoch[035/800], Step[0000/0626], Avg Loss: 0.7487
INFO:local_logger:Epoch[035/800], Step[0000/0626], Avg Loss: 0.7414
INFO:local_logger:Epoch[035/800], Step[0000/0626], Avg Loss: 0.7520
INFO:master_logger:Epoch[035/800], Step[0000/0626], Avg Loss: 0.7511
INFO:local_logger:Epoch[035/800], Step[0000/0626], Avg Loss: 0.7453
INFO:local_logger:Epoch[035/800], Step[0000/0626], Avg Loss: 0.7505
INFO:local_logger:Epoch[035/800], Step[0000/0626], Avg Loss: 0.7633
INFO:local_logger:Epoch[035/800], Step[0000/0626], Avg Loss: 0.7544
INFO:local_logger:Epoch[035/800], Step[0000/0626], Avg Loss: 0.7535
INFO:local_logger:Epoch[035/800], Step[0100/0626], Avg Loss: 0.7500
INFO:local_logger:Epoch[035/800], Step[0100/0626], Avg Loss: 0.7504
INFO:local_logger:Epoch[035/800], Step[0100/0626], Avg Loss: 0.7496
INFO:local_logger:Epoch[035/800], Step[0100/0626], Avg Loss: 0.7499
INFO:local_logger:Epoch[035/800], Step[0100/0626], Avg Loss: 0.7493
INFO:local_logger:Epoch[035/800], Step[0100/0626], Avg Loss: 0.7492
INFO:local_logger:Epoch[035/800], Step[0100/0626], Avg Loss: 0.7490
INFO:master_logger:Epoch[035/800], Step[0100/0626], Avg Loss: 0.7494
INFO:local_logger:Epoch[035/800], Step[0100/0626], Avg Loss: 0.7481
INFO:local_logger:Epoch[035/800], Step[0200/0626], Avg Loss: 0.7496
INFO:local_logger:Epoch[035/800], Step[0200/0626], Avg Loss: 0.7496
INFO:local_logger:Epoch[035/800], Step[0200/0626], Avg Loss: 0.7495
INFO:local_logger:Epoch[035/800], Step[0200/0626], Avg Loss: 0.7485
INFO:local_logger:Epoch[035/800], Step[0200/0626], Avg Loss: 0.7487
INFO:master_logger:Epoch[035/800], Step[0200/0626], Avg Loss: 0.7493
INFO:local_logger:Epoch[035/800], Step[0200/0626], Avg Loss: 0.7490
INFO:local_logger:Epoch[035/800], Step[0200/0626], Avg Loss: 0.7498
INFO:local_logger:Epoch[035/800], Step[0200/0626], Avg Loss: 0.7493
INFO:local_logger:Epoch[035/800], Step[0300/0626], Avg Loss: 0.7484
INFO:local_logger:Epoch[035/800], Step[0300/0626], Avg Loss: 0.7486
INFO:local_logger:Epoch[035/800], Step[0300/0626], Avg Loss: 0.7491
INFO:local_logger:Epoch[035/800], Step[0300/0626], Avg Loss: 0.7494
INFO:local_logger:Epoch[035/800], Step[0300/0626], Avg Loss: 0.7494
INFO:master_logger:Epoch[035/800], Step[0300/0626], Avg Loss: 0.7490
INFO:local_logger:Epoch[035/800], Step[0300/0626], Avg Loss: 0.7491
INFO:local_logger:Epoch[035/800], Step[0300/0626], Avg Loss: 0.7488
INFO:local_logger:Epoch[035/800], Step[0300/0626], Avg Loss: 0.7495
INFO:local_logger:Epoch[035/800], Step[0400/0626], Avg Loss: 0.7488
INFO:local_logger:Epoch[035/800], Step[0400/0626], Avg Loss: 0.7490
INFO:local_logger:Epoch[035/800], Step[0400/0626], Avg Loss: 0.7483
INFO:local_logger:Epoch[035/800], Step[0400/0626], Avg Loss: 0.7490
INFO:local_logger:Epoch[035/800], Step[0400/0626], Avg Loss: 0.7484
INFO:local_logger:Epoch[035/800], Step[0400/0626], Avg Loss: 0.7489
INFO:local_logger:Epoch[035/800], Step[0400/0626], Avg Loss: 0.7480
INFO:master_logger:Epoch[035/800], Step[0400/0626], Avg Loss: 0.7486
INFO:local_logger:Epoch[035/800], Step[0400/0626], Avg Loss: 0.7484
INFO:local_logger:Epoch[035/800], Step[0500/0626], Avg Loss: 0.7485
INFO:local_logger:Epoch[035/800], Step[0500/0626], Avg Loss: 0.7488
INFO:local_logger:Epoch[035/800], Step[0500/0626], Avg Loss: 0.7483
INFO:local_logger:Epoch[035/800], Step[0500/0626], Avg Loss: 0.7482
INFO:local_logger:Epoch[035/800], Step[0500/0626], Avg Loss: 0.7479
INFO:local_logger:Epoch[035/800], Step[0500/0626], Avg Loss: 0.7482
INFO:local_logger:Epoch[035/800], Step[0500/0626], Avg Loss: 0.7491
INFO:master_logger:Epoch[035/800], Step[0500/0626], Avg Loss: 0.7484
INFO:local_logger:Epoch[035/800], Step[0500/0626], Avg Loss: 0.7484
INFO:local_logger:Epoch[035/800], Step[0600/0626], Avg Loss: 0.7482
INFO:local_logger:Epoch[035/800], Step[0600/0626], Avg Loss: 0.7485
INFO:local_logger:Epoch[035/800], Step[0600/0626], Avg Loss: 0.7478
INFO:local_logger:Epoch[035/800], Step[0600/0626], Avg Loss: 0.7480
INFO:local_logger:Epoch[035/800], Step[0600/0626], Avg Loss: 0.7480
INFO:local_logger:Epoch[035/800], Step[0600/0626], Avg Loss: 0.7481
INFO:local_logger:Epoch[035/800], Step[0600/0626], Avg Loss: 0.7478
INFO:local_logger:Epoch[035/800], Step[0600/0626], Avg Loss: 0.7485
INFO:master_logger:Epoch[035/800], Step[0600/0626], Avg Loss: 0.7481
INFO:local_logger:----- Epoch[035/800], Train Loss: 0.7484, time: 858.24
INFO:local_logger:Now training epoch 36. LR=0.000135
INFO:local_logger:----- Epoch[035/800], Train Loss: 0.7479, time: 859.58
INFO:local_logger:Now training epoch 36. LR=0.000135
INFO:local_logger:----- Epoch[035/800], Train Loss: 0.7481, time: 859.55
INFO:local_logger:Now training epoch 36. LR=0.000135
INFO:local_logger:----- Epoch[035/800], Train Loss: 0.7480, time: 859.83
INFO:local_logger:Now training epoch 36. LR=0.000135
INFO:local_logger:----- Epoch[035/800], Train Loss: 0.7480, time: 859.45
INFO:local_logger:Now training epoch 36. LR=0.000135
INFO:local_logger:----- Epoch[035/800], Train Loss: 0.7484, time: 859.45
INFO:local_logger:Now training epoch 36. LR=0.000135
INFO:local_logger:----- Epoch[035/800], Train Loss: 0.7477, time: 855.75
INFO:master_logger:----- Epoch[035/800], Train Loss: 0.7480, time: 855.75
INFO:local_logger:----- Epoch[035/800], Train Loss: 0.7478, time: 859.47
INFO:local_logger:Now training epoch 36. LR=0.000135
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-35-Loss-0.7477136553201034.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-35-Loss-0.7477136553201034.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-35-Loss-0.7477136553201034.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-35-Loss-0.7477136553201034.pdopt
INFO:local_logger:Now training epoch 36. LR=0.000135
INFO:master_logger:Now training epoch 36. LR=0.000135
INFO:local_logger:Epoch[036/800], Step[0000/0626], Avg Loss: 0.7546
INFO:local_logger:Epoch[036/800], Step[0000/0626], Avg Loss: 0.7470
INFO:local_logger:Epoch[036/800], Step[0000/0626], Avg Loss: 0.7469
INFO:master_logger:Epoch[036/800], Step[0000/0626], Avg Loss: 0.7497
INFO:local_logger:Epoch[036/800], Step[0000/0626], Avg Loss: 0.7536
INFO:local_logger:Epoch[036/800], Step[0000/0626], Avg Loss: 0.7544
INFO:local_logger:Epoch[036/800], Step[0000/0626], Avg Loss: 0.7537
INFO:local_logger:Epoch[036/800], Step[0000/0626], Avg Loss: 0.7427
INFO:local_logger:Epoch[036/800], Step[0000/0626], Avg Loss: 0.7446
INFO:local_logger:Epoch[036/800], Step[0100/0626], Avg Loss: 0.7474
INFO:local_logger:Epoch[036/800], Step[0100/0626], Avg Loss: 0.7461
INFO:local_logger:Epoch[036/800], Step[0100/0626], Avg Loss: 0.7458
INFO:local_logger:Epoch[036/800], Step[0100/0626], Avg Loss: 0.7467
INFO:local_logger:Epoch[036/800], Step[0100/0626], Avg Loss: 0.7456
INFO:local_logger:Epoch[036/800], Step[0100/0626], Avg Loss: 0.7469
INFO:local_logger:Epoch[036/800], Step[0100/0626], Avg Loss: 0.7457
INFO:master_logger:Epoch[036/800], Step[0100/0626], Avg Loss: 0.7463
INFO:local_logger:Epoch[036/800], Step[0100/0626], Avg Loss: 0.7461
INFO:local_logger:Epoch[036/800], Step[0200/0626], Avg Loss: 0.7459
INFO:local_logger:Epoch[036/800], Step[0200/0626], Avg Loss: 0.7473
INFO:local_logger:Epoch[036/800], Step[0200/0626], Avg Loss: 0.7459
INFO:local_logger:Epoch[036/800], Step[0200/0626], Avg Loss: 0.7461
INFO:local_logger:Epoch[036/800], Step[0200/0626], Avg Loss: 0.7460
INFO:local_logger:Epoch[036/800], Step[0200/0626], Avg Loss: 0.7450
INFO:master_logger:Epoch[036/800], Step[0200/0626], Avg Loss: 0.7460
INFO:local_logger:Epoch[036/800], Step[0200/0626], Avg Loss: 0.7464
INFO:local_logger:Epoch[036/800], Step[0200/0626], Avg Loss: 0.7453
INFO:local_logger:Epoch[036/800], Step[0300/0626], Avg Loss: 0.7464
INFO:local_logger:Epoch[036/800], Step[0300/0626], Avg Loss: 0.7457
INFO:local_logger:Epoch[036/800], Step[0300/0626], Avg Loss: 0.7456
INFO:local_logger:Epoch[036/800], Step[0300/0626], Avg Loss: 0.7461
INFO:local_logger:Epoch[036/800], Step[0300/0626], Avg Loss: 0.7456
INFO:master_logger:Epoch[036/800], Step[0300/0626], Avg Loss: 0.7456
INFO:local_logger:Epoch[036/800], Step[0300/0626], Avg Loss: 0.7451
INFO:local_logger:Epoch[036/800], Step[0300/0626], Avg Loss: 0.7456
INFO:local_logger:Epoch[036/800], Step[0300/0626], Avg Loss: 0.7450
INFO:local_logger:Epoch[036/800], Step[0400/0626], Avg Loss: 0.7453
INFO:local_logger:Epoch[036/800], Step[0400/0626], Avg Loss: 0.7449
INFO:local_logger:Epoch[036/800], Step[0400/0626], Avg Loss: 0.7457
INFO:local_logger:Epoch[036/800], Step[0400/0626], Avg Loss: 0.7456
INFO:local_logger:Epoch[036/800], Step[0400/0626], Avg Loss: 0.7456
INFO:local_logger:Epoch[036/800], Step[0400/0626], Avg Loss: 0.7450
INFO:master_logger:Epoch[036/800], Step[0400/0626], Avg Loss: 0.7453
INFO:local_logger:Epoch[036/800], Step[0400/0626], Avg Loss: 0.7452
INFO:local_logger:Epoch[036/800], Step[0400/0626], Avg Loss: 0.7449
INFO:local_logger:Epoch[036/800], Step[0500/0626], Avg Loss: 0.7452
INFO:local_logger:Epoch[036/800], Step[0500/0626], Avg Loss: 0.7450
INFO:local_logger:Epoch[036/800], Step[0500/0626], Avg Loss: 0.7453
INFO:local_logger:Epoch[036/800], Step[0500/0626], Avg Loss: 0.7449
INFO:local_logger:Epoch[036/800], Step[0500/0626], Avg Loss: 0.7449
INFO:local_logger:Epoch[036/800], Step[0500/0626], Avg Loss: 0.7452
INFO:local_logger:Epoch[036/800], Step[0500/0626], Avg Loss: 0.7448
INFO:local_logger:Epoch[036/800], Step[0500/0626], Avg Loss: 0.7451
INFO:master_logger:Epoch[036/800], Step[0500/0626], Avg Loss: 0.7451
INFO:local_logger:Epoch[036/800], Step[0600/0626], Avg Loss: 0.7446
INFO:local_logger:Epoch[036/800], Step[0600/0626], Avg Loss: 0.7446
INFO:local_logger:Epoch[036/800], Step[0600/0626], Avg Loss: 0.7446
INFO:local_logger:Epoch[036/800], Step[0600/0626], Avg Loss: 0.7449
INFO:local_logger:Epoch[036/800], Step[0600/0626], Avg Loss: 0.7446
INFO:local_logger:Epoch[036/800], Step[0600/0626], Avg Loss: 0.7444
INFO:local_logger:Epoch[036/800], Step[0600/0626], Avg Loss: 0.7450
INFO:local_logger:Epoch[036/800], Step[0600/0626], Avg Loss: 0.7450
INFO:master_logger:Epoch[036/800], Step[0600/0626], Avg Loss: 0.7447
INFO:local_logger:----- Epoch[036/800], Train Loss: 0.7449, time: 890.54
INFO:local_logger:Now training epoch 37. LR=0.000139
INFO:local_logger:----- Epoch[036/800], Train Loss: 0.7445, time: 891.48
INFO:local_logger:----- Epoch[036/800], Train Loss: 0.7445, time: 891.09
INFO:local_logger:Now training epoch 37. LR=0.000139
INFO:local_logger:Now training epoch 37. LR=0.000139
INFO:local_logger:----- Epoch[036/800], Train Loss: 0.7446, time: 887.41
INFO:master_logger:----- Epoch[036/800], Train Loss: 0.7447, time: 887.41
INFO:local_logger:----- Epoch[036/800], Train Loss: 0.7448, time: 891.48
INFO:local_logger:Now training epoch 37. LR=0.000139
INFO:local_logger:----- Epoch[036/800], Train Loss: 0.7449, time: 891.12
INFO:local_logger:Now training epoch 37. LR=0.000139
INFO:local_logger:----- Epoch[036/800], Train Loss: 0.7446, time: 892.33
INFO:local_logger:Now training epoch 37. LR=0.000139
INFO:local_logger:----- Epoch[036/800], Train Loss: 0.7444, time: 891.12
INFO:local_logger:Now training epoch 37. LR=0.000139
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-36-Loss-0.7446498155174043.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-36-Loss-0.7446498155174043.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-36-Loss-0.7446498155174043.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-36-Loss-0.7446498155174043.pdopt
INFO:local_logger:Now training epoch 37. LR=0.000139
INFO:master_logger:Now training epoch 37. LR=0.000139
INFO:local_logger:Epoch[037/800], Step[0000/0626], Avg Loss: 0.7435
INFO:local_logger:Epoch[037/800], Step[0000/0626], Avg Loss: 0.7441
INFO:local_logger:Epoch[037/800], Step[0000/0626], Avg Loss: 0.7454
INFO:local_logger:Epoch[037/800], Step[0000/0626], Avg Loss: 0.7397
INFO:master_logger:Epoch[037/800], Step[0000/0626], Avg Loss: 0.7456
INFO:local_logger:Epoch[037/800], Step[0000/0626], Avg Loss: 0.7399
INFO:local_logger:Epoch[037/800], Step[0000/0626], Avg Loss: 0.7476
INFO:local_logger:Epoch[037/800], Step[0000/0626], Avg Loss: 0.7504
INFO:local_logger:Epoch[037/800], Step[0000/0626], Avg Loss: 0.7542
INFO:local_logger:Epoch[037/800], Step[0100/0626], Avg Loss: 0.7433
INFO:local_logger:Epoch[037/800], Step[0100/0626], Avg Loss: 0.7414
INFO:local_logger:Epoch[037/800], Step[0100/0626], Avg Loss: 0.7429
INFO:local_logger:Epoch[037/800], Step[0100/0626], Avg Loss: 0.7416
INFO:local_logger:Epoch[037/800], Step[0100/0626], Avg Loss: 0.7421
INFO:master_logger:Epoch[037/800], Step[0100/0626], Avg Loss: 0.7426
INFO:local_logger:Epoch[037/800], Step[0100/0626], Avg Loss: 0.7423
INFO:local_logger:Epoch[037/800], Step[0100/0626], Avg Loss: 0.7431
INFO:local_logger:Epoch[037/800], Step[0100/0626], Avg Loss: 0.7438
INFO:local_logger:Epoch[037/800], Step[0200/0626], Avg Loss: 0.7429
INFO:local_logger:Epoch[037/800], Step[0200/0626], Avg Loss: 0.7434
INFO:local_logger:Epoch[037/800], Step[0200/0626], Avg Loss: 0.7425
INFO:local_logger:Epoch[037/800], Step[0200/0626], Avg Loss: 0.7427
INFO:local_logger:Epoch[037/800], Step[0200/0626], Avg Loss: 0.7427
INFO:local_logger:Epoch[037/800], Step[0200/0626], Avg Loss: 0.7428
INFO:local_logger:Epoch[037/800], Step[0200/0626], Avg Loss: 0.7429
INFO:local_logger:Epoch[037/800], Step[0200/0626], Avg Loss: 0.7424
INFO:master_logger:Epoch[037/800], Step[0200/0626], Avg Loss: 0.7428
INFO:local_logger:Epoch[037/800], Step[0300/0626], Avg Loss: 0.7422
INFO:local_logger:Epoch[037/800], Step[0300/0626], Avg Loss: 0.7425
INFO:local_logger:Epoch[037/800], Step[0300/0626], Avg Loss: 0.7417
INFO:local_logger:Epoch[037/800], Step[0300/0626], Avg Loss: 0.7424
INFO:local_logger:Epoch[037/800], Step[0300/0626], Avg Loss: 0.7423
INFO:local_logger:Epoch[037/800], Step[0300/0626], Avg Loss: 0.7424
INFO:local_logger:Epoch[037/800], Step[0300/0626], Avg Loss: 0.7427
INFO:local_logger:Epoch[037/800], Step[0300/0626], Avg Loss: 0.7425
INFO:master_logger:Epoch[037/800], Step[0300/0626], Avg Loss: 0.7423
INFO:local_logger:Epoch[037/800], Step[0400/0626], Avg Loss: 0.7423
INFO:local_logger:Epoch[037/800], Step[0400/0626], Avg Loss: 0.7419
INFO:local_logger:Epoch[037/800], Step[0400/0626], Avg Loss: 0.7416
INFO:local_logger:Epoch[037/800], Step[0400/0626], Avg Loss: 0.7420
INFO:local_logger:Epoch[037/800], Step[0400/0626], Avg Loss: 0.7422
INFO:local_logger:Epoch[037/800], Step[0400/0626], Avg Loss: 0.7414
INFO:local_logger:Epoch[037/800], Step[0400/0626], Avg Loss: 0.7420
INFO:local_logger:Epoch[037/800], Step[0400/0626], Avg Loss: 0.7419
INFO:master_logger:Epoch[037/800], Step[0400/0626], Avg Loss: 0.7419
INFO:local_logger:Epoch[037/800], Step[0500/0626], Avg Loss: 0.7414
INFO:local_logger:Epoch[037/800], Step[0500/0626], Avg Loss: 0.7414
INFO:local_logger:Epoch[037/800], Step[0500/0626], Avg Loss: 0.7416
INFO:local_logger:Epoch[037/800], Step[0500/0626], Avg Loss: 0.7419
INFO:local_logger:Epoch[037/800], Step[0500/0626], Avg Loss: 0.7417
INFO:local_logger:Epoch[037/800], Step[0500/0626], Avg Loss: 0.7419
INFO:local_logger:Epoch[037/800], Step[0500/0626], Avg Loss: 0.7415
INFO:local_logger:Epoch[037/800], Step[0500/0626], Avg Loss: 0.7418
INFO:master_logger:Epoch[037/800], Step[0500/0626], Avg Loss: 0.7417
INFO:local_logger:Epoch[037/800], Step[0600/0626], Avg Loss: 0.7410
INFO:local_logger:Epoch[037/800], Step[0600/0626], Avg Loss: 0.7416
INFO:local_logger:Epoch[037/800], Step[0600/0626], Avg Loss: 0.7416
INFO:local_logger:Epoch[037/800], Step[0600/0626], Avg Loss: 0.7417
INFO:local_logger:Epoch[037/800], Step[0600/0626], Avg Loss: 0.7413
INFO:local_logger:Epoch[037/800], Step[0600/0626], Avg Loss: 0.7413
INFO:local_logger:Epoch[037/800], Step[0600/0626], Avg Loss: 0.7412
INFO:master_logger:Epoch[037/800], Step[0600/0626], Avg Loss: 0.7414
INFO:local_logger:Epoch[037/800], Step[0600/0626], Avg Loss: 0.7414
INFO:local_logger:----- Epoch[037/800], Train Loss: 0.7415, time: 861.30
INFO:local_logger:Now training epoch 38. LR=0.000143
INFO:local_logger:----- Epoch[037/800], Train Loss: 0.7415, time: 861.30
INFO:local_logger:Now training epoch 38. LR=0.000143
INFO:local_logger:----- Epoch[037/800], Train Loss: 0.7414, time: 861.29
INFO:local_logger:Now training epoch 38. LR=0.000143
INFO:local_logger:----- Epoch[037/800], Train Loss: 0.7412, time: 861.53
INFO:local_logger:Now training epoch 38. LR=0.000143
INFO:local_logger:----- Epoch[037/800], Train Loss: 0.7412, time: 862.22
INFO:local_logger:Now training epoch 38. LR=0.000143
INFO:local_logger:----- Epoch[037/800], Train Loss: 0.7411, time: 861.67
INFO:local_logger:Now training epoch 38. LR=0.000143
INFO:local_logger:----- Epoch[037/800], Train Loss: 0.7410, time: 861.66
INFO:local_logger:Now training epoch 38. LR=0.000143
INFO:local_logger:----- Epoch[037/800], Train Loss: 0.7415, time: 858.01
INFO:master_logger:----- Epoch[037/800], Train Loss: 0.7413, time: 858.01
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-37-Loss-0.7415279359559235.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-37-Loss-0.7415279359559235.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-37-Loss-0.7415279359559235.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-37-Loss-0.7415279359559235.pdopt
INFO:local_logger:Now training epoch 38. LR=0.000143
INFO:master_logger:Now training epoch 38. LR=0.000143
INFO:local_logger:Epoch[038/800], Step[0000/0626], Avg Loss: 0.7326
INFO:local_logger:Epoch[038/800], Step[0000/0626], Avg Loss: 0.7470
INFO:master_logger:Epoch[038/800], Step[0000/0626], Avg Loss: 0.7394
INFO:local_logger:Epoch[038/800], Step[0000/0626], Avg Loss: 0.7295
INFO:local_logger:Epoch[038/800], Step[0000/0626], Avg Loss: 0.7452
INFO:local_logger:Epoch[038/800], Step[0000/0626], Avg Loss: 0.7429
INFO:local_logger:Epoch[038/800], Step[0000/0626], Avg Loss: 0.7354
INFO:local_logger:Epoch[038/800], Step[0000/0626], Avg Loss: 0.7382
INFO:local_logger:Epoch[038/800], Step[0000/0626], Avg Loss: 0.7443
INFO:local_logger:Epoch[038/800], Step[0100/0626], Avg Loss: 0.7383
INFO:local_logger:Epoch[038/800], Step[0100/0626], Avg Loss: 0.7398
INFO:local_logger:Epoch[038/800], Step[0100/0626], Avg Loss: 0.7393
INFO:local_logger:Epoch[038/800], Step[0100/0626], Avg Loss: 0.7389
INFO:local_logger:Epoch[038/800], Step[0100/0626], Avg Loss: 0.7386
INFO:local_logger:Epoch[038/800], Step[0100/0626], Avg Loss: 0.7387
INFO:master_logger:Epoch[038/800], Step[0100/0626], Avg Loss: 0.7392
INFO:local_logger:Epoch[038/800], Step[0100/0626], Avg Loss: 0.7411
INFO:local_logger:Epoch[038/800], Step[0100/0626], Avg Loss: 0.7392
INFO:local_logger:Epoch[038/800], Step[0200/0626], Avg Loss: 0.7388
INFO:local_logger:Epoch[038/800], Step[0200/0626], Avg Loss: 0.7388
INFO:local_logger:Epoch[038/800], Step[0200/0626], Avg Loss: 0.7386
INFO:local_logger:Epoch[038/800], Step[0200/0626], Avg Loss: 0.7386
INFO:local_logger:Epoch[038/800], Step[0200/0626], Avg Loss: 0.7399
INFO:local_logger:Epoch[038/800], Step[0200/0626], Avg Loss: 0.7386
INFO:local_logger:Epoch[038/800], Step[0200/0626], Avg Loss: 0.7391
INFO:master_logger:Epoch[038/800], Step[0200/0626], Avg Loss: 0.7389
INFO:local_logger:Epoch[038/800], Step[0200/0626], Avg Loss: 0.7389
INFO:local_logger:Epoch[038/800], Step[0300/0626], Avg Loss: 0.7388
INFO:local_logger:Epoch[038/800], Step[0300/0626], Avg Loss: 0.7390
INFO:local_logger:Epoch[038/800], Step[0300/0626], Avg Loss: 0.7388
INFO:local_logger:Epoch[038/800], Step[0300/0626], Avg Loss: 0.7386
INFO:local_logger:Epoch[038/800], Step[0300/0626], Avg Loss: 0.7388
INFO:local_logger:Epoch[038/800], Step[0300/0626], Avg Loss: 0.7384
INFO:local_logger:Epoch[038/800], Step[0300/0626], Avg Loss: 0.7385
INFO:master_logger:Epoch[038/800], Step[0300/0626], Avg Loss: 0.7387
INFO:local_logger:Epoch[038/800], Step[0300/0626], Avg Loss: 0.7389
INFO:local_logger:Epoch[038/800], Step[0400/0626], Avg Loss: 0.7382
INFO:local_logger:Epoch[038/800], Step[0400/0626], Avg Loss: 0.7388
INFO:local_logger:Epoch[038/800], Step[0400/0626], Avg Loss: 0.7390
INFO:local_logger:Epoch[038/800], Step[0400/0626], Avg Loss: 0.7387
INFO:local_logger:Epoch[038/800], Step[0400/0626], Avg Loss: 0.7388
INFO:master_logger:Epoch[038/800], Step[0400/0626], Avg Loss: 0.7387
INFO:local_logger:Epoch[038/800], Step[0400/0626], Avg Loss: 0.7384
INFO:local_logger:Epoch[038/800], Step[0400/0626], Avg Loss: 0.7386
INFO:local_logger:Epoch[038/800], Step[0400/0626], Avg Loss: 0.7388
INFO:local_logger:Epoch[038/800], Step[0500/0626], Avg Loss: 0.7383
INFO:local_logger:Epoch[038/800], Step[0500/0626], Avg Loss: 0.7385
INFO:local_logger:Epoch[038/800], Step[0500/0626], Avg Loss: 0.7385
INFO:local_logger:Epoch[038/800], Step[0500/0626], Avg Loss: 0.7385
INFO:local_logger:Epoch[038/800], Step[0500/0626], Avg Loss: 0.7390
INFO:local_logger:Epoch[038/800], Step[0500/0626], Avg Loss: 0.7377
INFO:local_logger:Epoch[038/800], Step[0500/0626], Avg Loss: 0.7385
INFO:master_logger:Epoch[038/800], Step[0500/0626], Avg Loss: 0.7385
INFO:local_logger:Epoch[038/800], Step[0500/0626], Avg Loss: 0.7387
INFO:local_logger:Epoch[038/800], Step[0600/0626], Avg Loss: 0.7378
INFO:local_logger:Epoch[038/800], Step[0600/0626], Avg Loss: 0.7382
INFO:local_logger:Epoch[038/800], Step[0600/0626], Avg Loss: 0.7389
INFO:local_logger:Epoch[038/800], Step[0600/0626], Avg Loss: 0.7381
INFO:local_logger:Epoch[038/800], Step[0600/0626], Avg Loss: 0.7381
INFO:local_logger:Epoch[038/800], Step[0600/0626], Avg Loss: 0.7381
INFO:master_logger:Epoch[038/800], Step[0600/0626], Avg Loss: 0.7383
INFO:local_logger:Epoch[038/800], Step[0600/0626], Avg Loss: 0.7381
INFO:local_logger:Epoch[038/800], Step[0600/0626], Avg Loss: 0.7386
INFO:local_logger:----- Epoch[038/800], Train Loss: 0.7381, time: 895.61
INFO:local_logger:Now training epoch 39. LR=0.000146
INFO:local_logger:----- Epoch[038/800], Train Loss: 0.7378, time: 895.58
INFO:local_logger:Now training epoch 39. LR=0.000146
INFO:local_logger:----- Epoch[038/800], Train Loss: 0.7380, time: 895.99
INFO:local_logger:Now training epoch 39. LR=0.000146
INFO:local_logger:----- Epoch[038/800], Train Loss: 0.7381, time: 896.21
INFO:local_logger:Now training epoch 39. LR=0.000146
INFO:local_logger:----- Epoch[038/800], Train Loss: 0.7381, time: 892.19
INFO:master_logger:----- Epoch[038/800], Train Loss: 0.7382, time: 892.19
INFO:local_logger:----- Epoch[038/800], Train Loss: 0.7385, time: 895.95
INFO:local_logger:Now training epoch 39. LR=0.000146
INFO:local_logger:----- Epoch[038/800], Train Loss: 0.7388, time: 896.06
INFO:local_logger:Now training epoch 39. LR=0.000146
INFO:local_logger:----- Epoch[038/800], Train Loss: 0.7381, time: 895.94
INFO:local_logger:Now training epoch 39. LR=0.000146
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-38-Loss-0.7380608445157859.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-38-Loss-0.7380608445157859.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-38-Loss-0.7380608445157859.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-38-Loss-0.7380608445157859.pdopt
INFO:local_logger:Now training epoch 39. LR=0.000146
INFO:master_logger:Now training epoch 39. LR=0.000146
INFO:local_logger:Epoch[039/800], Step[0000/0626], Avg Loss: 0.7392
INFO:master_logger:Epoch[039/800], Step[0000/0626], Avg Loss: 0.7382
INFO:local_logger:Epoch[039/800], Step[0000/0626], Avg Loss: 0.7373
INFO:local_logger:Epoch[039/800], Step[0000/0626], Avg Loss: 0.7385
INFO:local_logger:Epoch[039/800], Step[0000/0626], Avg Loss: 0.7375
INFO:local_logger:Epoch[039/800], Step[0000/0626], Avg Loss: 0.7414
INFO:local_logger:Epoch[039/800], Step[0000/0626], Avg Loss: 0.7378
INFO:local_logger:Epoch[039/800], Step[0000/0626], Avg Loss: 0.7312
INFO:local_logger:Epoch[039/800], Step[0000/0626], Avg Loss: 0.7426
INFO:local_logger:Epoch[039/800], Step[0100/0626], Avg Loss: 0.7358
INFO:local_logger:Epoch[039/800], Step[0100/0626], Avg Loss: 0.7363
INFO:local_logger:Epoch[039/800], Step[0100/0626], Avg Loss: 0.7362
INFO:local_logger:Epoch[039/800], Step[0100/0626], Avg Loss: 0.7361
INFO:local_logger:Epoch[039/800], Step[0100/0626], Avg Loss: 0.7359
INFO:local_logger:Epoch[039/800], Step[0100/0626], Avg Loss: 0.7369
INFO:local_logger:Epoch[039/800], Step[0100/0626], Avg Loss: 0.7365
INFO:master_logger:Epoch[039/800], Step[0100/0626], Avg Loss: 0.7363
INFO:local_logger:Epoch[039/800], Step[0100/0626], Avg Loss: 0.7366
INFO:local_logger:Epoch[039/800], Step[0200/0626], Avg Loss: 0.7366
INFO:local_logger:Epoch[039/800], Step[0200/0626], Avg Loss: 0.7359
INFO:local_logger:Epoch[039/800], Step[0200/0626], Avg Loss: 0.7371
INFO:local_logger:Epoch[039/800], Step[0200/0626], Avg Loss: 0.7356
INFO:local_logger:Epoch[039/800], Step[0200/0626], Avg Loss: 0.7361
INFO:master_logger:Epoch[039/800], Step[0200/0626], Avg Loss: 0.7362
INFO:local_logger:Epoch[039/800], Step[0200/0626], Avg Loss: 0.7363
INFO:local_logger:Epoch[039/800], Step[0200/0626], Avg Loss: 0.7364
INFO:local_logger:Epoch[039/800], Step[0200/0626], Avg Loss: 0.7360
INFO:local_logger:Epoch[039/800], Step[0300/0626], Avg Loss: 0.7354
INFO:local_logger:Epoch[039/800], Step[0300/0626], Avg Loss: 0.7356
INFO:local_logger:Epoch[039/800], Step[0300/0626], Avg Loss: 0.7368
INFO:local_logger:Epoch[039/800], Step[0300/0626], Avg Loss: 0.7364
INFO:local_logger:Epoch[039/800], Step[0300/0626], Avg Loss: 0.7363
INFO:master_logger:Epoch[039/800], Step[0300/0626], Avg Loss: 0.7360
INFO:local_logger:Epoch[039/800], Step[0300/0626], Avg Loss: 0.7357
INFO:local_logger:Epoch[039/800], Step[0300/0626], Avg Loss: 0.7359
INFO:local_logger:Epoch[039/800], Step[0300/0626], Avg Loss: 0.7360
INFO:local_logger:Epoch[039/800], Step[0400/0626], Avg Loss: 0.7359
INFO:local_logger:Epoch[039/800], Step[0400/0626], Avg Loss: 0.7357
INFO:local_logger:Epoch[039/800], Step[0400/0626], Avg Loss: 0.7355
INFO:local_logger:Epoch[039/800], Step[0400/0626], Avg Loss: 0.7359
INFO:local_logger:Epoch[039/800], Step[0400/0626], Avg Loss: 0.7359
INFO:master_logger:Epoch[039/800], Step[0400/0626], Avg Loss: 0.7358
INFO:local_logger:Epoch[039/800], Step[0400/0626], Avg Loss: 0.7365
INFO:local_logger:Epoch[039/800], Step[0400/0626], Avg Loss: 0.7355
INFO:local_logger:Epoch[039/800], Step[0400/0626], Avg Loss: 0.7356
INFO:local_logger:Epoch[039/800], Step[0500/0626], Avg Loss: 0.7352
INFO:local_logger:Epoch[039/800], Step[0500/0626], Avg Loss: 0.7352
INFO:local_logger:Epoch[039/800], Step[0500/0626], Avg Loss: 0.7356
INFO:local_logger:Epoch[039/800], Step[0500/0626], Avg Loss: 0.7355
INFO:local_logger:Epoch[039/800], Step[0500/0626], Avg Loss: 0.7358
INFO:local_logger:Epoch[039/800], Step[0500/0626], Avg Loss: 0.7352
INFO:local_logger:Epoch[039/800], Step[0500/0626], Avg Loss: 0.7356
INFO:master_logger:Epoch[039/800], Step[0500/0626], Avg Loss: 0.7354
INFO:local_logger:Epoch[039/800], Step[0500/0626], Avg Loss: 0.7353
INFO:local_logger:Epoch[039/800], Step[0600/0626], Avg Loss: 0.7351
INFO:local_logger:Epoch[039/800], Step[0600/0626], Avg Loss: 0.7351
INFO:local_logger:Epoch[039/800], Step[0600/0626], Avg Loss: 0.7354
INFO:local_logger:Epoch[039/800], Step[0600/0626], Avg Loss: 0.7353
INFO:local_logger:Epoch[039/800], Step[0600/0626], Avg Loss: 0.7351
INFO:local_logger:Epoch[039/800], Step[0600/0626], Avg Loss: 0.7356
INFO:local_logger:Epoch[039/800], Step[0600/0626], Avg Loss: 0.7356
INFO:master_logger:Epoch[039/800], Step[0600/0626], Avg Loss: 0.7354
INFO:local_logger:Epoch[039/800], Step[0600/0626], Avg Loss: 0.7355
INFO:local_logger:----- Epoch[039/800], Train Loss: 0.7351, time: 865.25
INFO:local_logger:----- Epoch[039/800], Train Loss: 0.7350, time: 864.90
INFO:local_logger:Now training epoch 40. LR=0.000150
INFO:local_logger:Now training epoch 40. LR=0.000150
INFO:local_logger:----- Epoch[039/800], Train Loss: 0.7353, time: 860.87
INFO:master_logger:----- Epoch[039/800], Train Loss: 0.7353, time: 860.87
INFO:local_logger:----- Epoch[039/800], Train Loss: 0.7352, time: 864.70
INFO:local_logger:Now training epoch 40. LR=0.000150
INFO:local_logger:----- Epoch[039/800], Train Loss: 0.7355, time: 864.66
INFO:local_logger:Now training epoch 40. LR=0.000150
INFO:local_logger:----- Epoch[039/800], Train Loss: 0.7355, time: 864.70
INFO:local_logger:Now training epoch 40. LR=0.000150
INFO:local_logger:----- Epoch[039/800], Train Loss: 0.7356, time: 864.70
INFO:local_logger:Now training epoch 40. LR=0.000150
INFO:local_logger:----- Epoch[039/800], Train Loss: 0.7351, time: 865.02
INFO:local_logger:Now training epoch 40. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-39-Loss-0.7353187804344304.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-39-Loss-0.7353187804344304.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-39-Loss-0.7353187804344304.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-39-Loss-0.7353187804344304.pdopt
INFO:local_logger:Now training epoch 40. LR=0.000150
INFO:master_logger:Now training epoch 40. LR=0.000150
INFO:local_logger:Epoch[040/800], Step[0000/0626], Avg Loss: 0.7343
INFO:local_logger:Epoch[040/800], Step[0000/0626], Avg Loss: 0.7310
INFO:local_logger:Epoch[040/800], Step[0000/0626], Avg Loss: 0.7444
INFO:local_logger:Epoch[040/800], Step[0000/0626], Avg Loss: 0.7305
INFO:master_logger:Epoch[040/800], Step[0000/0626], Avg Loss: 0.7355
INFO:local_logger:Epoch[040/800], Step[0000/0626], Avg Loss: 0.7357
INFO:local_logger:Epoch[040/800], Step[0000/0626], Avg Loss: 0.7310
INFO:local_logger:Epoch[040/800], Step[0000/0626], Avg Loss: 0.7439
INFO:local_logger:Epoch[040/800], Step[0000/0626], Avg Loss: 0.7330
INFO:local_logger:Epoch[040/800], Step[0100/0626], Avg Loss: 0.7348
INFO:local_logger:Epoch[040/800], Step[0100/0626], Avg Loss: 0.7344
INFO:local_logger:Epoch[040/800], Step[0100/0626], Avg Loss: 0.7346
INFO:local_logger:Epoch[040/800], Step[0100/0626], Avg Loss: 0.7348
INFO:master_logger:Epoch[040/800], Step[0100/0626], Avg Loss: 0.7341
INFO:local_logger:Epoch[040/800], Step[0100/0626], Avg Loss: 0.7345
INFO:local_logger:Epoch[040/800], Step[0100/0626], Avg Loss: 0.7336
INFO:local_logger:Epoch[040/800], Step[0100/0626], Avg Loss: 0.7331
INFO:local_logger:Epoch[040/800], Step[0100/0626], Avg Loss: 0.7333
INFO:local_logger:Epoch[040/800], Step[0200/0626], Avg Loss: 0.7340
INFO:local_logger:Epoch[040/800], Step[0200/0626], Avg Loss: 0.7343
INFO:local_logger:Epoch[040/800], Step[0200/0626], Avg Loss: 0.7335
INFO:local_logger:Epoch[040/800], Step[0200/0626], Avg Loss: 0.7340
INFO:local_logger:Epoch[040/800], Step[0200/0626], Avg Loss: 0.7335
INFO:local_logger:Epoch[040/800], Step[0200/0626], Avg Loss: 0.7334
INFO:local_logger:Epoch[040/800], Step[0200/0626], Avg Loss: 0.7326
INFO:master_logger:Epoch[040/800], Step[0200/0626], Avg Loss: 0.7336
INFO:local_logger:Epoch[040/800], Step[0200/0626], Avg Loss: 0.7339
INFO:local_logger:Epoch[040/800], Step[0300/0626], Avg Loss: 0.7338
INFO:local_logger:Epoch[040/800], Step[0300/0626], Avg Loss: 0.7336
INFO:local_logger:Epoch[040/800], Step[0300/0626], Avg Loss: 0.7331
INFO:local_logger:Epoch[040/800], Step[0300/0626], Avg Loss: 0.7338
INFO:local_logger:Epoch[040/800], Step[0300/0626], Avg Loss: 0.7332
INFO:master_logger:Epoch[040/800], Step[0300/0626], Avg Loss: 0.7334
INFO:local_logger:Epoch[040/800], Step[0300/0626], Avg Loss: 0.7329
INFO:local_logger:Epoch[040/800], Step[0300/0626], Avg Loss: 0.7334
INFO:local_logger:Epoch[040/800], Step[0300/0626], Avg Loss: 0.7338
INFO:local_logger:Epoch[040/800], Step[0400/0626], Avg Loss: 0.7335
INFO:local_logger:Epoch[040/800], Step[0400/0626], Avg Loss: 0.7335
INFO:local_logger:Epoch[040/800], Step[0400/0626], Avg Loss: 0.7336
INFO:local_logger:Epoch[040/800], Step[0400/0626], Avg Loss: 0.7334
INFO:local_logger:Epoch[040/800], Step[0400/0626], Avg Loss: 0.7328
INFO:local_logger:Epoch[040/800], Step[0400/0626], Avg Loss: 0.7332
INFO:master_logger:Epoch[040/800], Step[0400/0626], Avg Loss: 0.7333
INFO:local_logger:Epoch[040/800], Step[0400/0626], Avg Loss: 0.7332
INFO:local_logger:Epoch[040/800], Step[0400/0626], Avg Loss: 0.7330
INFO:local_logger:Epoch[040/800], Step[0500/0626], Avg Loss: 0.7330
INFO:local_logger:Epoch[040/800], Step[0500/0626], Avg Loss: 0.7323
INFO:local_logger:Epoch[040/800], Step[0500/0626], Avg Loss: 0.7333
INFO:local_logger:Epoch[040/800], Step[0500/0626], Avg Loss: 0.7333
INFO:local_logger:Epoch[040/800], Step[0500/0626], Avg Loss: 0.7329
INFO:local_logger:Epoch[040/800], Step[0500/0626], Avg Loss: 0.7334
INFO:local_logger:Epoch[040/800], Step[0500/0626], Avg Loss: 0.7330
INFO:local_logger:Epoch[040/800], Step[0500/0626], Avg Loss: 0.7329
INFO:master_logger:Epoch[040/800], Step[0500/0626], Avg Loss: 0.7330
INFO:local_logger:Epoch[040/800], Step[0600/0626], Avg Loss: 0.7329
INFO:local_logger:Epoch[040/800], Step[0600/0626], Avg Loss: 0.7329
INFO:local_logger:Epoch[040/800], Step[0600/0626], Avg Loss: 0.7328
INFO:local_logger:Epoch[040/800], Step[0600/0626], Avg Loss: 0.7329
INFO:local_logger:Epoch[040/800], Step[0600/0626], Avg Loss: 0.7328
INFO:local_logger:Epoch[040/800], Step[0600/0626], Avg Loss: 0.7329
INFO:local_logger:Epoch[040/800], Step[0600/0626], Avg Loss: 0.7322
INFO:master_logger:Epoch[040/800], Step[0600/0626], Avg Loss: 0.7328
INFO:local_logger:Epoch[040/800], Step[0600/0626], Avg Loss: 0.7328
INFO:local_logger:----- Epoch[040/800], Train Loss: 0.7328, time: 895.01
INFO:local_logger:Now training epoch 41. LR=0.000150
INFO:local_logger:----- Epoch[040/800], Train Loss: 0.7328, time: 890.99
INFO:local_logger:----- Epoch[040/800], Train Loss: 0.7328, time: 895.12
INFO:master_logger:----- Epoch[040/800], Train Loss: 0.7327, time: 890.99
INFO:local_logger:Now training epoch 41. LR=0.000150
INFO:local_logger:----- Epoch[040/800], Train Loss: 0.7328, time: 895.13
INFO:local_logger:Now training epoch 41. LR=0.000150
INFO:local_logger:----- Epoch[040/800], Train Loss: 0.7328, time: 894.99
INFO:local_logger:Now training epoch 41. LR=0.000150
INFO:local_logger:----- Epoch[040/800], Train Loss: 0.7328, time: 894.98
INFO:local_logger:Now training epoch 41. LR=0.000150
INFO:local_logger:----- Epoch[040/800], Train Loss: 0.7321, time: 894.99
INFO:local_logger:Now training epoch 41. LR=0.000150
INFO:local_logger:----- Epoch[040/800], Train Loss: 0.7327, time: 895.07
INFO:local_logger:Now training epoch 41. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-40-Loss-0.7327702230552797.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-40-Loss-0.7327702230552797.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-40-Loss-0.7327702230552797.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-40-Loss-0.7327702230552797.pdopt
INFO:local_logger:Now training epoch 41. LR=0.000150
INFO:master_logger:Now training epoch 41. LR=0.000150
INFO:local_logger:Epoch[041/800], Step[0000/0626], Avg Loss: 0.7292
INFO:master_logger:Epoch[041/800], Step[0000/0626], Avg Loss: 0.7311
INFO:local_logger:Epoch[041/800], Step[0000/0626], Avg Loss: 0.7233
INFO:local_logger:Epoch[041/800], Step[0000/0626], Avg Loss: 0.7219
INFO:local_logger:Epoch[041/800], Step[0000/0626], Avg Loss: 0.7395
INFO:local_logger:Epoch[041/800], Step[0000/0626], Avg Loss: 0.7295
INFO:local_logger:Epoch[041/800], Step[0000/0626], Avg Loss: 0.7279
INFO:local_logger:Epoch[041/800], Step[0000/0626], Avg Loss: 0.7431
INFO:local_logger:Epoch[041/800], Step[0000/0626], Avg Loss: 0.7341
INFO:local_logger:Epoch[041/800], Step[0100/0626], Avg Loss: 0.7312
INFO:local_logger:Epoch[041/800], Step[0100/0626], Avg Loss: 0.7315
INFO:local_logger:Epoch[041/800], Step[0100/0626], Avg Loss: 0.7304
INFO:local_logger:Epoch[041/800], Step[0100/0626], Avg Loss: 0.7317
INFO:local_logger:Epoch[041/800], Step[0100/0626], Avg Loss: 0.7307
INFO:master_logger:Epoch[041/800], Step[0100/0626], Avg Loss: 0.7311
INFO:local_logger:Epoch[041/800], Step[0100/0626], Avg Loss: 0.7308
INFO:local_logger:Epoch[041/800], Step[0100/0626], Avg Loss: 0.7302
INFO:local_logger:Epoch[041/800], Step[0100/0626], Avg Loss: 0.7321
INFO:local_logger:Epoch[041/800], Step[0200/0626], Avg Loss: 0.7308
INFO:local_logger:Epoch[041/800], Step[0200/0626], Avg Loss: 0.7315
INFO:local_logger:Epoch[041/800], Step[0200/0626], Avg Loss: 0.7308
INFO:local_logger:Epoch[041/800], Step[0200/0626], Avg Loss: 0.7317
INFO:local_logger:Epoch[041/800], Step[0200/0626], Avg Loss: 0.7303
INFO:local_logger:Epoch[041/800], Step[0200/0626], Avg Loss: 0.7313
INFO:master_logger:Epoch[041/800], Step[0200/0626], Avg Loss: 0.7310
INFO:local_logger:Epoch[041/800], Step[0200/0626], Avg Loss: 0.7309
INFO:local_logger:Epoch[041/800], Step[0200/0626], Avg Loss: 0.7308
INFO:local_logger:Epoch[041/800], Step[0300/0626], Avg Loss: 0.7310
INFO:local_logger:Epoch[041/800], Step[0300/0626], Avg Loss: 0.7307
INFO:local_logger:Epoch[041/800], Step[0300/0626], Avg Loss: 0.7304
INFO:local_logger:Epoch[041/800], Step[0300/0626], Avg Loss: 0.7307
INFO:master_logger:Epoch[041/800], Step[0300/0626], Avg Loss: 0.7308
INFO:local_logger:Epoch[041/800], Step[0300/0626], Avg Loss: 0.7310
INFO:local_logger:Epoch[041/800], Step[0300/0626], Avg Loss: 0.7308
INFO:local_logger:Epoch[041/800], Step[0300/0626], Avg Loss: 0.7312
INFO:local_logger:Epoch[041/800], Step[0300/0626], Avg Loss: 0.7304
INFO:local_logger:Epoch[041/800], Step[0400/0626], Avg Loss: 0.7312
INFO:local_logger:Epoch[041/800], Step[0400/0626], Avg Loss: 0.7309
INFO:local_logger:Epoch[041/800], Step[0400/0626], Avg Loss: 0.7301
INFO:local_logger:Epoch[041/800], Step[0400/0626], Avg Loss: 0.7305
INFO:local_logger:Epoch[041/800], Step[0400/0626], Avg Loss: 0.7305
INFO:master_logger:Epoch[041/800], Step[0400/0626], Avg Loss: 0.7306
INFO:local_logger:Epoch[041/800], Step[0400/0626], Avg Loss: 0.7307
INFO:local_logger:Epoch[041/800], Step[0400/0626], Avg Loss: 0.7303
INFO:local_logger:Epoch[041/800], Step[0400/0626], Avg Loss: 0.7307
INFO:local_logger:Epoch[041/800], Step[0500/0626], Avg Loss: 0.7303
INFO:local_logger:Epoch[041/800], Step[0500/0626], Avg Loss: 0.7302
INFO:local_logger:Epoch[041/800], Step[0500/0626], Avg Loss: 0.7306
INFO:local_logger:Epoch[041/800], Step[0500/0626], Avg Loss: 0.7305
INFO:local_logger:Epoch[041/800], Step[0500/0626], Avg Loss: 0.7300
INFO:local_logger:Epoch[041/800], Step[0500/0626], Avg Loss: 0.7306
INFO:local_logger:Epoch[041/800], Step[0500/0626], Avg Loss: 0.7304
INFO:local_logger:Epoch[041/800], Step[0500/0626], Avg Loss: 0.7304
INFO:master_logger:Epoch[041/800], Step[0500/0626], Avg Loss: 0.7304
INFO:local_logger:Epoch[041/800], Step[0600/0626], Avg Loss: 0.7303
INFO:local_logger:Epoch[041/800], Step[0600/0626], Avg Loss: 0.7304
INFO:local_logger:Epoch[041/800], Step[0600/0626], Avg Loss: 0.7301
INFO:local_logger:Epoch[041/800], Step[0600/0626], Avg Loss: 0.7299
INFO:local_logger:Epoch[041/800], Step[0600/0626], Avg Loss: 0.7302
INFO:master_logger:Epoch[041/800], Step[0600/0626], Avg Loss: 0.7301
INFO:local_logger:Epoch[041/800], Step[0600/0626], Avg Loss: 0.7299
INFO:local_logger:Epoch[041/800], Step[0600/0626], Avg Loss: 0.7302
INFO:local_logger:Epoch[041/800], Step[0600/0626], Avg Loss: 0.7301
INFO:local_logger:----- Epoch[041/800], Train Loss: 0.7305, time: 866.86
INFO:local_logger:Now training epoch 42. LR=0.000150
INFO:local_logger:----- Epoch[041/800], Train Loss: 0.7300, time: 866.90
INFO:local_logger:Now training epoch 42. LR=0.000150
INFO:local_logger:----- Epoch[041/800], Train Loss: 0.7304, time: 867.40
INFO:local_logger:Now training epoch 42. LR=0.000150
INFO:local_logger:----- Epoch[041/800], Train Loss: 0.7303, time: 863.76
INFO:master_logger:----- Epoch[041/800], Train Loss: 0.7302, time: 863.76
INFO:local_logger:----- Epoch[041/800], Train Loss: 0.7301, time: 867.52
INFO:local_logger:Now training epoch 42. LR=0.000150
INFO:local_logger:----- Epoch[041/800], Train Loss: 0.7303, time: 867.54
INFO:local_logger:Now training epoch 42. LR=0.000150
INFO:local_logger:----- Epoch[041/800], Train Loss: 0.7302, time: 867.65
INFO:local_logger:Now training epoch 42. LR=0.000150
INFO:local_logger:----- Epoch[041/800], Train Loss: 0.7300, time: 867.66
INFO:local_logger:Now training epoch 42. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-41-Loss-0.7302703064055491.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-41-Loss-0.7302703064055491.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-41-Loss-0.7302703064055491.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-41-Loss-0.7302703064055491.pdopt
INFO:local_logger:Now training epoch 42. LR=0.000150
INFO:master_logger:Now training epoch 42. LR=0.000150
INFO:local_logger:Epoch[042/800], Step[0000/0626], Avg Loss: 0.7300
INFO:local_logger:Epoch[042/800], Step[0000/0626], Avg Loss: 0.7398
INFO:master_logger:Epoch[042/800], Step[0000/0626], Avg Loss: 0.7281
INFO:local_logger:Epoch[042/800], Step[0000/0626], Avg Loss: 0.7322
INFO:local_logger:Epoch[042/800], Step[0000/0626], Avg Loss: 0.7185
INFO:local_logger:Epoch[042/800], Step[0000/0626], Avg Loss: 0.7206
INFO:local_logger:Epoch[042/800], Step[0000/0626], Avg Loss: 0.7231
INFO:local_logger:Epoch[042/800], Step[0000/0626], Avg Loss: 0.7259
INFO:local_logger:Epoch[042/800], Step[0000/0626], Avg Loss: 0.7347
INFO:local_logger:Epoch[042/800], Step[0100/0626], Avg Loss: 0.7299
INFO:local_logger:Epoch[042/800], Step[0100/0626], Avg Loss: 0.7279
INFO:local_logger:Epoch[042/800], Step[0100/0626], Avg Loss: 0.7292
INFO:local_logger:Epoch[042/800], Step[0100/0626], Avg Loss: 0.7300
INFO:local_logger:Epoch[042/800], Step[0100/0626], Avg Loss: 0.7287
INFO:local_logger:Epoch[042/800], Step[0100/0626], Avg Loss: 0.7292
INFO:master_logger:Epoch[042/800], Step[0100/0626], Avg Loss: 0.7291
INFO:local_logger:Epoch[042/800], Step[0100/0626], Avg Loss: 0.7283
INFO:local_logger:Epoch[042/800], Step[0100/0626], Avg Loss: 0.7297
INFO:local_logger:Epoch[042/800], Step[0200/0626], Avg Loss: 0.7285
INFO:local_logger:Epoch[042/800], Step[0200/0626], Avg Loss: 0.7286
INFO:local_logger:Epoch[042/800], Step[0200/0626], Avg Loss: 0.7290
INFO:local_logger:Epoch[042/800], Step[0200/0626], Avg Loss: 0.7289
INFO:local_logger:Epoch[042/800], Step[0200/0626], Avg Loss: 0.7292
INFO:local_logger:Epoch[042/800], Step[0200/0626], Avg Loss: 0.7282
INFO:master_logger:Epoch[042/800], Step[0200/0626], Avg Loss: 0.7286
INFO:local_logger:Epoch[042/800], Step[0200/0626], Avg Loss: 0.7284
INFO:local_logger:Epoch[042/800], Step[0200/0626], Avg Loss: 0.7283
INFO:local_logger:Epoch[042/800], Step[0300/0626], Avg Loss: 0.7283
INFO:local_logger:Epoch[042/800], Step[0300/0626], Avg Loss: 0.7288
INFO:local_logger:Epoch[042/800], Step[0300/0626], Avg Loss: 0.7283
INFO:local_logger:Epoch[042/800], Step[0300/0626], Avg Loss: 0.7277
INFO:local_logger:Epoch[042/800], Step[0300/0626], Avg Loss: 0.7284
INFO:local_logger:Epoch[042/800], Step[0300/0626], Avg Loss: 0.7286
INFO:master_logger:Epoch[042/800], Step[0300/0626], Avg Loss: 0.7285
INFO:local_logger:Epoch[042/800], Step[0300/0626], Avg Loss: 0.7287
INFO:local_logger:Epoch[042/800], Step[0300/0626], Avg Loss: 0.7288
INFO:local_logger:Epoch[042/800], Step[0400/0626], Avg Loss: 0.7279
INFO:local_logger:Epoch[042/800], Step[0400/0626], Avg Loss: 0.7285
INFO:local_logger:Epoch[042/800], Step[0400/0626], Avg Loss: 0.7287
INFO:local_logger:Epoch[042/800], Step[0400/0626], Avg Loss: 0.7288
INFO:local_logger:Epoch[042/800], Step[0400/0626], Avg Loss: 0.7283
INFO:local_logger:Epoch[042/800], Step[0400/0626], Avg Loss: 0.7275
INFO:local_logger:Epoch[042/800], Step[0400/0626], Avg Loss: 0.7284
INFO:master_logger:Epoch[042/800], Step[0400/0626], Avg Loss: 0.7283
INFO:local_logger:Epoch[042/800], Step[0400/0626], Avg Loss: 0.7279
INFO:local_logger:Epoch[042/800], Step[0500/0626], Avg Loss: 0.7278
INFO:local_logger:Epoch[042/800], Step[0500/0626], Avg Loss: 0.7284
INFO:local_logger:Epoch[042/800], Step[0500/0626], Avg Loss: 0.7285
INFO:local_logger:Epoch[042/800], Step[0500/0626], Avg Loss: 0.7284
INFO:local_logger:Epoch[042/800], Step[0500/0626], Avg Loss: 0.7277
INFO:local_logger:Epoch[042/800], Step[0500/0626], Avg Loss: 0.7279
INFO:local_logger:Epoch[042/800], Step[0500/0626], Avg Loss: 0.7278
INFO:local_logger:Epoch[042/800], Step[0500/0626], Avg Loss: 0.7285
INFO:master_logger:Epoch[042/800], Step[0500/0626], Avg Loss: 0.7281
INFO:local_logger:Epoch[042/800], Step[0600/0626], Avg Loss: 0.7278
INFO:local_logger:Epoch[042/800], Step[0600/0626], Avg Loss: 0.7276
INFO:local_logger:Epoch[042/800], Step[0600/0626], Avg Loss: 0.7285
INFO:local_logger:Epoch[042/800], Step[0600/0626], Avg Loss: 0.7278
INFO:local_logger:Epoch[042/800], Step[0600/0626], Avg Loss: 0.7277
INFO:local_logger:Epoch[042/800], Step[0600/0626], Avg Loss: 0.7283
INFO:local_logger:Epoch[042/800], Step[0600/0626], Avg Loss: 0.7282
INFO:master_logger:Epoch[042/800], Step[0600/0626], Avg Loss: 0.7280
INFO:local_logger:Epoch[042/800], Step[0600/0626], Avg Loss: 0.7280
INFO:local_logger:----- Epoch[042/800], Train Loss: 0.7281, time: 892.19
INFO:local_logger:Now training epoch 43. LR=0.000150
INFO:local_logger:----- Epoch[042/800], Train Loss: 0.7280, time: 892.20
INFO:local_logger:Now training epoch 43. LR=0.000150
INFO:local_logger:----- Epoch[042/800], Train Loss: 0.7277, time: 892.57
INFO:local_logger:Now training epoch 43. LR=0.000150
INFO:local_logger:----- Epoch[042/800], Train Loss: 0.7276, time: 893.34
INFO:local_logger:Now training epoch 43. LR=0.000150
INFO:local_logger:----- Epoch[042/800], Train Loss: 0.7275, time: 892.63
INFO:local_logger:Now training epoch 43. LR=0.000150
INFO:local_logger:----- Epoch[042/800], Train Loss: 0.7283, time: 889.09
INFO:master_logger:----- Epoch[042/800], Train Loss: 0.7279, time: 889.09
INFO:local_logger:----- Epoch[042/800], Train Loss: 0.7282, time: 893.43
INFO:local_logger:Now training epoch 43. LR=0.000150
INFO:local_logger:----- Epoch[042/800], Train Loss: 0.7279, time: 892.89
INFO:local_logger:Now training epoch 43. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-42-Loss-0.728333370828915.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-42-Loss-0.728333370828915.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-42-Loss-0.728333370828915.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-42-Loss-0.728333370828915.pdopt
INFO:local_logger:Now training epoch 43. LR=0.000150
INFO:master_logger:Now training epoch 43. LR=0.000150
INFO:local_logger:Epoch[043/800], Step[0000/0626], Avg Loss: 0.7259
INFO:local_logger:Epoch[043/800], Step[0000/0626], Avg Loss: 0.7157
INFO:master_logger:Epoch[043/800], Step[0000/0626], Avg Loss: 0.7299
INFO:local_logger:Epoch[043/800], Step[0000/0626], Avg Loss: 0.7296
INFO:local_logger:Epoch[043/800], Step[0000/0626], Avg Loss: 0.7389
INFO:local_logger:Epoch[043/800], Step[0000/0626], Avg Loss: 0.7312
INFO:local_logger:Epoch[043/800], Step[0000/0626], Avg Loss: 0.7351
INFO:local_logger:Epoch[043/800], Step[0000/0626], Avg Loss: 0.7178
INFO:local_logger:Epoch[043/800], Step[0000/0626], Avg Loss: 0.7452
INFO:local_logger:Epoch[043/800], Step[0100/0626], Avg Loss: 0.7266
INFO:local_logger:Epoch[043/800], Step[0100/0626], Avg Loss: 0.7263
INFO:local_logger:Epoch[043/800], Step[0100/0626], Avg Loss: 0.7264
INFO:local_logger:Epoch[043/800], Step[0100/0626], Avg Loss: 0.7253
INFO:master_logger:Epoch[043/800], Step[0100/0626], Avg Loss: 0.7262
INFO:local_logger:Epoch[043/800], Step[0100/0626], Avg Loss: 0.7269
INFO:local_logger:Epoch[043/800], Step[0100/0626], Avg Loss: 0.7261
INFO:local_logger:Epoch[043/800], Step[0100/0626], Avg Loss: 0.7256
INFO:local_logger:Epoch[043/800], Step[0100/0626], Avg Loss: 0.7263
INFO:local_logger:Epoch[043/800], Step[0200/0626], Avg Loss: 0.7260
INFO:local_logger:Epoch[043/800], Step[0200/0626], Avg Loss: 0.7257
INFO:local_logger:Epoch[043/800], Step[0200/0626], Avg Loss: 0.7268
INFO:local_logger:Epoch[043/800], Step[0200/0626], Avg Loss: 0.7256
INFO:local_logger:Epoch[043/800], Step[0200/0626], Avg Loss: 0.7263
INFO:local_logger:Epoch[043/800], Step[0200/0626], Avg Loss: 0.7260
INFO:master_logger:Epoch[043/800], Step[0200/0626], Avg Loss: 0.7260
INFO:local_logger:Epoch[043/800], Step[0200/0626], Avg Loss: 0.7257
INFO:local_logger:Epoch[043/800], Step[0200/0626], Avg Loss: 0.7258
INFO:local_logger:Epoch[043/800], Step[0300/0626], Avg Loss: 0.7257
INFO:local_logger:Epoch[043/800], Step[0300/0626], Avg Loss: 0.7262
INFO:master_logger:Epoch[043/800], Step[0300/0626], Avg Loss: 0.7259
INFO:local_logger:Epoch[043/800], Step[0300/0626], Avg Loss: 0.7260
INFO:local_logger:Epoch[043/800], Step[0300/0626], Avg Loss: 0.7256
INFO:local_logger:Epoch[043/800], Step[0300/0626], Avg Loss: 0.7264
INFO:local_logger:Epoch[043/800], Step[0300/0626], Avg Loss: 0.7262
INFO:local_logger:Epoch[043/800], Step[0300/0626], Avg Loss: 0.7257
INFO:local_logger:Epoch[043/800], Step[0300/0626], Avg Loss: 0.7256
INFO:local_logger:Epoch[043/800], Step[0400/0626], Avg Loss: 0.7257
INFO:local_logger:Epoch[043/800], Step[0400/0626], Avg Loss: 0.7261
INFO:local_logger:Epoch[043/800], Step[0400/0626], Avg Loss: 0.7257
INFO:master_logger:Epoch[043/800], Step[0400/0626], Avg Loss: 0.7258
INFO:local_logger:Epoch[043/800], Step[0400/0626], Avg Loss: 0.7257
INFO:local_logger:Epoch[043/800], Step[0400/0626], Avg Loss: 0.7254
INFO:local_logger:Epoch[043/800], Step[0400/0626], Avg Loss: 0.7253
INFO:local_logger:Epoch[043/800], Step[0400/0626], Avg Loss: 0.7260
INFO:local_logger:Epoch[043/800], Step[0400/0626], Avg Loss: 0.7263
INFO:local_logger:Epoch[043/800], Step[0500/0626], Avg Loss: 0.7253
INFO:local_logger:Epoch[043/800], Step[0500/0626], Avg Loss: 0.7257
INFO:local_logger:Epoch[043/800], Step[0500/0626], Avg Loss: 0.7255
INFO:local_logger:Epoch[043/800], Step[0500/0626], Avg Loss: 0.7258
INFO:local_logger:Epoch[043/800], Step[0500/0626], Avg Loss: 0.7260
INFO:master_logger:Epoch[043/800], Step[0500/0626], Avg Loss: 0.7256
INFO:local_logger:Epoch[043/800], Step[0500/0626], Avg Loss: 0.7253
INFO:local_logger:Epoch[043/800], Step[0500/0626], Avg Loss: 0.7255
INFO:local_logger:Epoch[043/800], Step[0500/0626], Avg Loss: 0.7259
INFO:local_logger:Epoch[043/800], Step[0600/0626], Avg Loss: 0.7256
INFO:local_logger:Epoch[043/800], Step[0600/0626], Avg Loss: 0.7255
INFO:local_logger:Epoch[043/800], Step[0600/0626], Avg Loss: 0.7258
INFO:local_logger:Epoch[043/800], Step[0600/0626], Avg Loss: 0.7258
INFO:local_logger:Epoch[043/800], Step[0600/0626], Avg Loss: 0.7253
INFO:master_logger:Epoch[043/800], Step[0600/0626], Avg Loss: 0.7256
INFO:local_logger:Epoch[043/800], Step[0600/0626], Avg Loss: 0.7254
INFO:local_logger:Epoch[043/800], Step[0600/0626], Avg Loss: 0.7252
INFO:local_logger:Epoch[043/800], Step[0600/0626], Avg Loss: 0.7258
INFO:local_logger:----- Epoch[043/800], Train Loss: 0.7254, time: 856.34
INFO:local_logger:Now training epoch 44. LR=0.000150
INFO:local_logger:----- Epoch[043/800], Train Loss: 0.7256, time: 855.79
INFO:local_logger:Now training epoch 44. LR=0.000150
INFO:local_logger:----- Epoch[043/800], Train Loss: 0.7253, time: 856.30
INFO:local_logger:Now training epoch 44. LR=0.000150
INFO:local_logger:----- Epoch[043/800], Train Loss: 0.7258, time: 856.24
INFO:local_logger:Now training epoch 44. LR=0.000150
INFO:local_logger:----- Epoch[043/800], Train Loss: 0.7256, time: 852.55
INFO:master_logger:----- Epoch[043/800], Train Loss: 0.7255, time: 852.55
INFO:local_logger:----- Epoch[043/800], Train Loss: 0.7252, time: 856.26
INFO:local_logger:Now training epoch 44. LR=0.000150
INFO:local_logger:----- Epoch[043/800], Train Loss: 0.7258, time: 856.83
INFO:local_logger:Now training epoch 44. LR=0.000150
INFO:local_logger:----- Epoch[043/800], Train Loss: 0.7257, time: 856.33
INFO:local_logger:Now training epoch 44. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-43-Loss-0.7255999422779928.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-43-Loss-0.7255999422779928.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-43-Loss-0.7255999422779928.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-43-Loss-0.7255999422779928.pdopt
INFO:local_logger:Now training epoch 44. LR=0.000150
INFO:master_logger:Now training epoch 44. LR=0.000150
INFO:local_logger:Epoch[044/800], Step[0000/0626], Avg Loss: 0.7265
INFO:local_logger:Epoch[044/800], Step[0000/0626], Avg Loss: 0.7228
INFO:master_logger:Epoch[044/800], Step[0000/0626], Avg Loss: 0.7233
INFO:local_logger:Epoch[044/800], Step[0000/0626], Avg Loss: 0.7197
INFO:local_logger:Epoch[044/800], Step[0000/0626], Avg Loss: 0.7206
INFO:local_logger:Epoch[044/800], Step[0000/0626], Avg Loss: 0.7307
INFO:local_logger:Epoch[044/800], Step[0000/0626], Avg Loss: 0.7300
INFO:local_logger:Epoch[044/800], Step[0000/0626], Avg Loss: 0.7199
INFO:local_logger:Epoch[044/800], Step[0000/0626], Avg Loss: 0.7161
INFO:local_logger:Epoch[044/800], Step[0100/0626], Avg Loss: 0.7253
INFO:local_logger:Epoch[044/800], Step[0100/0626], Avg Loss: 0.7241
INFO:local_logger:Epoch[044/800], Step[0100/0626], Avg Loss: 0.7232
INFO:local_logger:Epoch[044/800], Step[0100/0626], Avg Loss: 0.7238
INFO:local_logger:Epoch[044/800], Step[0100/0626], Avg Loss: 0.7251
INFO:local_logger:Epoch[044/800], Step[0100/0626], Avg Loss: 0.7248
INFO:local_logger:Epoch[044/800], Step[0100/0626], Avg Loss: 0.7246
INFO:local_logger:Epoch[044/800], Step[0100/0626], Avg Loss: 0.7250
INFO:master_logger:Epoch[044/800], Step[0100/0626], Avg Loss: 0.7245
INFO:local_logger:Epoch[044/800], Step[0200/0626], Avg Loss: 0.7238
INFO:local_logger:Epoch[044/800], Step[0200/0626], Avg Loss: 0.7246
INFO:local_logger:Epoch[044/800], Step[0200/0626], Avg Loss: 0.7237
INFO:local_logger:Epoch[044/800], Step[0200/0626], Avg Loss: 0.7243
INFO:local_logger:Epoch[044/800], Step[0200/0626], Avg Loss: 0.7241
INFO:local_logger:Epoch[044/800], Step[0200/0626], Avg Loss: 0.7251
INFO:master_logger:Epoch[044/800], Step[0200/0626], Avg Loss: 0.7243
INFO:local_logger:Epoch[044/800], Step[0200/0626], Avg Loss: 0.7249
INFO:local_logger:Epoch[044/800], Step[0200/0626], Avg Loss: 0.7239
INFO:local_logger:Epoch[044/800], Step[0300/0626], Avg Loss: 0.7239
INFO:local_logger:Epoch[044/800], Step[0300/0626], Avg Loss: 0.7241
INFO:master_logger:Epoch[044/800], Step[0300/0626], Avg Loss: 0.7240
INFO:local_logger:Epoch[044/800], Step[0300/0626], Avg Loss: 0.7239
INFO:local_logger:Epoch[044/800], Step[0300/0626], Avg Loss: 0.7241
INFO:local_logger:Epoch[044/800], Step[0300/0626], Avg Loss: 0.7235
INFO:local_logger:Epoch[044/800], Step[0300/0626], Avg Loss: 0.7242
INFO:local_logger:Epoch[044/800], Step[0300/0626], Avg Loss: 0.7247
INFO:local_logger:Epoch[044/800], Step[0300/0626], Avg Loss: 0.7236
INFO:local_logger:Epoch[044/800], Step[0400/0626], Avg Loss: 0.7238
INFO:local_logger:Epoch[044/800], Step[0400/0626], Avg Loss: 0.7236
INFO:local_logger:Epoch[044/800], Step[0400/0626], Avg Loss: 0.7241
INFO:master_logger:Epoch[044/800], Step[0400/0626], Avg Loss: 0.7238
INFO:local_logger:Epoch[044/800], Step[0400/0626], Avg Loss: 0.7236
INFO:local_logger:Epoch[044/800], Step[0400/0626], Avg Loss: 0.7233
INFO:local_logger:Epoch[044/800], Step[0400/0626], Avg Loss: 0.7243
INFO:local_logger:Epoch[044/800], Step[0400/0626], Avg Loss: 0.7240
INFO:local_logger:Epoch[044/800], Step[0400/0626], Avg Loss: 0.7239
INFO:local_logger:Epoch[044/800], Step[0500/0626], Avg Loss: 0.7237
INFO:local_logger:Epoch[044/800], Step[0500/0626], Avg Loss: 0.7240
INFO:local_logger:Epoch[044/800], Step[0500/0626], Avg Loss: 0.7234
INFO:master_logger:Epoch[044/800], Step[0500/0626], Avg Loss: 0.7236
INFO:local_logger:Epoch[044/800], Step[0500/0626], Avg Loss: 0.7238
INFO:local_logger:Epoch[044/800], Step[0500/0626], Avg Loss: 0.7231
INFO:local_logger:Epoch[044/800], Step[0500/0626], Avg Loss: 0.7238
INFO:local_logger:Epoch[044/800], Step[0500/0626], Avg Loss: 0.7237
INFO:local_logger:Epoch[044/800], Step[0500/0626], Avg Loss: 0.7235
INFO:local_logger:Epoch[044/800], Step[0600/0626], Avg Loss: 0.7236
INFO:local_logger:Epoch[044/800], Step[0600/0626], Avg Loss: 0.7237
INFO:local_logger:Epoch[044/800], Step[0600/0626], Avg Loss: 0.7235
INFO:local_logger:Epoch[044/800], Step[0600/0626], Avg Loss: 0.7235
INFO:local_logger:Epoch[044/800], Step[0600/0626], Avg Loss: 0.7231
INFO:master_logger:Epoch[044/800], Step[0600/0626], Avg Loss: 0.7235
INFO:local_logger:Epoch[044/800], Step[0600/0626], Avg Loss: 0.7237
INFO:local_logger:Epoch[044/800], Step[0600/0626], Avg Loss: 0.7235
INFO:local_logger:Epoch[044/800], Step[0600/0626], Avg Loss: 0.7233
INFO:local_logger:----- Epoch[044/800], Train Loss: 0.7236, time: 893.49
INFO:local_logger:Now training epoch 45. LR=0.000150
INFO:local_logger:----- Epoch[044/800], Train Loss: 0.7231, time: 893.57
INFO:local_logger:Now training epoch 45. LR=0.000150
INFO:local_logger:----- Epoch[044/800], Train Loss: 0.7234, time: 893.55
INFO:local_logger:Now training epoch 45. LR=0.000150
INFO:local_logger:----- Epoch[044/800], Train Loss: 0.7234, time: 893.57
INFO:local_logger:Now training epoch 45. LR=0.000150
INFO:local_logger:----- Epoch[044/800], Train Loss: 0.7235, time: 889.85
INFO:master_logger:----- Epoch[044/800], Train Loss: 0.7235, time: 889.85
INFO:local_logger:----- Epoch[044/800], Train Loss: 0.7237, time: 894.16
INFO:local_logger:Now training epoch 45. LR=0.000150
INFO:local_logger:----- Epoch[044/800], Train Loss: 0.7238, time: 894.14
INFO:local_logger:Now training epoch 45. LR=0.000150
INFO:local_logger:----- Epoch[044/800], Train Loss: 0.7235, time: 893.86
INFO:local_logger:Now training epoch 45. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-44-Loss-0.723522978396613.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-44-Loss-0.723522978396613.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-44-Loss-0.723522978396613.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-44-Loss-0.723522978396613.pdopt
INFO:local_logger:Now training epoch 45. LR=0.000150
INFO:master_logger:Now training epoch 45. LR=0.000150
INFO:local_logger:Epoch[045/800], Step[0000/0626], Avg Loss: 0.7270
INFO:local_logger:Epoch[045/800], Step[0000/0626], Avg Loss: 0.7223
INFO:local_logger:Epoch[045/800], Step[0000/0626], Avg Loss: 0.7158
INFO:master_logger:Epoch[045/800], Step[0000/0626], Avg Loss: 0.7205
INFO:local_logger:Epoch[045/800], Step[0000/0626], Avg Loss: 0.7192
INFO:local_logger:Epoch[045/800], Step[0000/0626], Avg Loss: 0.7172
INFO:local_logger:Epoch[045/800], Step[0000/0626], Avg Loss: 0.7222
INFO:local_logger:Epoch[045/800], Step[0000/0626], Avg Loss: 0.7194
INFO:local_logger:Epoch[045/800], Step[0000/0626], Avg Loss: 0.7208
INFO:local_logger:Epoch[045/800], Step[0100/0626], Avg Loss: 0.7217
INFO:local_logger:Epoch[045/800], Step[0100/0626], Avg Loss: 0.7221
INFO:local_logger:Epoch[045/800], Step[0100/0626], Avg Loss: 0.7229
INFO:local_logger:Epoch[045/800], Step[0100/0626], Avg Loss: 0.7221
INFO:local_logger:Epoch[045/800], Step[0100/0626], Avg Loss: 0.7228
INFO:local_logger:Epoch[045/800], Step[0100/0626], Avg Loss: 0.7213
INFO:master_logger:Epoch[045/800], Step[0100/0626], Avg Loss: 0.7223
INFO:local_logger:Epoch[045/800], Step[0100/0626], Avg Loss: 0.7222
INFO:local_logger:Epoch[045/800], Step[0100/0626], Avg Loss: 0.7230
INFO:local_logger:Epoch[045/800], Step[0200/0626], Avg Loss: 0.7218
INFO:local_logger:Epoch[045/800], Step[0200/0626], Avg Loss: 0.7221
INFO:local_logger:Epoch[045/800], Step[0200/0626], Avg Loss: 0.7231
INFO:local_logger:Epoch[045/800], Step[0200/0626], Avg Loss: 0.7222
INFO:local_logger:Epoch[045/800], Step[0200/0626], Avg Loss: 0.7228
INFO:local_logger:Epoch[045/800], Step[0200/0626], Avg Loss: 0.7222
INFO:local_logger:Epoch[045/800], Step[0200/0626], Avg Loss: 0.7222
INFO:master_logger:Epoch[045/800], Step[0200/0626], Avg Loss: 0.7223
INFO:local_logger:Epoch[045/800], Step[0200/0626], Avg Loss: 0.7219
INFO:local_logger:Epoch[045/800], Step[0300/0626], Avg Loss: 0.7219
INFO:local_logger:Epoch[045/800], Step[0300/0626], Avg Loss: 0.7220
INFO:local_logger:Epoch[045/800], Step[0300/0626], Avg Loss: 0.7218
INFO:local_logger:Epoch[045/800], Step[0300/0626], Avg Loss: 0.7219
INFO:local_logger:Epoch[045/800], Step[0300/0626], Avg Loss: 0.7220
INFO:master_logger:Epoch[045/800], Step[0300/0626], Avg Loss: 0.7219
INFO:local_logger:Epoch[045/800], Step[0300/0626], Avg Loss: 0.7227
INFO:local_logger:Epoch[045/800], Step[0300/0626], Avg Loss: 0.7213
INFO:local_logger:Epoch[045/800], Step[0300/0626], Avg Loss: 0.7219
INFO:local_logger:Epoch[045/800], Step[0400/0626], Avg Loss: 0.7217
INFO:local_logger:Epoch[045/800], Step[0400/0626], Avg Loss: 0.7217
INFO:local_logger:Epoch[045/800], Step[0400/0626], Avg Loss: 0.7217
INFO:master_logger:Epoch[045/800], Step[0400/0626], Avg Loss: 0.7218
INFO:local_logger:Epoch[045/800], Step[0400/0626], Avg Loss: 0.7212
INFO:local_logger:Epoch[045/800], Step[0400/0626], Avg Loss: 0.7219
INFO:local_logger:Epoch[045/800], Step[0400/0626], Avg Loss: 0.7220
INFO:local_logger:Epoch[045/800], Step[0400/0626], Avg Loss: 0.7217
INFO:local_logger:Epoch[045/800], Step[0400/0626], Avg Loss: 0.7224
INFO:local_logger:Epoch[045/800], Step[0500/0626], Avg Loss: 0.7217
INFO:local_logger:Epoch[045/800], Step[0500/0626], Avg Loss: 0.7217
INFO:local_logger:Epoch[045/800], Step[0500/0626], Avg Loss: 0.7220
INFO:master_logger:Epoch[045/800], Step[0500/0626], Avg Loss: 0.7218
INFO:local_logger:Epoch[045/800], Step[0500/0626], Avg Loss: 0.7216
INFO:local_logger:Epoch[045/800], Step[0500/0626], Avg Loss: 0.7218
INFO:local_logger:Epoch[045/800], Step[0500/0626], Avg Loss: 0.7213
INFO:local_logger:Epoch[045/800], Step[0500/0626], Avg Loss: 0.7217
INFO:local_logger:Epoch[045/800], Step[0500/0626], Avg Loss: 0.7223
INFO:local_logger:Epoch[045/800], Step[0600/0626], Avg Loss: 0.7212
INFO:master_logger:Epoch[045/800], Step[0600/0626], Avg Loss: 0.7216
INFO:local_logger:Epoch[045/800], Step[0600/0626], Avg Loss: 0.7216
INFO:local_logger:Epoch[045/800], Step[0600/0626], Avg Loss: 0.7216
INFO:local_logger:Epoch[045/800], Step[0600/0626], Avg Loss: 0.7220
INFO:local_logger:Epoch[045/800], Step[0600/0626], Avg Loss: 0.7217
INFO:local_logger:Epoch[045/800], Step[0600/0626], Avg Loss: 0.7219
INFO:local_logger:Epoch[045/800], Step[0600/0626], Avg Loss: 0.7214
INFO:local_logger:Epoch[045/800], Step[0600/0626], Avg Loss: 0.7213
INFO:local_logger:----- Epoch[045/800], Train Loss: 0.7212, time: 859.11
INFO:local_logger:Now training epoch 46. LR=0.000150
INFO:local_logger:----- Epoch[045/800], Train Loss: 0.7212, time: 855.42
INFO:master_logger:----- Epoch[045/800], Train Loss: 0.7216, time: 855.42
INFO:local_logger:----- Epoch[045/800], Train Loss: 0.7216, time: 859.01
INFO:local_logger:Now training epoch 46. LR=0.000150
INFO:local_logger:----- Epoch[045/800], Train Loss: 0.7216, time: 859.54
INFO:local_logger:Now training epoch 46. LR=0.000150
INFO:local_logger:----- Epoch[045/800], Train Loss: 0.7219, time: 859.82
INFO:local_logger:Now training epoch 46. LR=0.000150
INFO:local_logger:----- Epoch[045/800], Train Loss: 0.7218, time: 859.92
INFO:local_logger:----- Epoch[045/800], Train Loss: 0.7214, time: 859.73
INFO:local_logger:Now training epoch 46. LR=0.000150
INFO:local_logger:Now training epoch 46. LR=0.000150
INFO:local_logger:----- Epoch[045/800], Train Loss: 0.7217, time: 859.75
INFO:local_logger:Now training epoch 46. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-45-Loss-0.7212178304741391.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-45-Loss-0.7212178304741391.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-45-Loss-0.7212178304741391.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-45-Loss-0.7212178304741391.pdopt
INFO:local_logger:Now training epoch 46. LR=0.000150
INFO:master_logger:Now training epoch 46. LR=0.000150
INFO:local_logger:Epoch[046/800], Step[0000/0626], Avg Loss: 0.7165
INFO:local_logger:Epoch[046/800], Step[0000/0626], Avg Loss: 0.7255
INFO:local_logger:Epoch[046/800], Step[0000/0626], Avg Loss: 0.7341
INFO:master_logger:Epoch[046/800], Step[0000/0626], Avg Loss: 0.7218
INFO:local_logger:Epoch[046/800], Step[0000/0626], Avg Loss: 0.7214
INFO:local_logger:Epoch[046/800], Step[0000/0626], Avg Loss: 0.7314
INFO:local_logger:Epoch[046/800], Step[0000/0626], Avg Loss: 0.7055
INFO:local_logger:Epoch[046/800], Step[0000/0626], Avg Loss: 0.7244
INFO:local_logger:Epoch[046/800], Step[0000/0626], Avg Loss: 0.7156
INFO:local_logger:Epoch[046/800], Step[0100/0626], Avg Loss: 0.7201
INFO:local_logger:Epoch[046/800], Step[0100/0626], Avg Loss: 0.7210
INFO:local_logger:Epoch[046/800], Step[0100/0626], Avg Loss: 0.7219
INFO:master_logger:Epoch[046/800], Step[0100/0626], Avg Loss: 0.7206
INFO:local_logger:Epoch[046/800], Step[0100/0626], Avg Loss: 0.7207
INFO:local_logger:Epoch[046/800], Step[0100/0626], Avg Loss: 0.7203
INFO:local_logger:Epoch[046/800], Step[0100/0626], Avg Loss: 0.7207
INFO:local_logger:Epoch[046/800], Step[0100/0626], Avg Loss: 0.7196
INFO:local_logger:Epoch[046/800], Step[0100/0626], Avg Loss: 0.7207
INFO:local_logger:Epoch[046/800], Step[0200/0626], Avg Loss: 0.7191
INFO:local_logger:Epoch[046/800], Step[0200/0626], Avg Loss: 0.7207
INFO:local_logger:Epoch[046/800], Step[0200/0626], Avg Loss: 0.7206
INFO:local_logger:Epoch[046/800], Step[0200/0626], Avg Loss: 0.7199
INFO:local_logger:Epoch[046/800], Step[0200/0626], Avg Loss: 0.7209
INFO:local_logger:Epoch[046/800], Step[0200/0626], Avg Loss: 0.7203
INFO:local_logger:Epoch[046/800], Step[0200/0626], Avg Loss: 0.7207
INFO:master_logger:Epoch[046/800], Step[0200/0626], Avg Loss: 0.7204
INFO:local_logger:Epoch[046/800], Step[0200/0626], Avg Loss: 0.7208
INFO:local_logger:Epoch[046/800], Step[0300/0626], Avg Loss: 0.7199
INFO:local_logger:Epoch[046/800], Step[0300/0626], Avg Loss: 0.7205
INFO:local_logger:Epoch[046/800], Step[0300/0626], Avg Loss: 0.7204
INFO:local_logger:Epoch[046/800], Step[0300/0626], Avg Loss: 0.7199
INFO:local_logger:Epoch[046/800], Step[0300/0626], Avg Loss: 0.7201
INFO:local_logger:Epoch[046/800], Step[0300/0626], Avg Loss: 0.7202
INFO:local_logger:Epoch[046/800], Step[0300/0626], Avg Loss: 0.7202
INFO:local_logger:Epoch[046/800], Step[0300/0626], Avg Loss: 0.7202
INFO:master_logger:Epoch[046/800], Step[0300/0626], Avg Loss: 0.7202
INFO:local_logger:Epoch[046/800], Step[0400/0626], Avg Loss: 0.7200
INFO:local_logger:Epoch[046/800], Step[0400/0626], Avg Loss: 0.7198
INFO:local_logger:Epoch[046/800], Step[0400/0626], Avg Loss: 0.7205
INFO:local_logger:Epoch[046/800], Step[0400/0626], Avg Loss: 0.7202
INFO:master_logger:Epoch[046/800], Step[0400/0626], Avg Loss: 0.7200
INFO:local_logger:Epoch[046/800], Step[0400/0626], Avg Loss: 0.7199
INFO:local_logger:Epoch[046/800], Step[0400/0626], Avg Loss: 0.7201
INFO:local_logger:Epoch[046/800], Step[0400/0626], Avg Loss: 0.7198
INFO:local_logger:Epoch[046/800], Step[0400/0626], Avg Loss: 0.7200
INFO:local_logger:Epoch[046/800], Step[0500/0626], Avg Loss: 0.7197
INFO:local_logger:Epoch[046/800], Step[0500/0626], Avg Loss: 0.7200
INFO:local_logger:Epoch[046/800], Step[0500/0626], Avg Loss: 0.7201
INFO:local_logger:Epoch[046/800], Step[0500/0626], Avg Loss: 0.7198
INFO:local_logger:Epoch[046/800], Step[0500/0626], Avg Loss: 0.7198
INFO:local_logger:Epoch[046/800], Step[0500/0626], Avg Loss: 0.7198
INFO:master_logger:Epoch[046/800], Step[0500/0626], Avg Loss: 0.7199
INFO:local_logger:Epoch[046/800], Step[0500/0626], Avg Loss: 0.7199
INFO:local_logger:Epoch[046/800], Step[0500/0626], Avg Loss: 0.7204
INFO:local_logger:Epoch[046/800], Step[0600/0626], Avg Loss: 0.7197
INFO:local_logger:Epoch[046/800], Step[0600/0626], Avg Loss: 0.7198
INFO:local_logger:Epoch[046/800], Step[0600/0626], Avg Loss: 0.7203
INFO:local_logger:Epoch[046/800], Step[0600/0626], Avg Loss: 0.7197
INFO:local_logger:Epoch[046/800], Step[0600/0626], Avg Loss: 0.7198
INFO:local_logger:Epoch[046/800], Step[0600/0626], Avg Loss: 0.7198
INFO:local_logger:Epoch[046/800], Step[0600/0626], Avg Loss: 0.7199
INFO:master_logger:Epoch[046/800], Step[0600/0626], Avg Loss: 0.7198
INFO:local_logger:Epoch[046/800], Step[0600/0626], Avg Loss: 0.7197
INFO:local_logger:----- Epoch[046/800], Train Loss: 0.7199, time: 894.85
INFO:local_logger:Now training epoch 47. LR=0.000150
INFO:local_logger:----- Epoch[046/800], Train Loss: 0.7197, time: 894.90
INFO:local_logger:Now training epoch 47. LR=0.000150
INFO:local_logger:----- Epoch[046/800], Train Loss: 0.7198, time: 895.63
INFO:local_logger:Now training epoch 47. LR=0.000150
INFO:local_logger:----- Epoch[046/800], Train Loss: 0.7197, time: 895.84
INFO:local_logger:Now training epoch 47. LR=0.000150
INFO:local_logger:----- Epoch[046/800], Train Loss: 0.7197, time: 895.79
INFO:local_logger:Now training epoch 47. LR=0.000150
INFO:local_logger:----- Epoch[046/800], Train Loss: 0.7198, time: 892.44
INFO:master_logger:----- Epoch[046/800], Train Loss: 0.7198, time: 892.44
INFO:local_logger:----- Epoch[046/800], Train Loss: 0.7198, time: 895.48
INFO:local_logger:Now training epoch 47. LR=0.000150
INFO:local_logger:----- Epoch[046/800], Train Loss: 0.7204, time: 895.48
INFO:local_logger:Now training epoch 47. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-46-Loss-0.7197591380592546.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-46-Loss-0.7197591380592546.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-46-Loss-0.7197591380592546.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-46-Loss-0.7197591380592546.pdopt
INFO:local_logger:Now training epoch 47. LR=0.000150
INFO:master_logger:Now training epoch 47. LR=0.000150
INFO:local_logger:Epoch[047/800], Step[0000/0626], Avg Loss: 0.7141
INFO:local_logger:Epoch[047/800], Step[0000/0626], Avg Loss: 0.7210
INFO:local_logger:Epoch[047/800], Step[0000/0626], Avg Loss: 0.7275
INFO:local_logger:Epoch[047/800], Step[0000/0626], Avg Loss: 0.7216
INFO:local_logger:Epoch[047/800], Step[0000/0626], Avg Loss: 0.7156
INFO:local_logger:Epoch[047/800], Step[0000/0626], Avg Loss: 0.7147
INFO:local_logger:Epoch[047/800], Step[0000/0626], Avg Loss: 0.7261
INFO:local_logger:Epoch[047/800], Step[0000/0626], Avg Loss: 0.7192
INFO:master_logger:Epoch[047/800], Step[0000/0626], Avg Loss: 0.7199
INFO:local_logger:Epoch[047/800], Step[0100/0626], Avg Loss: 0.7184
INFO:local_logger:Epoch[047/800], Step[0100/0626], Avg Loss: 0.7187
INFO:master_logger:Epoch[047/800], Step[0100/0626], Avg Loss: 0.7189
INFO:local_logger:Epoch[047/800], Step[0100/0626], Avg Loss: 0.7194
INFO:local_logger:Epoch[047/800], Step[0100/0626], Avg Loss: 0.7188
INFO:local_logger:Epoch[047/800], Step[0100/0626], Avg Loss: 0.7183
INFO:local_logger:Epoch[047/800], Step[0100/0626], Avg Loss: 0.7188
INFO:local_logger:Epoch[047/800], Step[0100/0626], Avg Loss: 0.7189
INFO:local_logger:Epoch[047/800], Step[0100/0626], Avg Loss: 0.7196
INFO:local_logger:Epoch[047/800], Step[0200/0626], Avg Loss: 0.7184
INFO:local_logger:Epoch[047/800], Step[0200/0626], Avg Loss: 0.7190
INFO:local_logger:Epoch[047/800], Step[0200/0626], Avg Loss: 0.7188
INFO:local_logger:Epoch[047/800], Step[0200/0626], Avg Loss: 0.7180
INFO:master_logger:Epoch[047/800], Step[0200/0626], Avg Loss: 0.7188
INFO:local_logger:Epoch[047/800], Step[0200/0626], Avg Loss: 0.7185
INFO:local_logger:Epoch[047/800], Step[0200/0626], Avg Loss: 0.7195
INFO:local_logger:Epoch[047/800], Step[0200/0626], Avg Loss: 0.7186
INFO:local_logger:Epoch[047/800], Step[0200/0626], Avg Loss: 0.7192
INFO:local_logger:Epoch[047/800], Step[0300/0626], Avg Loss: 0.7188
INFO:local_logger:Epoch[047/800], Step[0300/0626], Avg Loss: 0.7188
INFO:local_logger:Epoch[047/800], Step[0300/0626], Avg Loss: 0.7182
INFO:local_logger:Epoch[047/800], Step[0300/0626], Avg Loss: 0.7187
INFO:local_logger:Epoch[047/800], Step[0300/0626], Avg Loss: 0.7182
INFO:local_logger:Epoch[047/800], Step[0300/0626], Avg Loss: 0.7179
INFO:local_logger:Epoch[047/800], Step[0300/0626], Avg Loss: 0.7184
INFO:master_logger:Epoch[047/800], Step[0300/0626], Avg Loss: 0.7184
INFO:local_logger:Epoch[047/800], Step[0300/0626], Avg Loss: 0.7184
INFO:local_logger:Epoch[047/800], Step[0400/0626], Avg Loss: 0.7188
INFO:local_logger:Epoch[047/800], Step[0400/0626], Avg Loss: 0.7183
INFO:local_logger:Epoch[047/800], Step[0400/0626], Avg Loss: 0.7180
INFO:local_logger:Epoch[047/800], Step[0400/0626], Avg Loss: 0.7188
INFO:master_logger:Epoch[047/800], Step[0400/0626], Avg Loss: 0.7184
INFO:local_logger:Epoch[047/800], Step[0400/0626], Avg Loss: 0.7183
INFO:local_logger:Epoch[047/800], Step[0400/0626], Avg Loss: 0.7185
INFO:local_logger:Epoch[047/800], Step[0400/0626], Avg Loss: 0.7188
INFO:local_logger:Epoch[047/800], Step[0400/0626], Avg Loss: 0.7178
INFO:local_logger:Epoch[047/800], Step[0500/0626], Avg Loss: 0.7187
INFO:local_logger:Epoch[047/800], Step[0500/0626], Avg Loss: 0.7189
INFO:local_logger:Epoch[047/800], Step[0500/0626], Avg Loss: 0.7179
INFO:local_logger:Epoch[047/800], Step[0500/0626], Avg Loss: 0.7187
INFO:local_logger:Epoch[047/800], Step[0500/0626], Avg Loss: 0.7182
INFO:master_logger:Epoch[047/800], Step[0500/0626], Avg Loss: 0.7183
INFO:local_logger:Epoch[047/800], Step[0500/0626], Avg Loss: 0.7179
INFO:local_logger:Epoch[047/800], Step[0500/0626], Avg Loss: 0.7183
INFO:local_logger:Epoch[047/800], Step[0500/0626], Avg Loss: 0.7180
INFO:local_logger:Epoch[047/800], Step[0600/0626], Avg Loss: 0.7181
INFO:local_logger:Epoch[047/800], Step[0600/0626], Avg Loss: 0.7185
INFO:local_logger:Epoch[047/800], Step[0600/0626], Avg Loss: 0.7185
INFO:local_logger:Epoch[047/800], Step[0600/0626], Avg Loss: 0.7188
INFO:local_logger:Epoch[047/800], Step[0600/0626], Avg Loss: 0.7180
INFO:local_logger:Epoch[047/800], Step[0600/0626], Avg Loss: 0.7179
INFO:local_logger:Epoch[047/800], Step[0600/0626], Avg Loss: 0.7177
INFO:local_logger:Epoch[047/800], Step[0600/0626], Avg Loss: 0.7181
INFO:master_logger:Epoch[047/800], Step[0600/0626], Avg Loss: 0.7182
INFO:local_logger:----- Epoch[047/800], Train Loss: 0.7188, time: 864.28
INFO:local_logger:Now training epoch 48. LR=0.000150
INFO:local_logger:----- Epoch[047/800], Train Loss: 0.7180, time: 863.82
INFO:local_logger:Now training epoch 48. LR=0.000150
INFO:local_logger:----- Epoch[047/800], Train Loss: 0.7183, time: 863.91
INFO:local_logger:Now training epoch 48. LR=0.000150
INFO:local_logger:----- Epoch[047/800], Train Loss: 0.7185, time: 864.34
INFO:local_logger:Now training epoch 48. LR=0.000150
INFO:local_logger:----- Epoch[047/800], Train Loss: 0.7177, time: 864.67
INFO:local_logger:Now training epoch 48. LR=0.000150
INFO:local_logger:----- Epoch[047/800], Train Loss: 0.7179, time: 864.05
INFO:local_logger:Now training epoch 48. LR=0.000150
INFO:local_logger:----- Epoch[047/800], Train Loss: 0.7182, time: 864.42
INFO:local_logger:Now training epoch 48. LR=0.000150
INFO:local_logger:----- Epoch[047/800], Train Loss: 0.7181, time: 860.27
INFO:master_logger:----- Epoch[047/800], Train Loss: 0.7182, time: 860.27
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-47-Loss-0.7181137765215982.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-47-Loss-0.7181137765215982.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-47-Loss-0.7181137765215982.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-47-Loss-0.7181137765215982.pdopt
INFO:local_logger:Now training epoch 48. LR=0.000150
INFO:master_logger:Now training epoch 48. LR=0.000150
INFO:local_logger:Epoch[048/800], Step[0000/0626], Avg Loss: 0.7296
INFO:local_logger:Epoch[048/800], Step[0000/0626], Avg Loss: 0.7261
INFO:local_logger:Epoch[048/800], Step[0000/0626], Avg Loss: 0.7148
INFO:local_logger:Epoch[048/800], Step[0000/0626], Avg Loss: 0.7056
INFO:local_logger:Epoch[048/800], Step[0000/0626], Avg Loss: 0.7238
INFO:master_logger:Epoch[048/800], Step[0000/0626], Avg Loss: 0.7205
INFO:local_logger:Epoch[048/800], Step[0000/0626], Avg Loss: 0.7255
INFO:local_logger:Epoch[048/800], Step[0000/0626], Avg Loss: 0.7134
INFO:local_logger:Epoch[048/800], Step[0000/0626], Avg Loss: 0.7255
INFO:local_logger:Epoch[048/800], Step[0100/0626], Avg Loss: 0.7177
INFO:local_logger:Epoch[048/800], Step[0100/0626], Avg Loss: 0.7189
INFO:local_logger:Epoch[048/800], Step[0100/0626], Avg Loss: 0.7174
INFO:local_logger:Epoch[048/800], Step[0100/0626], Avg Loss: 0.7174
INFO:local_logger:Epoch[048/800], Step[0100/0626], Avg Loss: 0.7175
INFO:master_logger:Epoch[048/800], Step[0100/0626], Avg Loss: 0.7177
INFO:local_logger:Epoch[048/800], Step[0100/0626], Avg Loss: 0.7172
INFO:local_logger:Epoch[048/800], Step[0100/0626], Avg Loss: 0.7192
INFO:local_logger:Epoch[048/800], Step[0100/0626], Avg Loss: 0.7164
INFO:local_logger:Epoch[048/800], Step[0200/0626], Avg Loss: 0.7168
INFO:local_logger:Epoch[048/800], Step[0200/0626], Avg Loss: 0.7174
INFO:local_logger:Epoch[048/800], Step[0200/0626], Avg Loss: 0.7176
INFO:local_logger:Epoch[048/800], Step[0200/0626], Avg Loss: 0.7172
INFO:local_logger:Epoch[048/800], Step[0200/0626], Avg Loss: 0.7181
INFO:local_logger:Epoch[048/800], Step[0200/0626], Avg Loss: 0.7168
INFO:local_logger:Epoch[048/800], Step[0200/0626], Avg Loss: 0.7172
INFO:master_logger:Epoch[048/800], Step[0200/0626], Avg Loss: 0.7174
INFO:local_logger:Epoch[048/800], Step[0200/0626], Avg Loss: 0.7183
INFO:local_logger:Epoch[048/800], Step[0300/0626], Avg Loss: 0.7169
INFO:local_logger:Epoch[048/800], Step[0300/0626], Avg Loss: 0.7173
INFO:local_logger:Epoch[048/800], Step[0300/0626], Avg Loss: 0.7171
INFO:local_logger:Epoch[048/800], Step[0300/0626], Avg Loss: 0.7171
INFO:local_logger:Epoch[048/800], Step[0300/0626], Avg Loss: 0.7168
INFO:master_logger:Epoch[048/800], Step[0300/0626], Avg Loss: 0.7170
INFO:local_logger:Epoch[048/800], Step[0300/0626], Avg Loss: 0.7172
INFO:local_logger:Epoch[048/800], Step[0300/0626], Avg Loss: 0.7164
INFO:local_logger:Epoch[048/800], Step[0300/0626], Avg Loss: 0.7176
INFO:local_logger:Epoch[048/800], Step[0400/0626], Avg Loss: 0.7168
INFO:local_logger:Epoch[048/800], Step[0400/0626], Avg Loss: 0.7168
INFO:local_logger:Epoch[048/800], Step[0400/0626], Avg Loss: 0.7171
INFO:local_logger:Epoch[048/800], Step[0400/0626], Avg Loss: 0.7171
INFO:local_logger:Epoch[048/800], Step[0400/0626], Avg Loss: 0.7168
INFO:local_logger:Epoch[048/800], Step[0400/0626], Avg Loss: 0.7172
INFO:master_logger:Epoch[048/800], Step[0400/0626], Avg Loss: 0.7170
INFO:local_logger:Epoch[048/800], Step[0400/0626], Avg Loss: 0.7173
INFO:local_logger:Epoch[048/800], Step[0400/0626], Avg Loss: 0.7170
INFO:local_logger:Epoch[048/800], Step[0500/0626], Avg Loss: 0.7171
INFO:local_logger:Epoch[048/800], Step[0500/0626], Avg Loss: 0.7168
INFO:local_logger:Epoch[048/800], Step[0500/0626], Avg Loss: 0.7169
INFO:local_logger:Epoch[048/800], Step[0500/0626], Avg Loss: 0.7167
INFO:local_logger:Epoch[048/800], Step[0500/0626], Avg Loss: 0.7169
INFO:master_logger:Epoch[048/800], Step[0500/0626], Avg Loss: 0.7168
INFO:local_logger:Epoch[048/800], Step[0500/0626], Avg Loss: 0.7168
INFO:local_logger:Epoch[048/800], Step[0500/0626], Avg Loss: 0.7164
INFO:local_logger:Epoch[048/800], Step[0500/0626], Avg Loss: 0.7170
INFO:local_logger:Epoch[048/800], Step[0600/0626], Avg Loss: 0.7165
INFO:local_logger:Epoch[048/800], Step[0600/0626], Avg Loss: 0.7168
INFO:local_logger:Epoch[048/800], Step[0600/0626], Avg Loss: 0.7168
INFO:local_logger:Epoch[048/800], Step[0600/0626], Avg Loss: 0.7162
INFO:local_logger:Epoch[048/800], Step[0600/0626], Avg Loss: 0.7166
INFO:local_logger:Epoch[048/800], Step[0600/0626], Avg Loss: 0.7168
INFO:master_logger:Epoch[048/800], Step[0600/0626], Avg Loss: 0.7166
INFO:local_logger:Epoch[048/800], Step[0600/0626], Avg Loss: 0.7166
INFO:local_logger:Epoch[048/800], Step[0600/0626], Avg Loss: 0.7168
INFO:local_logger:----- Epoch[048/800], Train Loss: 0.7162, time: 894.29
INFO:local_logger:Now training epoch 49. LR=0.000150
INFO:local_logger:----- Epoch[048/800], Train Loss: 0.7165, time: 894.54
INFO:local_logger:Now training epoch 49. LR=0.000150
INFO:local_logger:----- Epoch[048/800], Train Loss: 0.7169, time: 894.25
INFO:local_logger:Now training epoch 49. LR=0.000150
INFO:local_logger:----- Epoch[048/800], Train Loss: 0.7165, time: 894.58
INFO:local_logger:Now training epoch 49. LR=0.000150
INFO:local_logger:----- Epoch[048/800], Train Loss: 0.7168, time: 894.96
INFO:local_logger:Now training epoch 49. LR=0.000150
INFO:local_logger:----- Epoch[048/800], Train Loss: 0.7165, time: 891.19
INFO:local_logger:----- Epoch[048/800], Train Loss: 0.7169, time: 894.94
INFO:master_logger:----- Epoch[048/800], Train Loss: 0.7166, time: 891.19
INFO:local_logger:Now training epoch 49. LR=0.000150
INFO:local_logger:----- Epoch[048/800], Train Loss: 0.7167, time: 895.17
INFO:local_logger:Now training epoch 49. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-48-Loss-0.7164831838117034.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-48-Loss-0.7164831838117034.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-48-Loss-0.7164831838117034.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-48-Loss-0.7164831838117034.pdopt
INFO:local_logger:Now training epoch 49. LR=0.000150
INFO:master_logger:Now training epoch 49. LR=0.000150
INFO:local_logger:Epoch[049/800], Step[0000/0626], Avg Loss: 0.7106
INFO:local_logger:Epoch[049/800], Step[0000/0626], Avg Loss: 0.7079
INFO:local_logger:Epoch[049/800], Step[0000/0626], Avg Loss: 0.7136
INFO:master_logger:Epoch[049/800], Step[0000/0626], Avg Loss: 0.7139
INFO:local_logger:Epoch[049/800], Step[0000/0626], Avg Loss: 0.7094
INFO:local_logger:Epoch[049/800], Step[0000/0626], Avg Loss: 0.7180
INFO:local_logger:Epoch[049/800], Step[0000/0626], Avg Loss: 0.7324
INFO:local_logger:Epoch[049/800], Step[0000/0626], Avg Loss: 0.7095
INFO:local_logger:Epoch[049/800], Step[0000/0626], Avg Loss: 0.7095
INFO:local_logger:Epoch[049/800], Step[0100/0626], Avg Loss: 0.7149
INFO:local_logger:Epoch[049/800], Step[0100/0626], Avg Loss: 0.7149
INFO:local_logger:Epoch[049/800], Step[0100/0626], Avg Loss: 0.7153
INFO:local_logger:Epoch[049/800], Step[0100/0626], Avg Loss: 0.7157
INFO:local_logger:Epoch[049/800], Step[0100/0626], Avg Loss: 0.7161
INFO:local_logger:Epoch[049/800], Step[0100/0626], Avg Loss: 0.7156
INFO:master_logger:Epoch[049/800], Step[0100/0626], Avg Loss: 0.7153
INFO:local_logger:Epoch[049/800], Step[0100/0626], Avg Loss: 0.7150
INFO:local_logger:Epoch[049/800], Step[0100/0626], Avg Loss: 0.7151
INFO:local_logger:Epoch[049/800], Step[0200/0626], Avg Loss: 0.7156
INFO:local_logger:Epoch[049/800], Step[0200/0626], Avg Loss: 0.7158
INFO:local_logger:Epoch[049/800], Step[0200/0626], Avg Loss: 0.7151
INFO:local_logger:Epoch[049/800], Step[0200/0626], Avg Loss: 0.7161
INFO:local_logger:Epoch[049/800], Step[0200/0626], Avg Loss: 0.7155
INFO:local_logger:Epoch[049/800], Step[0200/0626], Avg Loss: 0.7153
INFO:local_logger:Epoch[049/800], Step[0200/0626], Avg Loss: 0.7156
INFO:local_logger:Epoch[049/800], Step[0200/0626], Avg Loss: 0.7152
INFO:master_logger:Epoch[049/800], Step[0200/0626], Avg Loss: 0.7155
INFO:local_logger:Epoch[049/800], Step[0300/0626], Avg Loss: 0.7154
INFO:local_logger:Epoch[049/800], Step[0300/0626], Avg Loss: 0.7155
INFO:local_logger:Epoch[049/800], Step[0300/0626], Avg Loss: 0.7153
INFO:local_logger:Epoch[049/800], Step[0300/0626], Avg Loss: 0.7159
INFO:local_logger:Epoch[049/800], Step[0300/0626], Avg Loss: 0.7162
INFO:master_logger:Epoch[049/800], Step[0300/0626], Avg Loss: 0.7156
INFO:local_logger:Epoch[049/800], Step[0300/0626], Avg Loss: 0.7157
INFO:local_logger:Epoch[049/800], Step[0300/0626], Avg Loss: 0.7152
INFO:local_logger:Epoch[049/800], Step[0300/0626], Avg Loss: 0.7156
INFO:local_logger:Epoch[049/800], Step[0400/0626], Avg Loss: 0.7154
INFO:local_logger:Epoch[049/800], Step[0400/0626], Avg Loss: 0.7155
INFO:local_logger:Epoch[049/800], Step[0400/0626], Avg Loss: 0.7150
INFO:local_logger:Epoch[049/800], Step[0400/0626], Avg Loss: 0.7154
INFO:local_logger:Epoch[049/800], Step[0400/0626], Avg Loss: 0.7154
INFO:local_logger:Epoch[049/800], Step[0400/0626], Avg Loss: 0.7160
INFO:local_logger:Epoch[049/800], Step[0400/0626], Avg Loss: 0.7154
INFO:local_logger:Epoch[049/800], Step[0400/0626], Avg Loss: 0.7158
INFO:master_logger:Epoch[049/800], Step[0400/0626], Avg Loss: 0.7155
INFO:local_logger:Epoch[049/800], Step[0500/0626], Avg Loss: 0.7151
INFO:local_logger:Epoch[049/800], Step[0500/0626], Avg Loss: 0.7156
INFO:local_logger:Epoch[049/800], Step[0500/0626], Avg Loss: 0.7154
INFO:local_logger:Epoch[049/800], Step[0500/0626], Avg Loss: 0.7154
INFO:local_logger:Epoch[049/800], Step[0500/0626], Avg Loss: 0.7155
INFO:local_logger:Epoch[049/800], Step[0500/0626], Avg Loss: 0.7156
INFO:master_logger:Epoch[049/800], Step[0500/0626], Avg Loss: 0.7154
INFO:local_logger:Epoch[049/800], Step[0500/0626], Avg Loss: 0.7153
INFO:local_logger:Epoch[049/800], Step[0500/0626], Avg Loss: 0.7154
INFO:local_logger:Epoch[049/800], Step[0600/0626], Avg Loss: 0.7151
INFO:local_logger:Epoch[049/800], Step[0600/0626], Avg Loss: 0.7151
INFO:local_logger:Epoch[049/800], Step[0600/0626], Avg Loss: 0.7153
INFO:local_logger:Epoch[049/800], Step[0600/0626], Avg Loss: 0.7154
INFO:local_logger:Epoch[049/800], Step[0600/0626], Avg Loss: 0.7152
INFO:local_logger:Epoch[049/800], Step[0600/0626], Avg Loss: 0.7151
INFO:local_logger:Epoch[049/800], Step[0600/0626], Avg Loss: 0.7150
INFO:master_logger:Epoch[049/800], Step[0600/0626], Avg Loss: 0.7152
INFO:local_logger:Epoch[049/800], Step[0600/0626], Avg Loss: 0.7153
INFO:local_logger:----- Epoch[049/800], Train Loss: 0.7150, time: 858.69
INFO:local_logger:Now training epoch 50. LR=0.000150
INFO:local_logger:----- Epoch[049/800], Train Loss: 0.7151, time: 857.91
INFO:local_logger:Now training epoch 50. LR=0.000150
INFO:local_logger:----- Epoch[049/800], Train Loss: 0.7152, time: 858.69
INFO:local_logger:Now training epoch 50. LR=0.000150
INFO:local_logger:----- Epoch[049/800], Train Loss: 0.7152, time: 857.99
INFO:local_logger:Now training epoch 50. LR=0.000150
INFO:local_logger:----- Epoch[049/800], Train Loss: 0.7151, time: 858.78
INFO:local_logger:Now training epoch 50. LR=0.000150
INFO:local_logger:----- Epoch[049/800], Train Loss: 0.7151, time: 858.00
INFO:local_logger:Now training epoch 50. LR=0.000150
INFO:local_logger:----- Epoch[049/800], Train Loss: 0.7154, time: 858.39
INFO:local_logger:Now training epoch 50. LR=0.000150
INFO:local_logger:----- Epoch[049/800], Train Loss: 0.7153, time: 854.33
INFO:master_logger:----- Epoch[049/800], Train Loss: 0.7152, time: 854.33
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-49-Loss-0.715259619077928.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-49-Loss-0.715259619077928.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-49-Loss-0.715259619077928.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-49-Loss-0.715259619077928.pdopt
INFO:local_logger:Now training epoch 50. LR=0.000150
INFO:master_logger:Now training epoch 50. LR=0.000150
INFO:local_logger:Epoch[050/800], Step[0000/0626], Avg Loss: 0.7160
INFO:local_logger:Epoch[050/800], Step[0000/0626], Avg Loss: 0.7233
INFO:master_logger:Epoch[050/800], Step[0000/0626], Avg Loss: 0.7153
INFO:local_logger:Epoch[050/800], Step[0000/0626], Avg Loss: 0.7204
INFO:local_logger:Epoch[050/800], Step[0000/0626], Avg Loss: 0.7224
INFO:local_logger:Epoch[050/800], Step[0000/0626], Avg Loss: 0.7126
INFO:local_logger:Epoch[050/800], Step[0000/0626], Avg Loss: 0.7130
INFO:local_logger:Epoch[050/800], Step[0000/0626], Avg Loss: 0.7084
INFO:local_logger:Epoch[050/800], Step[0000/0626], Avg Loss: 0.7066
INFO:local_logger:Epoch[050/800], Step[0100/0626], Avg Loss: 0.7145
INFO:local_logger:Epoch[050/800], Step[0100/0626], Avg Loss: 0.7148
INFO:local_logger:Epoch[050/800], Step[0100/0626], Avg Loss: 0.7134
INFO:local_logger:Epoch[050/800], Step[0100/0626], Avg Loss: 0.7152
INFO:local_logger:Epoch[050/800], Step[0100/0626], Avg Loss: 0.7152
INFO:local_logger:Epoch[050/800], Step[0100/0626], Avg Loss: 0.7142
INFO:master_logger:Epoch[050/800], Step[0100/0626], Avg Loss: 0.7145
INFO:local_logger:Epoch[050/800], Step[0100/0626], Avg Loss: 0.7144
INFO:local_logger:Epoch[050/800], Step[0100/0626], Avg Loss: 0.7141
INFO:local_logger:Epoch[050/800], Step[0200/0626], Avg Loss: 0.7140
INFO:local_logger:Epoch[050/800], Step[0200/0626], Avg Loss: 0.7143
INFO:local_logger:Epoch[050/800], Step[0200/0626], Avg Loss: 0.7138
INFO:local_logger:Epoch[050/800], Step[0200/0626], Avg Loss: 0.7142
INFO:local_logger:Epoch[050/800], Step[0200/0626], Avg Loss: 0.7137
INFO:local_logger:Epoch[050/800], Step[0200/0626], Avg Loss: 0.7148
INFO:master_logger:Epoch[050/800], Step[0200/0626], Avg Loss: 0.7142
INFO:local_logger:Epoch[050/800], Step[0200/0626], Avg Loss: 0.7145
INFO:local_logger:Epoch[050/800], Step[0200/0626], Avg Loss: 0.7143
INFO:local_logger:Epoch[050/800], Step[0300/0626], Avg Loss: 0.7144
INFO:local_logger:Epoch[050/800], Step[0300/0626], Avg Loss: 0.7141
INFO:local_logger:Epoch[050/800], Step[0300/0626], Avg Loss: 0.7143
INFO:local_logger:Epoch[050/800], Step[0300/0626], Avg Loss: 0.7141
INFO:local_logger:Epoch[050/800], Step[0300/0626], Avg Loss: 0.7143
INFO:local_logger:Epoch[050/800], Step[0300/0626], Avg Loss: 0.7140
INFO:local_logger:Epoch[050/800], Step[0300/0626], Avg Loss: 0.7137
INFO:local_logger:Epoch[050/800], Step[0300/0626], Avg Loss: 0.7137
INFO:master_logger:Epoch[050/800], Step[0300/0626], Avg Loss: 0.7141
INFO:local_logger:Epoch[050/800], Step[0400/0626], Avg Loss: 0.7140
INFO:local_logger:Epoch[050/800], Step[0400/0626], Avg Loss: 0.7139
INFO:local_logger:Epoch[050/800], Step[0400/0626], Avg Loss: 0.7143
INFO:local_logger:Epoch[050/800], Step[0400/0626], Avg Loss: 0.7136
INFO:local_logger:Epoch[050/800], Step[0400/0626], Avg Loss: 0.7137
INFO:local_logger:Epoch[050/800], Step[0400/0626], Avg Loss: 0.7139
INFO:local_logger:Epoch[050/800], Step[0400/0626], Avg Loss: 0.7139
INFO:local_logger:Epoch[050/800], Step[0400/0626], Avg Loss: 0.7142
INFO:master_logger:Epoch[050/800], Step[0400/0626], Avg Loss: 0.7139
INFO:local_logger:Epoch[050/800], Step[0500/0626], Avg Loss: 0.7137
INFO:local_logger:Epoch[050/800], Step[0500/0626], Avg Loss: 0.7140
INFO:local_logger:Epoch[050/800], Step[0500/0626], Avg Loss: 0.7142
INFO:local_logger:Epoch[050/800], Step[0500/0626], Avg Loss: 0.7140
INFO:local_logger:Epoch[050/800], Step[0500/0626], Avg Loss: 0.7140
INFO:local_logger:Epoch[050/800], Step[0500/0626], Avg Loss: 0.7137
INFO:master_logger:Epoch[050/800], Step[0500/0626], Avg Loss: 0.7139
INFO:local_logger:Epoch[050/800], Step[0500/0626], Avg Loss: 0.7136
INFO:local_logger:Epoch[050/800], Step[0500/0626], Avg Loss: 0.7140
INFO:local_logger:Epoch[050/800], Step[0600/0626], Avg Loss: 0.7140
INFO:local_logger:Epoch[050/800], Step[0600/0626], Avg Loss: 0.7139
INFO:local_logger:Epoch[050/800], Step[0600/0626], Avg Loss: 0.7142
INFO:local_logger:Epoch[050/800], Step[0600/0626], Avg Loss: 0.7138
INFO:local_logger:Epoch[050/800], Step[0600/0626], Avg Loss: 0.7141
INFO:local_logger:Epoch[050/800], Step[0600/0626], Avg Loss: 0.7141
INFO:local_logger:Epoch[050/800], Step[0600/0626], Avg Loss: 0.7141
INFO:master_logger:Epoch[050/800], Step[0600/0626], Avg Loss: 0.7140
INFO:local_logger:Epoch[050/800], Step[0600/0626], Avg Loss: 0.7141
INFO:local_logger:----- Epoch[050/800], Train Loss: 0.7141, time: 882.33
INFO:local_logger:Now training epoch 51. LR=0.000150
INFO:local_logger:----- Epoch[050/800], Train Loss: 0.7141, time: 882.32
INFO:local_logger:Now training epoch 51. LR=0.000150
INFO:local_logger:----- Epoch[050/800], Train Loss: 0.7139, time: 878.40
INFO:master_logger:----- Epoch[050/800], Train Loss: 0.7140, time: 878.40
INFO:local_logger:----- Epoch[050/800], Train Loss: 0.7144, time: 882.28
INFO:local_logger:Now training epoch 51. LR=0.000150
INFO:local_logger:----- Epoch[050/800], Train Loss: 0.7139, time: 882.66
INFO:local_logger:Now training epoch 51. LR=0.000150
INFO:local_logger:----- Epoch[050/800], Train Loss: 0.7140, time: 882.66
INFO:local_logger:Now training epoch 51. LR=0.000150
INFO:local_logger:----- Epoch[050/800], Train Loss: 0.7138, time: 882.67
INFO:local_logger:Now training epoch 51. LR=0.000150
INFO:local_logger:----- Epoch[050/800], Train Loss: 0.7140, time: 882.77
INFO:local_logger:Now training epoch 51. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-50-Loss-0.7138677369669435.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-50-Loss-0.7138677369669435.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-50-Loss-0.7138677369669435.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-50-Loss-0.7138677369669435.pdopt
INFO:local_logger:Now training epoch 51. LR=0.000150
INFO:master_logger:Now training epoch 51. LR=0.000150
INFO:local_logger:Epoch[051/800], Step[0000/0626], Avg Loss: 0.7092
INFO:local_logger:Epoch[051/800], Step[0000/0626], Avg Loss: 0.7069
INFO:local_logger:Epoch[051/800], Step[0000/0626], Avg Loss: 0.7248
INFO:local_logger:Epoch[051/800], Step[0000/0626], Avg Loss: 0.7198
INFO:local_logger:Epoch[051/800], Step[0000/0626], Avg Loss: 0.7048
INFO:master_logger:Epoch[051/800], Step[0000/0626], Avg Loss: 0.7142
INFO:local_logger:Epoch[051/800], Step[0000/0626], Avg Loss: 0.7200
INFO:local_logger:Epoch[051/800], Step[0000/0626], Avg Loss: 0.7138
INFO:local_logger:Epoch[051/800], Step[0000/0626], Avg Loss: 0.7145
INFO:local_logger:Epoch[051/800], Step[0100/0626], Avg Loss: 0.7135
INFO:local_logger:Epoch[051/800], Step[0100/0626], Avg Loss: 0.7127
INFO:local_logger:Epoch[051/800], Step[0100/0626], Avg Loss: 0.7135
INFO:local_logger:Epoch[051/800], Step[0100/0626], Avg Loss: 0.7137
INFO:local_logger:Epoch[051/800], Step[0100/0626], Avg Loss: 0.7118
INFO:local_logger:Epoch[051/800], Step[0100/0626], Avg Loss: 0.7132
INFO:master_logger:Epoch[051/800], Step[0100/0626], Avg Loss: 0.7129
INFO:local_logger:Epoch[051/800], Step[0100/0626], Avg Loss: 0.7122
INFO:local_logger:Epoch[051/800], Step[0100/0626], Avg Loss: 0.7129
INFO:local_logger:Epoch[051/800], Step[0200/0626], Avg Loss: 0.7131
INFO:local_logger:Epoch[051/800], Step[0200/0626], Avg Loss: 0.7137
INFO:local_logger:Epoch[051/800], Step[0200/0626], Avg Loss: 0.7133
INFO:local_logger:Epoch[051/800], Step[0200/0626], Avg Loss: 0.7130
INFO:local_logger:Epoch[051/800], Step[0200/0626], Avg Loss: 0.7130
INFO:local_logger:Epoch[051/800], Step[0200/0626], Avg Loss: 0.7133
INFO:local_logger:Epoch[051/800], Step[0200/0626], Avg Loss: 0.7129
INFO:master_logger:Epoch[051/800], Step[0200/0626], Avg Loss: 0.7130
INFO:local_logger:Epoch[051/800], Step[0200/0626], Avg Loss: 0.7120
INFO:local_logger:Epoch[051/800], Step[0300/0626], Avg Loss: 0.7131
INFO:local_logger:Epoch[051/800], Step[0300/0626], Avg Loss: 0.7130
INFO:local_logger:Epoch[051/800], Step[0300/0626], Avg Loss: 0.7125
INFO:master_logger:Epoch[051/800], Step[0300/0626], Avg Loss: 0.7129
INFO:local_logger:Epoch[051/800], Step[0300/0626], Avg Loss: 0.7131
INFO:local_logger:Epoch[051/800], Step[0300/0626], Avg Loss: 0.7127
INFO:local_logger:Epoch[051/800], Step[0300/0626], Avg Loss: 0.7129
INFO:local_logger:Epoch[051/800], Step[0300/0626], Avg Loss: 0.7128
INFO:local_logger:Epoch[051/800], Step[0300/0626], Avg Loss: 0.7129
INFO:local_logger:Epoch[051/800], Step[0400/0626], Avg Loss: 0.7126
INFO:local_logger:Epoch[051/800], Step[0400/0626], Avg Loss: 0.7126
INFO:local_logger:Epoch[051/800], Step[0400/0626], Avg Loss: 0.7130
INFO:local_logger:Epoch[051/800], Step[0400/0626], Avg Loss: 0.7130
INFO:local_logger:Epoch[051/800], Step[0400/0626], Avg Loss: 0.7128
INFO:local_logger:Epoch[051/800], Step[0400/0626], Avg Loss: 0.7122
INFO:master_logger:Epoch[051/800], Step[0400/0626], Avg Loss: 0.7128
INFO:local_logger:Epoch[051/800], Step[0400/0626], Avg Loss: 0.7131
INFO:local_logger:Epoch[051/800], Step[0400/0626], Avg Loss: 0.7127
INFO:local_logger:Epoch[051/800], Step[0500/0626], Avg Loss: 0.7124
INFO:local_logger:Epoch[051/800], Step[0500/0626], Avg Loss: 0.7128
INFO:local_logger:Epoch[051/800], Step[0500/0626], Avg Loss: 0.7127
INFO:local_logger:Epoch[051/800], Step[0500/0626], Avg Loss: 0.7122
INFO:local_logger:Epoch[051/800], Step[0500/0626], Avg Loss: 0.7129
INFO:local_logger:Epoch[051/800], Step[0500/0626], Avg Loss: 0.7129
INFO:master_logger:Epoch[051/800], Step[0500/0626], Avg Loss: 0.7126
INFO:local_logger:Epoch[051/800], Step[0500/0626], Avg Loss: 0.7126
INFO:local_logger:Epoch[051/800], Step[0500/0626], Avg Loss: 0.7126
INFO:local_logger:Epoch[051/800], Step[0600/0626], Avg Loss: 0.7126
INFO:local_logger:Epoch[051/800], Step[0600/0626], Avg Loss: 0.7128
INFO:local_logger:Epoch[051/800], Step[0600/0626], Avg Loss: 0.7125
INFO:local_logger:Epoch[051/800], Step[0600/0626], Avg Loss: 0.7128
INFO:local_logger:Epoch[051/800], Step[0600/0626], Avg Loss: 0.7128
INFO:local_logger:Epoch[051/800], Step[0600/0626], Avg Loss: 0.7122
INFO:local_logger:Epoch[051/800], Step[0600/0626], Avg Loss: 0.7120
INFO:master_logger:Epoch[051/800], Step[0600/0626], Avg Loss: 0.7125
INFO:local_logger:Epoch[051/800], Step[0600/0626], Avg Loss: 0.7125
INFO:local_logger:----- Epoch[051/800], Train Loss: 0.7127, time: 848.53
INFO:local_logger:Now training epoch 52. LR=0.000150
INFO:local_logger:----- Epoch[051/800], Train Loss: 0.7120, time: 845.42
INFO:master_logger:----- Epoch[051/800], Train Loss: 0.7125, time: 845.42
INFO:local_logger:----- Epoch[051/800], Train Loss: 0.7125, time: 849.57
INFO:local_logger:Now training epoch 52. LR=0.000150
INFO:local_logger:----- Epoch[051/800], Train Loss: 0.7126, time: 849.50
INFO:local_logger:Now training epoch 52. LR=0.000150
INFO:local_logger:----- Epoch[051/800], Train Loss: 0.7127, time: 849.57
INFO:local_logger:Now training epoch 52. LR=0.000150
INFO:local_logger:----- Epoch[051/800], Train Loss: 0.7126, time: 849.14
INFO:local_logger:Now training epoch 52. LR=0.000150
INFO:local_logger:----- Epoch[051/800], Train Loss: 0.7122, time: 849.16
INFO:local_logger:Now training epoch 52. LR=0.000150
INFO:local_logger:----- Epoch[051/800], Train Loss: 0.7126, time: 849.16
INFO:local_logger:Now training epoch 52. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-51-Loss-0.7120018708518765.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-51-Loss-0.7120018708518765.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-51-Loss-0.7120018708518765.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-51-Loss-0.7120018708518765.pdopt
INFO:local_logger:Now training epoch 52. LR=0.000150
INFO:master_logger:Now training epoch 52. LR=0.000150
INFO:local_logger:Epoch[052/800], Step[0000/0626], Avg Loss: 0.7075
INFO:local_logger:Epoch[052/800], Step[0000/0626], Avg Loss: 0.7092
INFO:local_logger:Epoch[052/800], Step[0000/0626], Avg Loss: 0.7105
INFO:master_logger:Epoch[052/800], Step[0000/0626], Avg Loss: 0.7076
INFO:local_logger:Epoch[052/800], Step[0000/0626], Avg Loss: 0.6988
INFO:local_logger:Epoch[052/800], Step[0000/0626], Avg Loss: 0.7034
INFO:local_logger:Epoch[052/800], Step[0000/0626], Avg Loss: 0.7050
INFO:local_logger:Epoch[052/800], Step[0000/0626], Avg Loss: 0.7185
INFO:local_logger:Epoch[052/800], Step[0000/0626], Avg Loss: 0.7081
INFO:local_logger:Epoch[052/800], Step[0100/0626], Avg Loss: 0.7119
INFO:local_logger:Epoch[052/800], Step[0100/0626], Avg Loss: 0.7123
INFO:local_logger:Epoch[052/800], Step[0100/0626], Avg Loss: 0.7110
INFO:local_logger:Epoch[052/800], Step[0100/0626], Avg Loss: 0.7121
INFO:local_logger:Epoch[052/800], Step[0100/0626], Avg Loss: 0.7118
INFO:local_logger:Epoch[052/800], Step[0100/0626], Avg Loss: 0.7113
INFO:local_logger:Epoch[052/800], Step[0100/0626], Avg Loss: 0.7116
INFO:master_logger:Epoch[052/800], Step[0100/0626], Avg Loss: 0.7117
INFO:local_logger:Epoch[052/800], Step[0100/0626], Avg Loss: 0.7119
INFO:local_logger:Epoch[052/800], Step[0200/0626], Avg Loss: 0.7110
INFO:local_logger:Epoch[052/800], Step[0200/0626], Avg Loss: 0.7116
INFO:local_logger:Epoch[052/800], Step[0200/0626], Avg Loss: 0.7112
INFO:local_logger:Epoch[052/800], Step[0200/0626], Avg Loss: 0.7122
INFO:local_logger:Epoch[052/800], Step[0200/0626], Avg Loss: 0.7114
INFO:local_logger:Epoch[052/800], Step[0200/0626], Avg Loss: 0.7115
INFO:local_logger:Epoch[052/800], Step[0200/0626], Avg Loss: 0.7111
INFO:master_logger:Epoch[052/800], Step[0200/0626], Avg Loss: 0.7115
INFO:local_logger:Epoch[052/800], Step[0200/0626], Avg Loss: 0.7123
INFO:local_logger:Epoch[052/800], Step[0300/0626], Avg Loss: 0.7116
INFO:local_logger:Epoch[052/800], Step[0300/0626], Avg Loss: 0.7112
INFO:local_logger:Epoch[052/800], Step[0300/0626], Avg Loss: 0.7117
INFO:local_logger:Epoch[052/800], Step[0300/0626], Avg Loss: 0.7109
INFO:local_logger:Epoch[052/800], Step[0300/0626], Avg Loss: 0.7114
INFO:master_logger:Epoch[052/800], Step[0300/0626], Avg Loss: 0.7113
INFO:local_logger:Epoch[052/800], Step[0300/0626], Avg Loss: 0.7111
INFO:local_logger:Epoch[052/800], Step[0300/0626], Avg Loss: 0.7113
INFO:local_logger:Epoch[052/800], Step[0300/0626], Avg Loss: 0.7116
INFO:local_logger:Epoch[052/800], Step[0400/0626], Avg Loss: 0.7117
INFO:local_logger:Epoch[052/800], Step[0400/0626], Avg Loss: 0.7112
INFO:local_logger:Epoch[052/800], Step[0400/0626], Avg Loss: 0.7113
INFO:local_logger:Epoch[052/800], Step[0400/0626], Avg Loss: 0.7110
INFO:master_logger:Epoch[052/800], Step[0400/0626], Avg Loss: 0.7112
INFO:local_logger:Epoch[052/800], Step[0400/0626], Avg Loss: 0.7111
INFO:local_logger:Epoch[052/800], Step[0400/0626], Avg Loss: 0.7107
INFO:local_logger:Epoch[052/800], Step[0400/0626], Avg Loss: 0.7117
INFO:local_logger:Epoch[052/800], Step[0400/0626], Avg Loss: 0.7111
INFO:local_logger:Epoch[052/800], Step[0500/0626], Avg Loss: 0.7107
INFO:local_logger:Epoch[052/800], Step[0500/0626], Avg Loss: 0.7112
INFO:local_logger:Epoch[052/800], Step[0500/0626], Avg Loss: 0.7115
INFO:local_logger:Epoch[052/800], Step[0500/0626], Avg Loss: 0.7114
INFO:local_logger:Epoch[052/800], Step[0500/0626], Avg Loss: 0.7107
INFO:master_logger:Epoch[052/800], Step[0500/0626], Avg Loss: 0.7111
INFO:local_logger:Epoch[052/800], Step[0500/0626], Avg Loss: 0.7109
INFO:local_logger:Epoch[052/800], Step[0500/0626], Avg Loss: 0.7116
INFO:local_logger:Epoch[052/800], Step[0500/0626], Avg Loss: 0.7112
INFO:local_logger:Epoch[052/800], Step[0600/0626], Avg Loss: 0.7106
INFO:local_logger:Epoch[052/800], Step[0600/0626], Avg Loss: 0.7113
INFO:local_logger:Epoch[052/800], Step[0600/0626], Avg Loss: 0.7114
INFO:local_logger:Epoch[052/800], Step[0600/0626], Avg Loss: 0.7109
INFO:local_logger:Epoch[052/800], Step[0600/0626], Avg Loss: 0.7113
INFO:local_logger:Epoch[052/800], Step[0600/0626], Avg Loss: 0.7110
INFO:local_logger:Epoch[052/800], Step[0600/0626], Avg Loss: 0.7113
INFO:master_logger:Epoch[052/800], Step[0600/0626], Avg Loss: 0.7111
INFO:local_logger:Epoch[052/800], Step[0600/0626], Avg Loss: 0.7107
INFO:local_logger:----- Epoch[052/800], Train Loss: 0.7107, time: 899.12
INFO:local_logger:Now training epoch 53. LR=0.000150
INFO:local_logger:----- Epoch[052/800], Train Loss: 0.7109, time: 899.76
INFO:local_logger:Now training epoch 53. LR=0.000150
INFO:local_logger:----- Epoch[052/800], Train Loss: 0.7113, time: 899.78
INFO:local_logger:Now training epoch 53. LR=0.000150
INFO:local_logger:----- Epoch[052/800], Train Loss: 0.7113, time: 900.39
INFO:local_logger:----- Epoch[052/800], Train Loss: 0.7114, time: 896.39
INFO:local_logger:Now training epoch 53. LR=0.000150
INFO:master_logger:----- Epoch[052/800], Train Loss: 0.7110, time: 896.39
INFO:local_logger:----- Epoch[052/800], Train Loss: 0.7109, time: 899.78
INFO:local_logger:Now training epoch 53. LR=0.000150
INFO:local_logger:----- Epoch[052/800], Train Loss: 0.7105, time: 899.79
INFO:local_logger:Now training epoch 53. LR=0.000150
INFO:local_logger:----- Epoch[052/800], Train Loss: 0.7112, time: 899.77
INFO:local_logger:Now training epoch 53. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-52-Loss-0.7113885321068728.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-52-Loss-0.7113885321068728.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-52-Loss-0.7113885321068728.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-52-Loss-0.7113885321068728.pdopt
INFO:local_logger:Now training epoch 53. LR=0.000150
INFO:master_logger:Now training epoch 53. LR=0.000150
INFO:local_logger:Epoch[053/800], Step[0000/0626], Avg Loss: 0.7064
INFO:master_logger:Epoch[053/800], Step[0000/0626], Avg Loss: 0.7112
INFO:local_logger:Epoch[053/800], Step[0000/0626], Avg Loss: 0.7098
INFO:local_logger:Epoch[053/800], Step[0000/0626], Avg Loss: 0.7075
INFO:local_logger:Epoch[053/800], Step[0000/0626], Avg Loss: 0.7109
INFO:local_logger:Epoch[053/800], Step[0000/0626], Avg Loss: 0.7036
INFO:local_logger:Epoch[053/800], Step[0000/0626], Avg Loss: 0.7135
INFO:local_logger:Epoch[053/800], Step[0000/0626], Avg Loss: 0.7272
INFO:local_logger:Epoch[053/800], Step[0000/0626], Avg Loss: 0.7107
INFO:local_logger:Epoch[053/800], Step[0100/0626], Avg Loss: 0.7100
INFO:master_logger:Epoch[053/800], Step[0100/0626], Avg Loss: 0.7105
INFO:local_logger:Epoch[053/800], Step[0100/0626], Avg Loss: 0.7112
INFO:local_logger:Epoch[053/800], Step[0100/0626], Avg Loss: 0.7106
INFO:local_logger:Epoch[053/800], Step[0100/0626], Avg Loss: 0.7117
INFO:local_logger:Epoch[053/800], Step[0100/0626], Avg Loss: 0.7093
INFO:local_logger:Epoch[053/800], Step[0100/0626], Avg Loss: 0.7106
INFO:local_logger:Epoch[053/800], Step[0100/0626], Avg Loss: 0.7110
INFO:local_logger:Epoch[053/800], Step[0100/0626], Avg Loss: 0.7096
INFO:local_logger:Epoch[053/800], Step[0200/0626], Avg Loss: 0.7102
INFO:local_logger:Epoch[053/800], Step[0200/0626], Avg Loss: 0.7106
INFO:local_logger:Epoch[053/800], Step[0200/0626], Avg Loss: 0.7117
INFO:local_logger:Epoch[053/800], Step[0200/0626], Avg Loss: 0.7098
INFO:local_logger:Epoch[053/800], Step[0200/0626], Avg Loss: 0.7102
INFO:local_logger:Epoch[053/800], Step[0200/0626], Avg Loss: 0.7105
INFO:master_logger:Epoch[053/800], Step[0200/0626], Avg Loss: 0.7103
INFO:local_logger:Epoch[053/800], Step[0200/0626], Avg Loss: 0.7102
INFO:local_logger:Epoch[053/800], Step[0200/0626], Avg Loss: 0.7094
INFO:local_logger:Epoch[053/800], Step[0300/0626], Avg Loss: 0.7105
INFO:local_logger:Epoch[053/800], Step[0300/0626], Avg Loss: 0.7101
INFO:local_logger:Epoch[053/800], Step[0300/0626], Avg Loss: 0.7099
INFO:local_logger:Epoch[053/800], Step[0300/0626], Avg Loss: 0.7094
INFO:local_logger:Epoch[053/800], Step[0300/0626], Avg Loss: 0.7103
INFO:local_logger:Epoch[053/800], Step[0300/0626], Avg Loss: 0.7107
INFO:master_logger:Epoch[053/800], Step[0300/0626], Avg Loss: 0.7101
INFO:local_logger:Epoch[053/800], Step[0300/0626], Avg Loss: 0.7099
INFO:local_logger:Epoch[053/800], Step[0300/0626], Avg Loss: 0.7101
INFO:local_logger:Epoch[053/800], Step[0400/0626], Avg Loss: 0.7103
INFO:local_logger:Epoch[053/800], Step[0400/0626], Avg Loss: 0.7096
INFO:local_logger:Epoch[053/800], Step[0400/0626], Avg Loss: 0.7104
INFO:local_logger:Epoch[053/800], Step[0400/0626], Avg Loss: 0.7103
INFO:local_logger:Epoch[053/800], Step[0400/0626], Avg Loss: 0.7102
INFO:master_logger:Epoch[053/800], Step[0400/0626], Avg Loss: 0.7102
INFO:local_logger:Epoch[053/800], Step[0400/0626], Avg Loss: 0.7104
INFO:local_logger:Epoch[053/800], Step[0400/0626], Avg Loss: 0.7101
INFO:local_logger:Epoch[053/800], Step[0400/0626], Avg Loss: 0.7103
INFO:local_logger:Epoch[053/800], Step[0500/0626], Avg Loss: 0.7105
INFO:local_logger:Epoch[053/800], Step[0500/0626], Avg Loss: 0.7104
INFO:local_logger:Epoch[053/800], Step[0500/0626], Avg Loss: 0.7103
INFO:local_logger:Epoch[053/800], Step[0500/0626], Avg Loss: 0.7101
INFO:local_logger:Epoch[053/800], Step[0500/0626], Avg Loss: 0.7096
INFO:local_logger:Epoch[053/800], Step[0500/0626], Avg Loss: 0.7102
INFO:local_logger:Epoch[053/800], Step[0500/0626], Avg Loss: 0.7102
INFO:master_logger:Epoch[053/800], Step[0500/0626], Avg Loss: 0.7102
INFO:local_logger:Epoch[053/800], Step[0500/0626], Avg Loss: 0.7103
INFO:local_logger:Epoch[053/800], Step[0600/0626], Avg Loss: 0.7094
INFO:local_logger:Epoch[053/800], Step[0600/0626], Avg Loss: 0.7102
INFO:local_logger:Epoch[053/800], Step[0600/0626], Avg Loss: 0.7101
INFO:local_logger:Epoch[053/800], Step[0600/0626], Avg Loss: 0.7100
INFO:local_logger:Epoch[053/800], Step[0600/0626], Avg Loss: 0.7102
INFO:local_logger:Epoch[053/800], Step[0600/0626], Avg Loss: 0.7101
INFO:master_logger:Epoch[053/800], Step[0600/0626], Avg Loss: 0.7100
INFO:local_logger:Epoch[053/800], Step[0600/0626], Avg Loss: 0.7101
INFO:local_logger:Epoch[053/800], Step[0600/0626], Avg Loss: 0.7098
INFO:local_logger:----- Epoch[053/800], Train Loss: 0.7102, time: 889.34
INFO:master_logger:----- Epoch[053/800], Train Loss: 0.7100, time: 889.34
INFO:local_logger:----- Epoch[053/800], Train Loss: 0.7102, time: 893.08
INFO:local_logger:Now training epoch 54. LR=0.000150
INFO:local_logger:----- Epoch[053/800], Train Loss: 0.7100, time: 893.08
INFO:local_logger:Now training epoch 54. LR=0.000150
INFO:local_logger:----- Epoch[053/800], Train Loss: 0.7094, time: 893.08
INFO:local_logger:Now training epoch 54. LR=0.000150
INFO:local_logger:----- Epoch[053/800], Train Loss: 0.7100, time: 893.10
INFO:local_logger:Now training epoch 54. LR=0.000150
INFO:local_logger:----- Epoch[053/800], Train Loss: 0.7100, time: 893.10
INFO:local_logger:Now training epoch 54. LR=0.000150
INFO:local_logger:----- Epoch[053/800], Train Loss: 0.7100, time: 893.75
INFO:local_logger:Now training epoch 54. LR=0.000150
INFO:local_logger:----- Epoch[053/800], Train Loss: 0.7098, time: 893.11
INFO:local_logger:Now training epoch 54. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-53-Loss-0.7102464560284915.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-53-Loss-0.7102464560284915.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-53-Loss-0.7102464560284915.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-53-Loss-0.7102464560284915.pdopt
INFO:local_logger:Now training epoch 54. LR=0.000150
INFO:master_logger:Now training epoch 54. LR=0.000150
INFO:local_logger:Epoch[054/800], Step[0000/0626], Avg Loss: 0.7001
INFO:local_logger:Epoch[054/800], Step[0000/0626], Avg Loss: 0.7124
INFO:local_logger:Epoch[054/800], Step[0000/0626], Avg Loss: 0.7103
INFO:local_logger:Epoch[054/800], Step[0000/0626], Avg Loss: 0.7021
INFO:master_logger:Epoch[054/800], Step[0000/0626], Avg Loss: 0.7094
INFO:local_logger:Epoch[054/800], Step[0000/0626], Avg Loss: 0.7254
INFO:local_logger:Epoch[054/800], Step[0000/0626], Avg Loss: 0.7077
INFO:local_logger:Epoch[054/800], Step[0000/0626], Avg Loss: 0.7109
INFO:local_logger:Epoch[054/800], Step[0000/0626], Avg Loss: 0.7063
INFO:local_logger:Epoch[054/800], Step[0100/0626], Avg Loss: 0.7094
INFO:local_logger:Epoch[054/800], Step[0100/0626], Avg Loss: 0.7105
INFO:local_logger:Epoch[054/800], Step[0100/0626], Avg Loss: 0.7093
INFO:master_logger:Epoch[054/800], Step[0100/0626], Avg Loss: 0.7094
INFO:local_logger:Epoch[054/800], Step[0100/0626], Avg Loss: 0.7095
INFO:local_logger:Epoch[054/800], Step[0100/0626], Avg Loss: 0.7089
INFO:local_logger:Epoch[054/800], Step[0100/0626], Avg Loss: 0.7095
INFO:local_logger:Epoch[054/800], Step[0100/0626], Avg Loss: 0.7097
INFO:local_logger:Epoch[054/800], Step[0100/0626], Avg Loss: 0.7087
INFO:local_logger:Epoch[054/800], Step[0200/0626], Avg Loss: 0.7082
INFO:local_logger:Epoch[054/800], Step[0200/0626], Avg Loss: 0.7086
INFO:local_logger:Epoch[054/800], Step[0200/0626], Avg Loss: 0.7092
INFO:local_logger:Epoch[054/800], Step[0200/0626], Avg Loss: 0.7092
INFO:local_logger:Epoch[054/800], Step[0200/0626], Avg Loss: 0.7087
INFO:local_logger:Epoch[054/800], Step[0200/0626], Avg Loss: 0.7088
INFO:local_logger:Epoch[054/800], Step[0200/0626], Avg Loss: 0.7085
INFO:master_logger:Epoch[054/800], Step[0200/0626], Avg Loss: 0.7088
INFO:local_logger:Epoch[054/800], Step[0200/0626], Avg Loss: 0.7093
INFO:local_logger:Epoch[054/800], Step[0300/0626], Avg Loss: 0.7083
INFO:local_logger:Epoch[054/800], Step[0300/0626], Avg Loss: 0.7089
INFO:local_logger:Epoch[054/800], Step[0300/0626], Avg Loss: 0.7089
INFO:local_logger:Epoch[054/800], Step[0300/0626], Avg Loss: 0.7089
INFO:local_logger:Epoch[054/800], Step[0300/0626], Avg Loss: 0.7088
INFO:local_logger:Epoch[054/800], Step[0300/0626], Avg Loss: 0.7085
INFO:master_logger:Epoch[054/800], Step[0300/0626], Avg Loss: 0.7088
INFO:local_logger:Epoch[054/800], Step[0300/0626], Avg Loss: 0.7088
INFO:local_logger:Epoch[054/800], Step[0300/0626], Avg Loss: 0.7092
INFO:local_logger:Epoch[054/800], Step[0400/0626], Avg Loss: 0.7089
INFO:local_logger:Epoch[054/800], Step[0400/0626], Avg Loss: 0.7088
INFO:local_logger:Epoch[054/800], Step[0400/0626], Avg Loss: 0.7090
INFO:local_logger:Epoch[054/800], Step[0400/0626], Avg Loss: 0.7088
INFO:local_logger:Epoch[054/800], Step[0400/0626], Avg Loss: 0.7083
INFO:master_logger:Epoch[054/800], Step[0400/0626], Avg Loss: 0.7088
INFO:local_logger:Epoch[054/800], Step[0400/0626], Avg Loss: 0.7090
INFO:local_logger:Epoch[054/800], Step[0400/0626], Avg Loss: 0.7091
INFO:local_logger:Epoch[054/800], Step[0400/0626], Avg Loss: 0.7083
INFO:local_logger:Epoch[054/800], Step[0500/0626], Avg Loss: 0.7087
INFO:local_logger:Epoch[054/800], Step[0500/0626], Avg Loss: 0.7083
INFO:local_logger:Epoch[054/800], Step[0500/0626], Avg Loss: 0.7084
INFO:local_logger:Epoch[054/800], Step[0500/0626], Avg Loss: 0.7085
INFO:local_logger:Epoch[054/800], Step[0500/0626], Avg Loss: 0.7087
INFO:local_logger:Epoch[054/800], Step[0500/0626], Avg Loss: 0.7090
INFO:local_logger:Epoch[054/800], Step[0500/0626], Avg Loss: 0.7090
INFO:master_logger:Epoch[054/800], Step[0500/0626], Avg Loss: 0.7086
INFO:local_logger:Epoch[054/800], Step[0500/0626], Avg Loss: 0.7083
INFO:local_logger:Epoch[054/800], Step[0600/0626], Avg Loss: 0.7083
INFO:local_logger:Epoch[054/800], Step[0600/0626], Avg Loss: 0.7088
INFO:local_logger:Epoch[054/800], Step[0600/0626], Avg Loss: 0.7088
INFO:local_logger:Epoch[054/800], Step[0600/0626], Avg Loss: 0.7085
INFO:local_logger:Epoch[054/800], Step[0600/0626], Avg Loss: 0.7087
INFO:master_logger:Epoch[054/800], Step[0600/0626], Avg Loss: 0.7085
INFO:local_logger:Epoch[054/800], Step[0600/0626], Avg Loss: 0.7088
INFO:local_logger:Epoch[054/800], Step[0600/0626], Avg Loss: 0.7081
INFO:local_logger:Epoch[054/800], Step[0600/0626], Avg Loss: 0.7081
INFO:local_logger:----- Epoch[054/800], Train Loss: 0.7087, time: 903.80
INFO:local_logger:Now training epoch 55. LR=0.000150
INFO:local_logger:----- Epoch[054/800], Train Loss: 0.7082, time: 903.85
INFO:local_logger:Now training epoch 55. LR=0.000150
INFO:local_logger:----- Epoch[054/800], Train Loss: 0.7083, time: 904.37
INFO:local_logger:Now training epoch 55. LR=0.000150
INFO:local_logger:----- Epoch[054/800], Train Loss: 0.7081, time: 904.37
INFO:local_logger:Now training epoch 55. LR=0.000150
INFO:local_logger:----- Epoch[054/800], Train Loss: 0.7085, time: 904.35
INFO:local_logger:Now training epoch 55. LR=0.000150
INFO:local_logger:----- Epoch[054/800], Train Loss: 0.7087, time: 904.43
INFO:local_logger:Now training epoch 55. LR=0.000150
INFO:local_logger:----- Epoch[054/800], Train Loss: 0.7088, time: 900.86
INFO:master_logger:----- Epoch[054/800], Train Loss: 0.7085, time: 900.86
INFO:local_logger:----- Epoch[054/800], Train Loss: 0.7087, time: 904.47
INFO:local_logger:Now training epoch 55. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-54-Loss-0.7087632936406654.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-54-Loss-0.7087632936406654.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-54-Loss-0.7087632936406654.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-54-Loss-0.7087632936406654.pdopt
INFO:local_logger:Now training epoch 55. LR=0.000150
INFO:master_logger:Now training epoch 55. LR=0.000150
INFO:local_logger:Epoch[055/800], Step[0000/0626], Avg Loss: 0.6897
INFO:master_logger:Epoch[055/800], Step[0000/0626], Avg Loss: 0.7057
INFO:local_logger:Epoch[055/800], Step[0000/0626], Avg Loss: 0.7023
INFO:local_logger:Epoch[055/800], Step[0000/0626], Avg Loss: 0.6937
INFO:local_logger:Epoch[055/800], Step[0000/0626], Avg Loss: 0.7040
INFO:local_logger:Epoch[055/800], Step[0000/0626], Avg Loss: 0.7203
INFO:local_logger:Epoch[055/800], Step[0000/0626], Avg Loss: 0.7165
INFO:local_logger:Epoch[055/800], Step[0000/0626], Avg Loss: 0.7079
INFO:local_logger:Epoch[055/800], Step[0000/0626], Avg Loss: 0.7108
INFO:local_logger:Epoch[055/800], Step[0100/0626], Avg Loss: 0.7093
INFO:local_logger:Epoch[055/800], Step[0100/0626], Avg Loss: 0.7084
INFO:local_logger:Epoch[055/800], Step[0100/0626], Avg Loss: 0.7081
INFO:local_logger:Epoch[055/800], Step[0100/0626], Avg Loss: 0.7084
INFO:master_logger:Epoch[055/800], Step[0100/0626], Avg Loss: 0.7083
INFO:local_logger:Epoch[055/800], Step[0100/0626], Avg Loss: 0.7082
INFO:local_logger:Epoch[055/800], Step[0100/0626], Avg Loss: 0.7082
INFO:local_logger:Epoch[055/800], Step[0100/0626], Avg Loss: 0.7078
INFO:local_logger:Epoch[055/800], Step[0100/0626], Avg Loss: 0.7078
INFO:local_logger:Epoch[055/800], Step[0200/0626], Avg Loss: 0.7074
INFO:local_logger:Epoch[055/800], Step[0200/0626], Avg Loss: 0.7086
INFO:local_logger:Epoch[055/800], Step[0200/0626], Avg Loss: 0.7080
INFO:local_logger:Epoch[055/800], Step[0200/0626], Avg Loss: 0.7086
INFO:master_logger:Epoch[055/800], Step[0200/0626], Avg Loss: 0.7081
INFO:local_logger:Epoch[055/800], Step[0200/0626], Avg Loss: 0.7088
INFO:local_logger:Epoch[055/800], Step[0200/0626], Avg Loss: 0.7078
INFO:local_logger:Epoch[055/800], Step[0200/0626], Avg Loss: 0.7078
INFO:local_logger:Epoch[055/800], Step[0200/0626], Avg Loss: 0.7077
INFO:local_logger:Epoch[055/800], Step[0300/0626], Avg Loss: 0.7071
INFO:local_logger:Epoch[055/800], Step[0300/0626], Avg Loss: 0.7086
INFO:local_logger:Epoch[055/800], Step[0300/0626], Avg Loss: 0.7076
INFO:master_logger:Epoch[055/800], Step[0300/0626], Avg Loss: 0.7080
INFO:local_logger:Epoch[055/800], Step[0300/0626], Avg Loss: 0.7086
INFO:local_logger:Epoch[055/800], Step[0300/0626], Avg Loss: 0.7078
INFO:local_logger:Epoch[055/800], Step[0300/0626], Avg Loss: 0.7079
INFO:local_logger:Epoch[055/800], Step[0300/0626], Avg Loss: 0.7079
INFO:local_logger:Epoch[055/800], Step[0300/0626], Avg Loss: 0.7083
INFO:local_logger:Epoch[055/800], Step[0400/0626], Avg Loss: 0.7073
INFO:local_logger:Epoch[055/800], Step[0400/0626], Avg Loss: 0.7081
INFO:local_logger:Epoch[055/800], Step[0400/0626], Avg Loss: 0.7079
INFO:local_logger:Epoch[055/800], Step[0400/0626], Avg Loss: 0.7076
INFO:local_logger:Epoch[055/800], Step[0400/0626], Avg Loss: 0.7074
INFO:local_logger:Epoch[055/800], Step[0400/0626], Avg Loss: 0.7078
INFO:local_logger:Epoch[055/800], Step[0400/0626], Avg Loss: 0.7087
INFO:master_logger:Epoch[055/800], Step[0400/0626], Avg Loss: 0.7079
INFO:local_logger:Epoch[055/800], Step[0400/0626], Avg Loss: 0.7081
INFO:local_logger:Epoch[055/800], Step[0500/0626], Avg Loss: 0.7087
INFO:local_logger:Epoch[055/800], Step[0500/0626], Avg Loss: 0.7074
INFO:local_logger:Epoch[055/800], Step[0500/0626], Avg Loss: 0.7077
INFO:local_logger:Epoch[055/800], Step[0500/0626], Avg Loss: 0.7082
INFO:local_logger:Epoch[055/800], Step[0500/0626], Avg Loss: 0.7076
INFO:local_logger:Epoch[055/800], Step[0500/0626], Avg Loss: 0.7073
INFO:master_logger:Epoch[055/800], Step[0500/0626], Avg Loss: 0.7078
INFO:local_logger:Epoch[055/800], Step[0500/0626], Avg Loss: 0.7079
INFO:local_logger:Epoch[055/800], Step[0500/0626], Avg Loss: 0.7074
INFO:local_logger:Epoch[055/800], Step[0600/0626], Avg Loss: 0.7073
INFO:local_logger:Epoch[055/800], Step[0600/0626], Avg Loss: 0.7074
INFO:local_logger:Epoch[055/800], Step[0600/0626], Avg Loss: 0.7081
INFO:local_logger:Epoch[055/800], Step[0600/0626], Avg Loss: 0.7074
INFO:local_logger:Epoch[055/800], Step[0600/0626], Avg Loss: 0.7072
INFO:local_logger:Epoch[055/800], Step[0600/0626], Avg Loss: 0.7084
INFO:master_logger:Epoch[055/800], Step[0600/0626], Avg Loss: 0.7076
INFO:local_logger:Epoch[055/800], Step[0600/0626], Avg Loss: 0.7077
INFO:local_logger:Epoch[055/800], Step[0600/0626], Avg Loss: 0.7074
INFO:local_logger:----- Epoch[055/800], Train Loss: 0.7081, time: 859.40
INFO:local_logger:Now training epoch 56. LR=0.000150
INFO:local_logger:----- Epoch[055/800], Train Loss: 0.7074, time: 855.79
INFO:master_logger:----- Epoch[055/800], Train Loss: 0.7076, time: 855.79
INFO:local_logger:----- Epoch[055/800], Train Loss: 0.7073, time: 859.94
INFO:local_logger:Now training epoch 56. LR=0.000150
INFO:local_logger:----- Epoch[055/800], Train Loss: 0.7073, time: 859.85
INFO:local_logger:Now training epoch 56. LR=0.000150
INFO:local_logger:----- Epoch[055/800], Train Loss: 0.7072, time: 860.45
INFO:local_logger:Now training epoch 56. LR=0.000150
INFO:local_logger:----- Epoch[055/800], Train Loss: 0.7074, time: 859.97
INFO:local_logger:Now training epoch 56. LR=0.000150
INFO:local_logger:----- Epoch[055/800], Train Loss: 0.7077, time: 859.89
INFO:local_logger:Now training epoch 56. LR=0.000150
INFO:local_logger:----- Epoch[055/800], Train Loss: 0.7083, time: 860.52
INFO:local_logger:Now training epoch 56. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-55-Loss-0.7074442110429905.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-55-Loss-0.7074442110429905.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-55-Loss-0.7074442110429905.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-55-Loss-0.7074442110429905.pdopt
INFO:local_logger:Now training epoch 56. LR=0.000150
INFO:master_logger:Now training epoch 56. LR=0.000150
INFO:local_logger:Epoch[056/800], Step[0000/0626], Avg Loss: 0.6969
INFO:local_logger:Epoch[056/800], Step[0000/0626], Avg Loss: 0.7058
INFO:local_logger:Epoch[056/800], Step[0000/0626], Avg Loss: 0.7068
INFO:master_logger:Epoch[056/800], Step[0000/0626], Avg Loss: 0.7070
INFO:local_logger:Epoch[056/800], Step[0000/0626], Avg Loss: 0.7114
INFO:local_logger:Epoch[056/800], Step[0000/0626], Avg Loss: 0.7124
INFO:local_logger:Epoch[056/800], Step[0000/0626], Avg Loss: 0.6985
INFO:local_logger:Epoch[056/800], Step[0000/0626], Avg Loss: 0.7150
INFO:local_logger:Epoch[056/800], Step[0000/0626], Avg Loss: 0.7089
INFO:local_logger:Epoch[056/800], Step[0100/0626], Avg Loss: 0.7066
INFO:local_logger:Epoch[056/800], Step[0100/0626], Avg Loss: 0.7074
INFO:local_logger:Epoch[056/800], Step[0100/0626], Avg Loss: 0.7062
INFO:local_logger:Epoch[056/800], Step[0100/0626], Avg Loss: 0.7079
INFO:local_logger:Epoch[056/800], Step[0100/0626], Avg Loss: 0.7056
INFO:master_logger:Epoch[056/800], Step[0100/0626], Avg Loss: 0.7068
INFO:local_logger:Epoch[056/800], Step[0100/0626], Avg Loss: 0.7069
INFO:local_logger:Epoch[056/800], Step[0100/0626], Avg Loss: 0.7066
INFO:local_logger:Epoch[056/800], Step[0100/0626], Avg Loss: 0.7070
INFO:local_logger:Epoch[056/800], Step[0200/0626], Avg Loss: 0.7071
INFO:local_logger:Epoch[056/800], Step[0200/0626], Avg Loss: 0.7066
INFO:local_logger:Epoch[056/800], Step[0200/0626], Avg Loss: 0.7069
INFO:local_logger:Epoch[056/800], Step[0200/0626], Avg Loss: 0.7075
INFO:local_logger:Epoch[056/800], Step[0200/0626], Avg Loss: 0.7069
INFO:local_logger:Epoch[056/800], Step[0200/0626], Avg Loss: 0.7069
INFO:local_logger:Epoch[056/800], Step[0200/0626], Avg Loss: 0.7069
INFO:local_logger:Epoch[056/800], Step[0200/0626], Avg Loss: 0.7059
INFO:master_logger:Epoch[056/800], Step[0200/0626], Avg Loss: 0.7068
INFO:local_logger:Epoch[056/800], Step[0300/0626], Avg Loss: 0.7077
INFO:local_logger:Epoch[056/800], Step[0300/0626], Avg Loss: 0.7064
INFO:local_logger:Epoch[056/800], Step[0300/0626], Avg Loss: 0.7068
INFO:local_logger:Epoch[056/800], Step[0300/0626], Avg Loss: 0.7067
INFO:local_logger:Epoch[056/800], Step[0300/0626], Avg Loss: 0.7067
INFO:local_logger:Epoch[056/800], Step[0300/0626], Avg Loss: 0.7074
INFO:local_logger:Epoch[056/800], Step[0300/0626], Avg Loss: 0.7059
INFO:local_logger:Epoch[056/800], Step[0300/0626], Avg Loss: 0.7065
INFO:master_logger:Epoch[056/800], Step[0300/0626], Avg Loss: 0.7068
INFO:local_logger:Epoch[056/800], Step[0400/0626], Avg Loss: 0.7060
INFO:local_logger:Epoch[056/800], Step[0400/0626], Avg Loss: 0.7065
INFO:local_logger:Epoch[056/800], Step[0400/0626], Avg Loss: 0.7066
INFO:local_logger:Epoch[056/800], Step[0400/0626], Avg Loss: 0.7069
INFO:local_logger:Epoch[056/800], Step[0400/0626], Avg Loss: 0.7073
INFO:local_logger:Epoch[056/800], Step[0400/0626], Avg Loss: 0.7072
INFO:master_logger:Epoch[056/800], Step[0400/0626], Avg Loss: 0.7067
INFO:local_logger:Epoch[056/800], Step[0400/0626], Avg Loss: 0.7063
INFO:local_logger:Epoch[056/800], Step[0400/0626], Avg Loss: 0.7067
INFO:local_logger:Epoch[056/800], Step[0500/0626], Avg Loss: 0.7062
INFO:local_logger:Epoch[056/800], Step[0500/0626], Avg Loss: 0.7063
INFO:local_logger:Epoch[056/800], Step[0500/0626], Avg Loss: 0.7069
INFO:local_logger:Epoch[056/800], Step[0500/0626], Avg Loss: 0.7069
INFO:master_logger:Epoch[056/800], Step[0500/0626], Avg Loss: 0.7066
INFO:local_logger:Epoch[056/800], Step[0500/0626], Avg Loss: 0.7067
INFO:local_logger:Epoch[056/800], Step[0500/0626], Avg Loss: 0.7062
INFO:local_logger:Epoch[056/800], Step[0500/0626], Avg Loss: 0.7068
INFO:local_logger:Epoch[056/800], Step[0500/0626], Avg Loss: 0.7066
INFO:local_logger:Epoch[056/800], Step[0600/0626], Avg Loss: 0.7062
INFO:local_logger:Epoch[056/800], Step[0600/0626], Avg Loss: 0.7066
INFO:local_logger:Epoch[056/800], Step[0600/0626], Avg Loss: 0.7068
INFO:local_logger:Epoch[056/800], Step[0600/0626], Avg Loss: 0.7069
INFO:local_logger:Epoch[056/800], Step[0600/0626], Avg Loss: 0.7063
INFO:local_logger:Epoch[056/800], Step[0600/0626], Avg Loss: 0.7068
INFO:local_logger:Epoch[056/800], Step[0600/0626], Avg Loss: 0.7061
INFO:master_logger:Epoch[056/800], Step[0600/0626], Avg Loss: 0.7065
INFO:local_logger:Epoch[056/800], Step[0600/0626], Avg Loss: 0.7065
INFO:local_logger:----- Epoch[056/800], Train Loss: 0.7067, time: 890.40
INFO:local_logger:Now training epoch 57. LR=0.000150
INFO:local_logger:----- Epoch[056/800], Train Loss: 0.7065, time: 890.61
INFO:local_logger:Now training epoch 57. LR=0.000150
INFO:local_logger:----- Epoch[056/800], Train Loss: 0.7066, time: 890.94
INFO:local_logger:Now training epoch 57. LR=0.000150
INFO:local_logger:----- Epoch[056/800], Train Loss: 0.7068, time: 891.55
INFO:local_logger:----- Epoch[056/800], Train Loss: 0.7063, time: 890.99
INFO:local_logger:Now training epoch 57. LR=0.000150
INFO:local_logger:----- Epoch[056/800], Train Loss: 0.7060, time: 891.01
INFO:local_logger:Now training epoch 57. LR=0.000150
INFO:local_logger:Now training epoch 57. LR=0.000150
INFO:local_logger:----- Epoch[056/800], Train Loss: 0.7063, time: 887.67
INFO:master_logger:----- Epoch[056/800], Train Loss: 0.7065, time: 887.67
INFO:local_logger:----- Epoch[056/800], Train Loss: 0.7069, time: 890.98
INFO:local_logger:Now training epoch 57. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-56-Loss-0.7062517588045653.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-56-Loss-0.7062517588045653.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-56-Loss-0.7062517588045653.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-56-Loss-0.7062517588045653.pdopt
INFO:local_logger:Now training epoch 57. LR=0.000150
INFO:master_logger:Now training epoch 57. LR=0.000150
INFO:local_logger:Epoch[057/800], Step[0000/0626], Avg Loss: 0.7140
INFO:local_logger:Epoch[057/800], Step[0000/0626], Avg Loss: 0.6994
INFO:local_logger:Epoch[057/800], Step[0000/0626], Avg Loss: 0.7131
INFO:master_logger:Epoch[057/800], Step[0000/0626], Avg Loss: 0.7096
INFO:local_logger:Epoch[057/800], Step[0000/0626], Avg Loss: 0.7112
INFO:local_logger:Epoch[057/800], Step[0000/0626], Avg Loss: 0.7165
INFO:local_logger:Epoch[057/800], Step[0000/0626], Avg Loss: 0.7022
INFO:local_logger:Epoch[057/800], Step[0000/0626], Avg Loss: 0.7034
INFO:local_logger:Epoch[057/800], Step[0000/0626], Avg Loss: 0.7171
INFO:local_logger:Epoch[057/800], Step[0100/0626], Avg Loss: 0.7063
INFO:local_logger:Epoch[057/800], Step[0100/0626], Avg Loss: 0.7057
INFO:local_logger:Epoch[057/800], Step[0100/0626], Avg Loss: 0.7054
INFO:local_logger:Epoch[057/800], Step[0100/0626], Avg Loss: 0.7062
INFO:local_logger:Epoch[057/800], Step[0100/0626], Avg Loss: 0.7068
INFO:local_logger:Epoch[057/800], Step[0100/0626], Avg Loss: 0.7067
INFO:local_logger:Epoch[057/800], Step[0100/0626], Avg Loss: 0.7053
INFO:local_logger:Epoch[057/800], Step[0100/0626], Avg Loss: 0.7065
INFO:master_logger:Epoch[057/800], Step[0100/0626], Avg Loss: 0.7061
INFO:local_logger:Epoch[057/800], Step[0200/0626], Avg Loss: 0.7062
INFO:local_logger:Epoch[057/800], Step[0200/0626], Avg Loss: 0.7056
INFO:local_logger:Epoch[057/800], Step[0200/0626], Avg Loss: 0.7062
INFO:local_logger:Epoch[057/800], Step[0200/0626], Avg Loss: 0.7067
INFO:local_logger:Epoch[057/800], Step[0200/0626], Avg Loss: 0.7058
INFO:local_logger:Epoch[057/800], Step[0200/0626], Avg Loss: 0.7057
INFO:local_logger:Epoch[057/800], Step[0200/0626], Avg Loss: 0.7051
INFO:master_logger:Epoch[057/800], Step[0200/0626], Avg Loss: 0.7059
INFO:local_logger:Epoch[057/800], Step[0200/0626], Avg Loss: 0.7061
INFO:local_logger:Epoch[057/800], Step[0300/0626], Avg Loss: 0.7056
INFO:local_logger:Epoch[057/800], Step[0300/0626], Avg Loss: 0.7059
INFO:local_logger:Epoch[057/800], Step[0300/0626], Avg Loss: 0.7052
INFO:local_logger:Epoch[057/800], Step[0300/0626], Avg Loss: 0.7058
INFO:local_logger:Epoch[057/800], Step[0300/0626], Avg Loss: 0.7054
INFO:master_logger:Epoch[057/800], Step[0300/0626], Avg Loss: 0.7057
INFO:local_logger:Epoch[057/800], Step[0300/0626], Avg Loss: 0.7054
INFO:local_logger:Epoch[057/800], Step[0300/0626], Avg Loss: 0.7063
INFO:local_logger:Epoch[057/800], Step[0300/0626], Avg Loss: 0.7061
INFO:local_logger:Epoch[057/800], Step[0400/0626], Avg Loss: 0.7058
INFO:local_logger:Epoch[057/800], Step[0400/0626], Avg Loss: 0.7060
INFO:local_logger:Epoch[057/800], Step[0400/0626], Avg Loss: 0.7052
INFO:local_logger:Epoch[057/800], Step[0400/0626], Avg Loss: 0.7059
INFO:local_logger:Epoch[057/800], Step[0400/0626], Avg Loss: 0.7056
INFO:local_logger:Epoch[057/800], Step[0400/0626], Avg Loss: 0.7063
INFO:local_logger:Epoch[057/800], Step[0400/0626], Avg Loss: 0.7062
INFO:local_logger:Epoch[057/800], Step[0400/0626], Avg Loss: 0.7055
INFO:master_logger:Epoch[057/800], Step[0400/0626], Avg Loss: 0.7058
INFO:local_logger:Epoch[057/800], Step[0500/0626], Avg Loss: 0.7054
INFO:local_logger:Epoch[057/800], Step[0500/0626], Avg Loss: 0.7062
INFO:local_logger:Epoch[057/800], Step[0500/0626], Avg Loss: 0.7050
INFO:local_logger:Epoch[057/800], Step[0500/0626], Avg Loss: 0.7059
INFO:local_logger:Epoch[057/800], Step[0500/0626], Avg Loss: 0.7055
INFO:local_logger:Epoch[057/800], Step[0500/0626], Avg Loss: 0.7061
INFO:local_logger:Epoch[057/800], Step[0500/0626], Avg Loss: 0.7057
INFO:local_logger:Epoch[057/800], Step[0500/0626], Avg Loss: 0.7059
INFO:master_logger:Epoch[057/800], Step[0500/0626], Avg Loss: 0.7057
INFO:local_logger:Epoch[057/800], Step[0600/0626], Avg Loss: 0.7051
INFO:local_logger:Epoch[057/800], Step[0600/0626], Avg Loss: 0.7058
INFO:local_logger:Epoch[057/800], Step[0600/0626], Avg Loss: 0.7054
INFO:local_logger:Epoch[057/800], Step[0600/0626], Avg Loss: 0.7054
INFO:local_logger:Epoch[057/800], Step[0600/0626], Avg Loss: 0.7057
INFO:local_logger:Epoch[057/800], Step[0600/0626], Avg Loss: 0.7062
INFO:local_logger:Epoch[057/800], Step[0600/0626], Avg Loss: 0.7062
INFO:local_logger:Epoch[057/800], Step[0600/0626], Avg Loss: 0.7056
INFO:master_logger:Epoch[057/800], Step[0600/0626], Avg Loss: 0.7057
INFO:local_logger:----- Epoch[057/800], Train Loss: 0.7060, time: 853.51
INFO:local_logger:Now training epoch 58. LR=0.000150
INFO:local_logger:----- Epoch[057/800], Train Loss: 0.7053, time: 853.54
INFO:local_logger:Now training epoch 58. LR=0.000150
INFO:local_logger:----- Epoch[057/800], Train Loss: 0.7057, time: 854.16
INFO:local_logger:Now training epoch 58. LR=0.000150
INFO:local_logger:----- Epoch[057/800], Train Loss: 0.7050, time: 854.00
INFO:local_logger:----- Epoch[057/800], Train Loss: 0.7061, time: 854.36
INFO:local_logger:Now training epoch 58. LR=0.000150
INFO:local_logger:Now training epoch 58. LR=0.000150
INFO:local_logger:----- Epoch[057/800], Train Loss: 0.7056, time: 849.91
INFO:local_logger:----- Epoch[057/800], Train Loss: 0.7054, time: 853.96
INFO:master_logger:----- Epoch[057/800], Train Loss: 0.7056, time: 849.91
INFO:local_logger:Now training epoch 58. LR=0.000150
INFO:local_logger:----- Epoch[057/800], Train Loss: 0.7056, time: 853.96
INFO:local_logger:Now training epoch 58. LR=0.000150
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-57-Loss-0.70561545900947.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-57-Loss-0.70561545900947.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-57-Loss-0.70561545900947.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-57-Loss-0.70561545900947.pdopt
INFO:local_logger:Now training epoch 58. LR=0.000150
INFO:master_logger:Now training epoch 58. LR=0.000150
INFO:local_logger:Epoch[058/800], Step[0000/0626], Avg Loss: 0.7087
INFO:local_logger:Epoch[058/800], Step[0000/0626], Avg Loss: 0.6950
INFO:master_logger:Epoch[058/800], Step[0000/0626], Avg Loss: 0.7032
INFO:local_logger:Epoch[058/800], Step[0000/0626], Avg Loss: 0.7035
INFO:local_logger:Epoch[058/800], Step[0000/0626], Avg Loss: 0.7008
INFO:local_logger:Epoch[058/800], Step[0000/0626], Avg Loss: 0.6990
INFO:local_logger:Epoch[058/800], Step[0000/0626], Avg Loss: 0.7091
INFO:local_logger:Epoch[058/800], Step[0000/0626], Avg Loss: 0.7010
INFO:local_logger:Epoch[058/800], Step[0000/0626], Avg Loss: 0.7083
INFO:local_logger:Epoch[058/800], Step[0100/0626], Avg Loss: 0.7057
INFO:local_logger:Epoch[058/800], Step[0100/0626], Avg Loss: 0.7047
INFO:local_logger:Epoch[058/800], Step[0100/0626], Avg Loss: 0.7033
INFO:local_logger:Epoch[058/800], Step[0100/0626], Avg Loss: 0.7035
INFO:local_logger:Epoch[058/800], Step[0100/0626], Avg Loss: 0.7044
INFO:local_logger:Epoch[058/800], Step[0100/0626], Avg Loss: 0.7042
INFO:local_logger:Epoch[058/800], Step[0100/0626], Avg Loss: 0.7048
INFO:local_logger:Epoch[058/800], Step[0100/0626], Avg Loss: 0.7051
INFO:master_logger:Epoch[058/800], Step[0100/0626], Avg Loss: 0.7045
INFO:local_logger:Epoch[058/800], Step[0200/0626], Avg Loss: 0.7056
INFO:local_logger:Epoch[058/800], Step[0200/0626], Avg Loss: 0.7048
INFO:local_logger:Epoch[058/800], Step[0200/0626], Avg Loss: 0.7050
INFO:local_logger:Epoch[058/800], Step[0200/0626], Avg Loss: 0.7052
INFO:local_logger:Epoch[058/800], Step[0200/0626], Avg Loss: 0.7048
INFO:master_logger:Epoch[058/800], Step[0200/0626], Avg Loss: 0.7048
INFO:local_logger:Epoch[058/800], Step[0200/0626], Avg Loss: 0.7046
INFO:local_logger:Epoch[058/800], Step[0200/0626], Avg Loss: 0.7051
INFO:local_logger:Epoch[058/800], Step[0200/0626], Avg Loss: 0.7032
INFO:local_logger:Epoch[058/800], Step[0300/0626], Avg Loss: 0.7048
INFO:local_logger:Epoch[058/800], Step[0300/0626], Avg Loss: 0.7052
INFO:local_logger:Epoch[058/800], Step[0300/0626], Avg Loss: 0.7046
INFO:local_logger:Epoch[058/800], Step[0300/0626], Avg Loss: 0.7046
INFO:local_logger:Epoch[058/800], Step[0300/0626], Avg Loss: 0.7051
INFO:local_logger:Epoch[058/800], Step[0300/0626], Avg Loss: 0.7046
INFO:master_logger:Epoch[058/800], Step[0300/0626], Avg Loss: 0.7047
INFO:local_logger:Epoch[058/800], Step[0300/0626], Avg Loss: 0.7049
INFO:local_logger:Epoch[058/800], Step[0300/0626], Avg Loss: 0.7035
INFO:local_logger:Epoch[058/800], Step[0400/0626], Avg Loss: 0.7047
INFO:local_logger:Epoch[058/800], Step[0400/0626], Avg Loss: 0.7047
INFO:local_logger:Epoch[058/800], Step[0400/0626], Avg Loss: 0.7048
INFO:local_logger:Epoch[058/800], Step[0400/0626], Avg Loss: 0.7035
INFO:local_logger:Epoch[058/800], Step[0400/0626], Avg Loss: 0.7048
INFO:master_logger:Epoch[058/800], Step[0400/0626], Avg Loss: 0.7046
INFO:local_logger:Epoch[058/800], Step[0400/0626], Avg Loss: 0.7047
INFO:local_logger:Epoch[058/800], Step[0400/0626], Avg Loss: 0.7046
INFO:local_logger:Epoch[058/800], Step[0400/0626], Avg Loss: 0.7047
INFO:local_logger:Epoch[058/800], Step[0500/0626], Avg Loss: 0.7048
INFO:local_logger:Epoch[058/800], Step[0500/0626], Avg Loss: 0.7046
INFO:local_logger:Epoch[058/800], Step[0500/0626], Avg Loss: 0.7046
INFO:local_logger:Epoch[058/800], Step[0500/0626], Avg Loss: 0.7046
INFO:local_logger:Epoch[058/800], Step[0500/0626], Avg Loss: 0.7046
INFO:local_logger:Epoch[058/800], Step[0500/0626], Avg Loss: 0.7043
INFO:local_logger:Epoch[058/800], Step[0500/0626], Avg Loss: 0.7034
INFO:master_logger:Epoch[058/800], Step[0500/0626], Avg Loss: 0.7044
INFO:local_logger:Epoch[058/800], Step[0500/0626], Avg Loss: 0.7046
INFO:local_logger:Epoch[058/800], Step[0600/0626], Avg Loss: 0.7046
INFO:local_logger:Epoch[058/800], Step[0600/0626], Avg Loss: 0.7046
INFO:local_logger:Epoch[058/800], Step[0600/0626], Avg Loss: 0.7045
INFO:local_logger:Epoch[058/800], Step[0600/0626], Avg Loss: 0.7036
INFO:local_logger:Epoch[058/800], Step[0600/0626], Avg Loss: 0.7045
INFO:local_logger:Epoch[058/800], Step[0600/0626], Avg Loss: 0.7046
INFO:master_logger:Epoch[058/800], Step[0600/0626], Avg Loss: 0.7044
INFO:local_logger:Epoch[058/800], Step[0600/0626], Avg Loss: 0.7045
INFO:local_logger:Epoch[058/800], Step[0600/0626], Avg Loss: 0.7042
INFO:local_logger:----- Epoch[058/800], Train Loss: 0.7045, time: 883.86
INFO:master_logger:----- Epoch[058/800], Train Loss: 0.7044, time: 883.86
INFO:local_logger:----- Epoch[058/800], Train Loss: 0.7044, time: 888.09
INFO:local_logger:Now training epoch 59. LR=0.000151
INFO:local_logger:----- Epoch[058/800], Train Loss: 0.7043, time: 888.09
INFO:local_logger:Now training epoch 59. LR=0.000151
INFO:local_logger:----- Epoch[058/800], Train Loss: 0.7045, time: 887.67
INFO:local_logger:Now training epoch 59. LR=0.000151
INFO:local_logger:----- Epoch[058/800], Train Loss: 0.7036, time: 887.78
INFO:local_logger:Now training epoch 59. LR=0.000151
INFO:local_logger:----- Epoch[058/800], Train Loss: 0.7046, time: 888.37
INFO:local_logger:Now training epoch 59. LR=0.000151
INFO:local_logger:----- Epoch[058/800], Train Loss: 0.7045, time: 887.96
INFO:local_logger:Now training epoch 59. LR=0.000151
INFO:local_logger:----- Epoch[058/800], Train Loss: 0.7046, time: 887.96
INFO:local_logger:Now training epoch 59. LR=0.000151
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-58-Loss-0.704508643255555.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-58-Loss-0.704508643255555.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-58-Loss-0.704508643255555.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-58-Loss-0.704508643255555.pdopt
INFO:local_logger:Now training epoch 59. LR=0.000151
INFO:master_logger:Now training epoch 59. LR=0.000151
INFO:local_logger:Epoch[059/800], Step[0000/0626], Avg Loss: 0.6945
INFO:master_logger:Epoch[059/800], Step[0000/0626], Avg Loss: 0.7041
INFO:local_logger:Epoch[059/800], Step[0000/0626], Avg Loss: 0.7028
INFO:local_logger:Epoch[059/800], Step[0000/0626], Avg Loss: 0.7075
INFO:local_logger:Epoch[059/800], Step[0000/0626], Avg Loss: 0.7132
INFO:local_logger:Epoch[059/800], Step[0000/0626], Avg Loss: 0.7134
INFO:local_logger:Epoch[059/800], Step[0000/0626], Avg Loss: 0.7138
INFO:local_logger:Epoch[059/800], Step[0000/0626], Avg Loss: 0.6925
INFO:local_logger:Epoch[059/800], Step[0000/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[059/800], Step[0100/0626], Avg Loss: 0.7022
INFO:local_logger:Epoch[059/800], Step[0100/0626], Avg Loss: 0.7047
INFO:local_logger:Epoch[059/800], Step[0100/0626], Avg Loss: 0.7039
INFO:local_logger:Epoch[059/800], Step[0100/0626], Avg Loss: 0.7043
INFO:master_logger:Epoch[059/800], Step[0100/0626], Avg Loss: 0.7041
INFO:local_logger:Epoch[059/800], Step[0100/0626], Avg Loss: 0.7048
INFO:local_logger:Epoch[059/800], Step[0100/0626], Avg Loss: 0.7046
INFO:local_logger:Epoch[059/800], Step[0100/0626], Avg Loss: 0.7044
INFO:local_logger:Epoch[059/800], Step[0100/0626], Avg Loss: 0.7034
INFO:local_logger:Epoch[059/800], Step[0200/0626], Avg Loss: 0.7040
INFO:local_logger:Epoch[059/800], Step[0200/0626], Avg Loss: 0.7040
INFO:local_logger:Epoch[059/800], Step[0200/0626], Avg Loss: 0.7041
INFO:local_logger:Epoch[059/800], Step[0200/0626], Avg Loss: 0.7032
INFO:local_logger:Epoch[059/800], Step[0200/0626], Avg Loss: 0.7036
INFO:local_logger:Epoch[059/800], Step[0200/0626], Avg Loss: 0.7039
INFO:master_logger:Epoch[059/800], Step[0200/0626], Avg Loss: 0.7037
INFO:local_logger:Epoch[059/800], Step[0200/0626], Avg Loss: 0.7021
INFO:local_logger:Epoch[059/800], Step[0200/0626], Avg Loss: 0.7043
INFO:local_logger:Epoch[059/800], Step[0300/0626], Avg Loss: 0.7041
INFO:local_logger:Epoch[059/800], Step[0300/0626], Avg Loss: 0.7036
INFO:local_logger:Epoch[059/800], Step[0300/0626], Avg Loss: 0.7036
INFO:local_logger:Epoch[059/800], Step[0300/0626], Avg Loss: 0.7040
INFO:master_logger:Epoch[059/800], Step[0300/0626], Avg Loss: 0.7036
INFO:local_logger:Epoch[059/800], Step[0300/0626], Avg Loss: 0.7033
INFO:local_logger:Epoch[059/800], Step[0300/0626], Avg Loss: 0.7027
INFO:local_logger:Epoch[059/800], Step[0300/0626], Avg Loss: 0.7040
INFO:local_logger:Epoch[059/800], Step[0300/0626], Avg Loss: 0.7034
INFO:local_logger:Epoch[059/800], Step[0400/0626], Avg Loss: 0.7037
INFO:local_logger:Epoch[059/800], Step[0400/0626], Avg Loss: 0.7040
INFO:local_logger:Epoch[059/800], Step[0400/0626], Avg Loss: 0.7029
INFO:local_logger:Epoch[059/800], Step[0400/0626], Avg Loss: 0.7037
INFO:local_logger:Epoch[059/800], Step[0400/0626], Avg Loss: 0.7035
INFO:local_logger:Epoch[059/800], Step[0400/0626], Avg Loss: 0.7033
INFO:local_logger:Epoch[059/800], Step[0400/0626], Avg Loss: 0.7036
INFO:local_logger:Epoch[059/800], Step[0400/0626], Avg Loss: 0.7039
INFO:master_logger:Epoch[059/800], Step[0400/0626], Avg Loss: 0.7036
INFO:local_logger:Epoch[059/800], Step[0500/0626], Avg Loss: 0.7036
INFO:local_logger:Epoch[059/800], Step[0500/0626], Avg Loss: 0.7037
INFO:local_logger:Epoch[059/800], Step[0500/0626], Avg Loss: 0.7034
INFO:local_logger:Epoch[059/800], Step[0500/0626], Avg Loss: 0.7039
INFO:local_logger:Epoch[059/800], Step[0500/0626], Avg Loss: 0.7031
INFO:local_logger:Epoch[059/800], Step[0500/0626], Avg Loss: 0.7037
INFO:master_logger:Epoch[059/800], Step[0500/0626], Avg Loss: 0.7035
INFO:local_logger:Epoch[059/800], Step[0500/0626], Avg Loss: 0.7031
INFO:local_logger:Epoch[059/800], Step[0500/0626], Avg Loss: 0.7036
INFO:local_logger:Epoch[059/800], Step[0600/0626], Avg Loss: 0.7035
INFO:local_logger:Epoch[059/800], Step[0600/0626], Avg Loss: 0.7037
INFO:local_logger:Epoch[059/800], Step[0600/0626], Avg Loss: 0.7035
INFO:local_logger:Epoch[059/800], Step[0600/0626], Avg Loss: 0.7034
INFO:master_logger:Epoch[059/800], Step[0600/0626], Avg Loss: 0.7034
INFO:local_logger:Epoch[059/800], Step[0600/0626], Avg Loss: 0.7037
INFO:local_logger:Epoch[059/800], Step[0600/0626], Avg Loss: 0.7034
INFO:local_logger:Epoch[059/800], Step[0600/0626], Avg Loss: 0.7032
INFO:local_logger:Epoch[059/800], Step[0600/0626], Avg Loss: 0.7030
INFO:local_logger:----- Epoch[059/800], Train Loss: 0.7034, time: 855.63
INFO:master_logger:----- Epoch[059/800], Train Loss: 0.7034, time: 855.63
INFO:local_logger:----- Epoch[059/800], Train Loss: 0.7034, time: 859.41
INFO:local_logger:Now training epoch 60. LR=0.000151
INFO:local_logger:----- Epoch[059/800], Train Loss: 0.7035, time: 859.16
INFO:local_logger:Now training epoch 60. LR=0.000151
INFO:local_logger:----- Epoch[059/800], Train Loss: 0.7030, time: 859.83
INFO:local_logger:Now training epoch 60. LR=0.000151
INFO:local_logger:----- Epoch[059/800], Train Loss: 0.7033, time: 859.66
INFO:local_logger:Now training epoch 60. LR=0.000151
INFO:local_logger:----- Epoch[059/800], Train Loss: 0.7036, time: 859.94
INFO:local_logger:Now training epoch 60. LR=0.000151
INFO:local_logger:----- Epoch[059/800], Train Loss: 0.7037, time: 859.65
INFO:local_logger:Now training epoch 60. LR=0.000151
INFO:local_logger:----- Epoch[059/800], Train Loss: 0.7032, time: 859.97
INFO:local_logger:Now training epoch 60. LR=0.000151
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-59-Loss-0.7033895635093321.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-59-Loss-0.7033895635093321.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-59-Loss-0.7033895635093321.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-59-Loss-0.7033895635093321.pdopt
INFO:local_logger:Now training epoch 60. LR=0.000151
INFO:master_logger:Now training epoch 60. LR=0.000151
INFO:local_logger:Epoch[060/800], Step[0000/0626], Avg Loss: 0.7205
INFO:local_logger:Epoch[060/800], Step[0000/0626], Avg Loss: 0.7122
INFO:local_logger:Epoch[060/800], Step[0000/0626], Avg Loss: 0.7150
INFO:master_logger:Epoch[060/800], Step[0000/0626], Avg Loss: 0.7093
INFO:local_logger:Epoch[060/800], Step[0000/0626], Avg Loss: 0.7005
INFO:local_logger:Epoch[060/800], Step[0000/0626], Avg Loss: 0.6991
INFO:local_logger:Epoch[060/800], Step[0000/0626], Avg Loss: 0.7068
INFO:local_logger:Epoch[060/800], Step[0000/0626], Avg Loss: 0.7212
INFO:local_logger:Epoch[060/800], Step[0000/0626], Avg Loss: 0.6992
INFO:local_logger:Epoch[060/800], Step[0100/0626], Avg Loss: 0.7019
INFO:local_logger:Epoch[060/800], Step[0100/0626], Avg Loss: 0.7031
INFO:local_logger:Epoch[060/800], Step[0100/0626], Avg Loss: 0.7024
INFO:local_logger:Epoch[060/800], Step[0100/0626], Avg Loss: 0.7027
INFO:local_logger:Epoch[060/800], Step[0100/0626], Avg Loss: 0.7024
INFO:local_logger:Epoch[060/800], Step[0100/0626], Avg Loss: 0.7041
INFO:master_logger:Epoch[060/800], Step[0100/0626], Avg Loss: 0.7029
INFO:local_logger:Epoch[060/800], Step[0100/0626], Avg Loss: 0.7026
INFO:local_logger:Epoch[060/800], Step[0100/0626], Avg Loss: 0.7037
INFO:local_logger:Epoch[060/800], Step[0200/0626], Avg Loss: 0.7033
INFO:local_logger:Epoch[060/800], Step[0200/0626], Avg Loss: 0.7022
INFO:local_logger:Epoch[060/800], Step[0200/0626], Avg Loss: 0.7043
INFO:local_logger:Epoch[060/800], Step[0200/0626], Avg Loss: 0.7025
INFO:local_logger:Epoch[060/800], Step[0200/0626], Avg Loss: 0.7025
INFO:local_logger:Epoch[060/800], Step[0200/0626], Avg Loss: 0.7030
INFO:master_logger:Epoch[060/800], Step[0200/0626], Avg Loss: 0.7029
INFO:local_logger:Epoch[060/800], Step[0200/0626], Avg Loss: 0.7025
INFO:local_logger:Epoch[060/800], Step[0200/0626], Avg Loss: 0.7033
INFO:local_logger:Epoch[060/800], Step[0300/0626], Avg Loss: 0.7030
INFO:local_logger:Epoch[060/800], Step[0300/0626], Avg Loss: 0.7022
INFO:local_logger:Epoch[060/800], Step[0300/0626], Avg Loss: 0.7026
INFO:local_logger:Epoch[060/800], Step[0300/0626], Avg Loss: 0.7029
INFO:local_logger:Epoch[060/800], Step[0300/0626], Avg Loss: 0.7034
INFO:master_logger:Epoch[060/800], Step[0300/0626], Avg Loss: 0.7028
INFO:local_logger:Epoch[060/800], Step[0300/0626], Avg Loss: 0.7027
INFO:local_logger:Epoch[060/800], Step[0300/0626], Avg Loss: 0.7025
INFO:local_logger:Epoch[060/800], Step[0300/0626], Avg Loss: 0.7030
INFO:local_logger:Epoch[060/800], Step[0400/0626], Avg Loss: 0.7024
INFO:local_logger:Epoch[060/800], Step[0400/0626], Avg Loss: 0.7024
INFO:local_logger:Epoch[060/800], Step[0400/0626], Avg Loss: 0.7025
INFO:local_logger:Epoch[060/800], Step[0400/0626], Avg Loss: 0.7026
INFO:local_logger:Epoch[060/800], Step[0400/0626], Avg Loss: 0.7031
INFO:local_logger:Epoch[060/800], Step[0400/0626], Avg Loss: 0.7029
INFO:master_logger:Epoch[060/800], Step[0400/0626], Avg Loss: 0.7026
INFO:local_logger:Epoch[060/800], Step[0400/0626], Avg Loss: 0.7028
INFO:local_logger:Epoch[060/800], Step[0400/0626], Avg Loss: 0.7023
INFO:local_logger:Epoch[060/800], Step[0500/0626], Avg Loss: 0.7026
INFO:local_logger:Epoch[060/800], Step[0500/0626], Avg Loss: 0.7023
INFO:local_logger:Epoch[060/800], Step[0500/0626], Avg Loss: 0.7029
INFO:local_logger:Epoch[060/800], Step[0500/0626], Avg Loss: 0.7024
INFO:local_logger:Epoch[060/800], Step[0500/0626], Avg Loss: 0.7029
INFO:local_logger:Epoch[060/800], Step[0500/0626], Avg Loss: 0.7025
INFO:local_logger:Epoch[060/800], Step[0500/0626], Avg Loss: 0.7028
INFO:local_logger:Epoch[060/800], Step[0500/0626], Avg Loss: 0.7022
INFO:master_logger:Epoch[060/800], Step[0500/0626], Avg Loss: 0.7026
INFO:local_logger:Epoch[060/800], Step[0600/0626], Avg Loss: 0.7026
INFO:local_logger:Epoch[060/800], Step[0600/0626], Avg Loss: 0.7029
INFO:local_logger:Epoch[060/800], Step[0600/0626], Avg Loss: 0.7025
INFO:master_logger:Epoch[060/800], Step[0600/0626], Avg Loss: 0.7025
INFO:local_logger:Epoch[060/800], Step[0600/0626], Avg Loss: 0.7023
INFO:local_logger:Epoch[060/800], Step[0600/0626], Avg Loss: 0.7021
INFO:local_logger:Epoch[060/800], Step[0600/0626], Avg Loss: 0.7023
INFO:local_logger:Epoch[060/800], Step[0600/0626], Avg Loss: 0.7024
INFO:local_logger:Epoch[060/800], Step[0600/0626], Avg Loss: 0.7028
INFO:local_logger:----- Epoch[060/800], Train Loss: 0.7020, time: 883.22
INFO:local_logger:Now training epoch 61. LR=0.000151
INFO:local_logger:----- Epoch[060/800], Train Loss: 0.7027, time: 884.34
INFO:local_logger:Now training epoch 61. LR=0.000151
INFO:local_logger:----- Epoch[060/800], Train Loss: 0.7025, time: 883.83
INFO:local_logger:Now training epoch 61. LR=0.000151
INFO:local_logger:----- Epoch[060/800], Train Loss: 0.7022, time: 883.86
INFO:local_logger:Now training epoch 61. LR=0.000151
INFO:local_logger:----- Epoch[060/800], Train Loss: 0.7030, time: 883.87
INFO:local_logger:Now training epoch 61. LR=0.000151
INFO:local_logger:----- Epoch[060/800], Train Loss: 0.7023, time: 883.92
INFO:local_logger:Now training epoch 61. LR=0.000151
INFO:local_logger:----- Epoch[060/800], Train Loss: 0.7024, time: 883.95
INFO:local_logger:Now training epoch 61. LR=0.000151
INFO:local_logger:----- Epoch[060/800], Train Loss: 0.7025, time: 880.74
INFO:master_logger:----- Epoch[060/800], Train Loss: 0.7024, time: 880.74
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-60-Loss-0.7024735177213701.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-60-Loss-0.7024735177213701.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-60-Loss-0.7024735177213701.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-60-Loss-0.7024735177213701.pdopt
INFO:local_logger:Now training epoch 61. LR=0.000151
INFO:master_logger:Now training epoch 61. LR=0.000151
INFO:local_logger:Epoch[061/800], Step[0000/0626], Avg Loss: 0.6994
INFO:local_logger:Epoch[061/800], Step[0000/0626], Avg Loss: 0.6940
INFO:master_logger:Epoch[061/800], Step[0000/0626], Avg Loss: 0.6945
INFO:local_logger:Epoch[061/800], Step[0000/0626], Avg Loss: 0.6854
INFO:local_logger:Epoch[061/800], Step[0000/0626], Avg Loss: 0.6741
INFO:local_logger:Epoch[061/800], Step[0000/0626], Avg Loss: 0.7025
INFO:local_logger:Epoch[061/800], Step[0000/0626], Avg Loss: 0.7017
INFO:local_logger:Epoch[061/800], Step[0000/0626], Avg Loss: 0.6991
INFO:local_logger:Epoch[061/800], Step[0000/0626], Avg Loss: 0.7000
INFO:local_logger:Epoch[061/800], Step[0100/0626], Avg Loss: 0.7019
INFO:local_logger:Epoch[061/800], Step[0100/0626], Avg Loss: 0.7020
INFO:local_logger:Epoch[061/800], Step[0100/0626], Avg Loss: 0.7019
INFO:local_logger:Epoch[061/800], Step[0100/0626], Avg Loss: 0.7011
INFO:local_logger:Epoch[061/800], Step[0100/0626], Avg Loss: 0.7002
INFO:local_logger:Epoch[061/800], Step[0100/0626], Avg Loss: 0.7016
INFO:master_logger:Epoch[061/800], Step[0100/0626], Avg Loss: 0.7018
INFO:local_logger:Epoch[061/800], Step[0100/0626], Avg Loss: 0.7024
INFO:local_logger:Epoch[061/800], Step[0100/0626], Avg Loss: 0.7036
INFO:local_logger:Epoch[061/800], Step[0200/0626], Avg Loss: 0.7020
INFO:local_logger:Epoch[061/800], Step[0200/0626], Avg Loss: 0.7028
INFO:local_logger:Epoch[061/800], Step[0200/0626], Avg Loss: 0.7020
INFO:local_logger:Epoch[061/800], Step[0200/0626], Avg Loss: 0.7015
INFO:local_logger:Epoch[061/800], Step[0200/0626], Avg Loss: 0.7027
INFO:local_logger:Epoch[061/800], Step[0200/0626], Avg Loss: 0.7022
INFO:local_logger:Epoch[061/800], Step[0200/0626], Avg Loss: 0.7017
INFO:local_logger:Epoch[061/800], Step[0200/0626], Avg Loss: 0.7011
INFO:master_logger:Epoch[061/800], Step[0200/0626], Avg Loss: 0.7020
INFO:local_logger:Epoch[061/800], Step[0300/0626], Avg Loss: 0.7016
INFO:local_logger:Epoch[061/800], Step[0300/0626], Avg Loss: 0.7010
INFO:local_logger:Epoch[061/800], Step[0300/0626], Avg Loss: 0.7024
INFO:local_logger:Epoch[061/800], Step[0300/0626], Avg Loss: 0.7012
INFO:local_logger:Epoch[061/800], Step[0300/0626], Avg Loss: 0.7020
INFO:local_logger:Epoch[061/800], Step[0300/0626], Avg Loss: 0.7020
INFO:master_logger:Epoch[061/800], Step[0300/0626], Avg Loss: 0.7018
INFO:local_logger:Epoch[061/800], Step[0300/0626], Avg Loss: 0.7017
INFO:local_logger:Epoch[061/800], Step[0300/0626], Avg Loss: 0.7026
INFO:local_logger:Epoch[061/800], Step[0400/0626], Avg Loss: 0.7012
INFO:local_logger:Epoch[061/800], Step[0400/0626], Avg Loss: 0.7012
INFO:master_logger:Epoch[061/800], Step[0400/0626], Avg Loss: 0.7018
INFO:local_logger:Epoch[061/800], Step[0400/0626], Avg Loss: 0.7019
INFO:local_logger:Epoch[061/800], Step[0400/0626], Avg Loss: 0.7021
INFO:local_logger:Epoch[061/800], Step[0400/0626], Avg Loss: 0.7020
INFO:local_logger:Epoch[061/800], Step[0400/0626], Avg Loss: 0.7022
INFO:local_logger:Epoch[061/800], Step[0400/0626], Avg Loss: 0.7016
INFO:local_logger:Epoch[061/800], Step[0400/0626], Avg Loss: 0.7019
INFO:local_logger:Epoch[061/800], Step[0500/0626], Avg Loss: 0.7012
INFO:local_logger:Epoch[061/800], Step[0500/0626], Avg Loss: 0.7013
INFO:local_logger:Epoch[061/800], Step[0500/0626], Avg Loss: 0.7017
INFO:local_logger:Epoch[061/800], Step[0500/0626], Avg Loss: 0.7019
INFO:local_logger:Epoch[061/800], Step[0500/0626], Avg Loss: 0.7022
INFO:local_logger:Epoch[061/800], Step[0500/0626], Avg Loss: 0.7022
INFO:master_logger:Epoch[061/800], Step[0500/0626], Avg Loss: 0.7017
INFO:local_logger:Epoch[061/800], Step[0500/0626], Avg Loss: 0.7018
INFO:local_logger:Epoch[061/800], Step[0500/0626], Avg Loss: 0.7015
INFO:local_logger:Epoch[061/800], Step[0600/0626], Avg Loss: 0.7021
INFO:local_logger:Epoch[061/800], Step[0600/0626], Avg Loss: 0.7013
INFO:local_logger:Epoch[061/800], Step[0600/0626], Avg Loss: 0.7018
INFO:local_logger:Epoch[061/800], Step[0600/0626], Avg Loss: 0.7013
INFO:local_logger:Epoch[061/800], Step[0600/0626], Avg Loss: 0.7012
INFO:local_logger:Epoch[061/800], Step[0600/0626], Avg Loss: 0.7015
INFO:local_logger:Epoch[061/800], Step[0600/0626], Avg Loss: 0.7014
INFO:master_logger:Epoch[061/800], Step[0600/0626], Avg Loss: 0.7016
INFO:local_logger:Epoch[061/800], Step[0600/0626], Avg Loss: 0.7019
INFO:local_logger:----- Epoch[061/800], Train Loss: 0.7015, time: 862.93
INFO:local_logger:Now training epoch 62. LR=0.000151
INFO:local_logger:----- Epoch[061/800], Train Loss: 0.7014, time: 863.80
INFO:local_logger:Now training epoch 62. LR=0.000151
INFO:local_logger:----- Epoch[061/800], Train Loss: 0.7013, time: 863.80
INFO:local_logger:Now training epoch 62. LR=0.000151
INFO:local_logger:----- Epoch[061/800], Train Loss: 0.7012, time: 860.38
INFO:master_logger:----- Epoch[061/800], Train Loss: 0.7015, time: 860.38
INFO:local_logger:----- Epoch[061/800], Train Loss: 0.7017, time: 864.16
INFO:local_logger:Now training epoch 62. LR=0.000151
INFO:local_logger:----- Epoch[061/800], Train Loss: 0.7012, time: 864.25
INFO:local_logger:Now training epoch 62. LR=0.000151
INFO:local_logger:----- Epoch[061/800], Train Loss: 0.7019, time: 865.44
INFO:local_logger:Now training epoch 62. LR=0.000151
INFO:local_logger:----- Epoch[061/800], Train Loss: 0.7021, time: 864.25
INFO:local_logger:Now training epoch 62. LR=0.000151
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-61-Loss-0.7012385332086987.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-61-Loss-0.7012385332086987.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-61-Loss-0.7012385332086987.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-61-Loss-0.7012385332086987.pdopt
INFO:local_logger:Now training epoch 62. LR=0.000151
INFO:master_logger:Now training epoch 62. LR=0.000151
INFO:local_logger:Epoch[062/800], Step[0000/0626], Avg Loss: 0.6976
INFO:local_logger:Epoch[062/800], Step[0000/0626], Avg Loss: 0.6886
INFO:local_logger:Epoch[062/800], Step[0000/0626], Avg Loss: 0.7062
INFO:master_logger:Epoch[062/800], Step[0000/0626], Avg Loss: 0.7003
INFO:local_logger:Epoch[062/800], Step[0000/0626], Avg Loss: 0.6922
INFO:local_logger:Epoch[062/800], Step[0000/0626], Avg Loss: 0.7018
INFO:local_logger:Epoch[062/800], Step[0000/0626], Avg Loss: 0.7010
INFO:local_logger:Epoch[062/800], Step[0000/0626], Avg Loss: 0.7098
INFO:local_logger:Epoch[062/800], Step[0000/0626], Avg Loss: 0.7053
INFO:local_logger:Epoch[062/800], Step[0100/0626], Avg Loss: 0.7008
INFO:local_logger:Epoch[062/800], Step[0100/0626], Avg Loss: 0.7009
INFO:local_logger:Epoch[062/800], Step[0100/0626], Avg Loss: 0.7013
INFO:local_logger:Epoch[062/800], Step[0100/0626], Avg Loss: 0.7006
INFO:local_logger:Epoch[062/800], Step[0100/0626], Avg Loss: 0.7017
INFO:local_logger:Epoch[062/800], Step[0100/0626], Avg Loss: 0.6986
INFO:master_logger:Epoch[062/800], Step[0100/0626], Avg Loss: 0.7006
INFO:local_logger:Epoch[062/800], Step[0100/0626], Avg Loss: 0.7006
INFO:local_logger:Epoch[062/800], Step[0100/0626], Avg Loss: 0.7000
INFO:local_logger:Epoch[062/800], Step[0200/0626], Avg Loss: 0.7008
INFO:local_logger:Epoch[062/800], Step[0200/0626], Avg Loss: 0.7005
INFO:local_logger:Epoch[062/800], Step[0200/0626], Avg Loss: 0.7011
INFO:local_logger:Epoch[062/800], Step[0200/0626], Avg Loss: 0.6999
INFO:local_logger:Epoch[062/800], Step[0200/0626], Avg Loss: 0.6996
INFO:local_logger:Epoch[062/800], Step[0200/0626], Avg Loss: 0.7010
INFO:local_logger:Epoch[062/800], Step[0200/0626], Avg Loss: 0.7009
INFO:master_logger:Epoch[062/800], Step[0200/0626], Avg Loss: 0.7006
INFO:local_logger:Epoch[062/800], Step[0200/0626], Avg Loss: 0.7012
INFO:local_logger:Epoch[062/800], Step[0300/0626], Avg Loss: 0.6999
INFO:local_logger:Epoch[062/800], Step[0300/0626], Avg Loss: 0.7012
INFO:local_logger:Epoch[062/800], Step[0300/0626], Avg Loss: 0.7001
INFO:local_logger:Epoch[062/800], Step[0300/0626], Avg Loss: 0.7006
INFO:local_logger:Epoch[062/800], Step[0300/0626], Avg Loss: 0.7012
INFO:local_logger:Epoch[062/800], Step[0300/0626], Avg Loss: 0.7011
INFO:master_logger:Epoch[062/800], Step[0300/0626], Avg Loss: 0.7007
INFO:local_logger:Epoch[062/800], Step[0300/0626], Avg Loss: 0.7008
INFO:local_logger:Epoch[062/800], Step[0300/0626], Avg Loss: 0.7007
INFO:local_logger:Epoch[062/800], Step[0400/0626], Avg Loss: 0.7004
INFO:local_logger:Epoch[062/800], Step[0400/0626], Avg Loss: 0.7012
INFO:local_logger:Epoch[062/800], Step[0400/0626], Avg Loss: 0.7005
INFO:local_logger:Epoch[062/800], Step[0400/0626], Avg Loss: 0.7000
INFO:master_logger:Epoch[062/800], Step[0400/0626], Avg Loss: 0.7006
INFO:local_logger:Epoch[062/800], Step[0400/0626], Avg Loss: 0.7005
INFO:local_logger:Epoch[062/800], Step[0400/0626], Avg Loss: 0.7008
INFO:local_logger:Epoch[062/800], Step[0400/0626], Avg Loss: 0.7005
INFO:local_logger:Epoch[062/800], Step[0400/0626], Avg Loss: 0.7011
INFO:local_logger:Epoch[062/800], Step[0500/0626], Avg Loss: 0.7002
INFO:local_logger:Epoch[062/800], Step[0500/0626], Avg Loss: 0.7014
INFO:local_logger:Epoch[062/800], Step[0500/0626], Avg Loss: 0.7009
INFO:local_logger:Epoch[062/800], Step[0500/0626], Avg Loss: 0.7009
INFO:local_logger:Epoch[062/800], Step[0500/0626], Avg Loss: 0.7005
INFO:local_logger:Epoch[062/800], Step[0500/0626], Avg Loss: 0.7006
INFO:local_logger:Epoch[062/800], Step[0500/0626], Avg Loss: 0.7002
INFO:master_logger:Epoch[062/800], Step[0500/0626], Avg Loss: 0.7006
INFO:local_logger:Epoch[062/800], Step[0500/0626], Avg Loss: 0.7005
INFO:local_logger:Epoch[062/800], Step[0600/0626], Avg Loss: 0.7014
INFO:local_logger:Epoch[062/800], Step[0600/0626], Avg Loss: 0.7008
INFO:local_logger:Epoch[062/800], Step[0600/0626], Avg Loss: 0.7001
INFO:local_logger:Epoch[062/800], Step[0600/0626], Avg Loss: 0.7008
INFO:local_logger:Epoch[062/800], Step[0600/0626], Avg Loss: 0.7000
INFO:local_logger:Epoch[062/800], Step[0600/0626], Avg Loss: 0.7006
INFO:master_logger:Epoch[062/800], Step[0600/0626], Avg Loss: 0.7006
INFO:local_logger:Epoch[062/800], Step[0600/0626], Avg Loss: 0.7002
INFO:local_logger:Epoch[062/800], Step[0600/0626], Avg Loss: 0.7006
INFO:local_logger:----- Epoch[062/800], Train Loss: 0.7001, time: 880.83
INFO:local_logger:Now training epoch 63. LR=0.000151
INFO:local_logger:----- Epoch[062/800], Train Loss: 0.7001, time: 877.21
INFO:master_logger:----- Epoch[062/800], Train Loss: 0.7005, time: 877.21
INFO:local_logger:----- Epoch[062/800], Train Loss: 0.7008, time: 880.96
INFO:local_logger:Now training epoch 63. LR=0.000151
INFO:local_logger:----- Epoch[062/800], Train Loss: 0.7008, time: 881.01
INFO:local_logger:Now training epoch 63. LR=0.000151
INFO:local_logger:----- Epoch[062/800], Train Loss: 0.7006, time: 881.03
INFO:local_logger:Now training epoch 63. LR=0.000151
INFO:local_logger:----- Epoch[062/800], Train Loss: 0.7005, time: 881.40
INFO:local_logger:Now training epoch 63. LR=0.000151
INFO:local_logger:----- Epoch[062/800], Train Loss: 0.7001, time: 881.51
INFO:local_logger:Now training epoch 63. LR=0.000151
INFO:local_logger:----- Epoch[062/800], Train Loss: 0.7013, time: 882.39
INFO:local_logger:Now training epoch 63. LR=0.000151
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-62-Loss-0.7001281701651857.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-62-Loss-0.7001281701651857.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-62-Loss-0.7001281701651857.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-62-Loss-0.7001281701651857.pdopt
INFO:local_logger:Now training epoch 63. LR=0.000151
INFO:master_logger:Now training epoch 63. LR=0.000151
INFO:local_logger:Epoch[063/800], Step[0000/0626], Avg Loss: 0.7051
INFO:local_logger:Epoch[063/800], Step[0000/0626], Avg Loss: 0.7172
INFO:local_logger:Epoch[063/800], Step[0000/0626], Avg Loss: 0.6949
INFO:master_logger:Epoch[063/800], Step[0000/0626], Avg Loss: 0.6984
INFO:local_logger:Epoch[063/800], Step[0000/0626], Avg Loss: 0.7023
INFO:local_logger:Epoch[063/800], Step[0000/0626], Avg Loss: 0.7033
INFO:local_logger:Epoch[063/800], Step[0000/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[063/800], Step[0000/0626], Avg Loss: 0.6916
INFO:local_logger:Epoch[063/800], Step[0000/0626], Avg Loss: 0.6763
INFO:local_logger:Epoch[063/800], Step[0100/0626], Avg Loss: 0.6997
INFO:local_logger:Epoch[063/800], Step[0100/0626], Avg Loss: 0.7004
INFO:local_logger:Epoch[063/800], Step[0100/0626], Avg Loss: 0.7001
INFO:master_logger:Epoch[063/800], Step[0100/0626], Avg Loss: 0.7000
INFO:local_logger:Epoch[063/800], Step[0100/0626], Avg Loss: 0.7002
INFO:local_logger:Epoch[063/800], Step[0100/0626], Avg Loss: 0.7000
INFO:local_logger:Epoch[063/800], Step[0100/0626], Avg Loss: 0.6995
INFO:local_logger:Epoch[063/800], Step[0100/0626], Avg Loss: 0.7000
INFO:local_logger:Epoch[063/800], Step[0100/0626], Avg Loss: 0.7001
INFO:local_logger:Epoch[063/800], Step[0200/0626], Avg Loss: 0.7000
INFO:local_logger:Epoch[063/800], Step[0200/0626], Avg Loss: 0.6995
INFO:local_logger:Epoch[063/800], Step[0200/0626], Avg Loss: 0.7004
INFO:local_logger:Epoch[063/800], Step[0200/0626], Avg Loss: 0.6985
INFO:local_logger:Epoch[063/800], Step[0200/0626], Avg Loss: 0.7006
INFO:local_logger:Epoch[063/800], Step[0200/0626], Avg Loss: 0.6999
INFO:master_logger:Epoch[063/800], Step[0200/0626], Avg Loss: 0.6998
INFO:local_logger:Epoch[063/800], Step[0200/0626], Avg Loss: 0.6997
INFO:local_logger:Epoch[063/800], Step[0200/0626], Avg Loss: 0.7001
INFO:local_logger:Epoch[063/800], Step[0300/0626], Avg Loss: 0.6999
INFO:local_logger:Epoch[063/800], Step[0300/0626], Avg Loss: 0.7002
INFO:local_logger:Epoch[063/800], Step[0300/0626], Avg Loss: 0.6997
INFO:local_logger:Epoch[063/800], Step[0300/0626], Avg Loss: 0.7005
INFO:local_logger:Epoch[063/800], Step[0300/0626], Avg Loss: 0.7013
INFO:master_logger:Epoch[063/800], Step[0300/0626], Avg Loss: 0.7001
INFO:local_logger:Epoch[063/800], Step[0300/0626], Avg Loss: 0.7002
INFO:local_logger:Epoch[063/800], Step[0300/0626], Avg Loss: 0.6989
INFO:local_logger:Epoch[063/800], Step[0300/0626], Avg Loss: 0.6998
INFO:local_logger:Epoch[063/800], Step[0400/0626], Avg Loss: 0.6996
INFO:local_logger:Epoch[063/800], Step[0400/0626], Avg Loss: 0.6997
INFO:local_logger:Epoch[063/800], Step[0400/0626], Avg Loss: 0.6995
INFO:local_logger:Epoch[063/800], Step[0400/0626], Avg Loss: 0.6991
INFO:local_logger:Epoch[063/800], Step[0400/0626], Avg Loss: 0.7004
INFO:local_logger:Epoch[063/800], Step[0400/0626], Avg Loss: 0.7011
INFO:local_logger:Epoch[063/800], Step[0400/0626], Avg Loss: 0.7001
INFO:master_logger:Epoch[063/800], Step[0400/0626], Avg Loss: 0.6999
INFO:local_logger:Epoch[063/800], Step[0400/0626], Avg Loss: 0.7000
INFO:local_logger:Epoch[063/800], Step[0500/0626], Avg Loss: 0.6998
INFO:local_logger:Epoch[063/800], Step[0500/0626], Avg Loss: 0.7001
INFO:local_logger:Epoch[063/800], Step[0500/0626], Avg Loss: 0.7000
INFO:local_logger:Epoch[063/800], Step[0500/0626], Avg Loss: 0.6995
INFO:local_logger:Epoch[063/800], Step[0500/0626], Avg Loss: 0.7001
INFO:local_logger:Epoch[063/800], Step[0500/0626], Avg Loss: 0.7000
INFO:local_logger:Epoch[063/800], Step[0500/0626], Avg Loss: 0.7008
INFO:master_logger:Epoch[063/800], Step[0500/0626], Avg Loss: 0.7000
INFO:local_logger:Epoch[063/800], Step[0500/0626], Avg Loss: 0.6994
INFO:local_logger:Epoch[063/800], Step[0600/0626], Avg Loss: 0.6997
INFO:local_logger:Epoch[063/800], Step[0600/0626], Avg Loss: 0.6993
INFO:local_logger:Epoch[063/800], Step[0600/0626], Avg Loss: 0.6993
INFO:local_logger:Epoch[063/800], Step[0600/0626], Avg Loss: 0.6997
INFO:local_logger:Epoch[063/800], Step[0600/0626], Avg Loss: 0.6998
INFO:local_logger:Epoch[063/800], Step[0600/0626], Avg Loss: 0.7005
INFO:local_logger:Epoch[063/800], Step[0600/0626], Avg Loss: 0.6997
INFO:master_logger:Epoch[063/800], Step[0600/0626], Avg Loss: 0.6997
INFO:local_logger:Epoch[063/800], Step[0600/0626], Avg Loss: 0.6998
INFO:local_logger:----- Epoch[063/800], Train Loss: 0.6997, time: 870.66
INFO:master_logger:----- Epoch[063/800], Train Loss: 0.6997, time: 870.66
INFO:local_logger:----- Epoch[063/800], Train Loss: 0.6998, time: 874.64
INFO:local_logger:Now training epoch 64. LR=0.000151
INFO:local_logger:----- Epoch[063/800], Train Loss: 0.6993, time: 874.57
INFO:local_logger:Now training epoch 64. LR=0.000151
INFO:local_logger:----- Epoch[063/800], Train Loss: 0.6998, time: 874.56
INFO:local_logger:Now training epoch 64. LR=0.000151
INFO:local_logger:----- Epoch[063/800], Train Loss: 0.6997, time: 874.53
INFO:local_logger:Now training epoch 64. LR=0.000151
INFO:local_logger:----- Epoch[063/800], Train Loss: 0.6993, time: 874.53
INFO:local_logger:Now training epoch 64. LR=0.000151
INFO:local_logger:----- Epoch[063/800], Train Loss: 0.6998, time: 874.79
INFO:local_logger:Now training epoch 64. LR=0.000151
INFO:local_logger:----- Epoch[063/800], Train Loss: 0.7004, time: 874.66
INFO:local_logger:Now training epoch 64. LR=0.000151
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-63-Loss-0.6997380468063858.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-63-Loss-0.6997380468063858.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-63-Loss-0.6997380468063858.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-63-Loss-0.6997380468063858.pdopt
INFO:local_logger:Now training epoch 64. LR=0.000151
INFO:master_logger:Now training epoch 64. LR=0.000151
INFO:local_logger:Epoch[064/800], Step[0000/0626], Avg Loss: 0.7015
INFO:local_logger:Epoch[064/800], Step[0000/0626], Avg Loss: 0.6893
INFO:master_logger:Epoch[064/800], Step[0000/0626], Avg Loss: 0.7009
INFO:local_logger:Epoch[064/800], Step[0000/0626], Avg Loss: 0.7021
INFO:local_logger:Epoch[064/800], Step[0000/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[064/800], Step[0000/0626], Avg Loss: 0.7124
INFO:local_logger:Epoch[064/800], Step[0000/0626], Avg Loss: 0.7052
INFO:local_logger:Epoch[064/800], Step[0000/0626], Avg Loss: 0.7017
INFO:local_logger:Epoch[064/800], Step[0000/0626], Avg Loss: 0.6999
INFO:local_logger:Epoch[064/800], Step[0100/0626], Avg Loss: 0.7004
INFO:local_logger:Epoch[064/800], Step[0100/0626], Avg Loss: 0.6997
INFO:local_logger:Epoch[064/800], Step[0100/0626], Avg Loss: 0.6991
INFO:local_logger:Epoch[064/800], Step[0100/0626], Avg Loss: 0.6999
INFO:local_logger:Epoch[064/800], Step[0100/0626], Avg Loss: 0.6988
INFO:local_logger:Epoch[064/800], Step[0100/0626], Avg Loss: 0.6993
INFO:master_logger:Epoch[064/800], Step[0100/0626], Avg Loss: 0.6995
INFO:local_logger:Epoch[064/800], Step[0100/0626], Avg Loss: 0.6986
INFO:local_logger:Epoch[064/800], Step[0100/0626], Avg Loss: 0.6997
INFO:local_logger:Epoch[064/800], Step[0200/0626], Avg Loss: 0.6995
INFO:local_logger:Epoch[064/800], Step[0200/0626], Avg Loss: 0.6994
INFO:local_logger:Epoch[064/800], Step[0200/0626], Avg Loss: 0.6987
INFO:local_logger:Epoch[064/800], Step[0200/0626], Avg Loss: 0.7007
INFO:local_logger:Epoch[064/800], Step[0200/0626], Avg Loss: 0.6995
INFO:local_logger:Epoch[064/800], Step[0200/0626], Avg Loss: 0.6998
INFO:master_logger:Epoch[064/800], Step[0200/0626], Avg Loss: 0.6997
INFO:local_logger:Epoch[064/800], Step[0200/0626], Avg Loss: 0.6999
INFO:local_logger:Epoch[064/800], Step[0200/0626], Avg Loss: 0.7003
INFO:local_logger:Epoch[064/800], Step[0300/0626], Avg Loss: 0.6992
INFO:local_logger:Epoch[064/800], Step[0300/0626], Avg Loss: 0.6998
INFO:local_logger:Epoch[064/800], Step[0300/0626], Avg Loss: 0.6999
INFO:local_logger:Epoch[064/800], Step[0300/0626], Avg Loss: 0.6997
INFO:local_logger:Epoch[064/800], Step[0300/0626], Avg Loss: 0.6986
INFO:master_logger:Epoch[064/800], Step[0300/0626], Avg Loss: 0.6994
INFO:local_logger:Epoch[064/800], Step[0300/0626], Avg Loss: 0.6991
INFO:local_logger:Epoch[064/800], Step[0300/0626], Avg Loss: 0.6993
INFO:local_logger:Epoch[064/800], Step[0300/0626], Avg Loss: 0.6998
INFO:local_logger:Epoch[064/800], Step[0400/0626], Avg Loss: 0.6996
INFO:local_logger:Epoch[064/800], Step[0400/0626], Avg Loss: 0.6997
INFO:local_logger:Epoch[064/800], Step[0400/0626], Avg Loss: 0.6995
INFO:local_logger:Epoch[064/800], Step[0400/0626], Avg Loss: 0.6996
INFO:local_logger:Epoch[064/800], Step[0400/0626], Avg Loss: 0.6992
INFO:master_logger:Epoch[064/800], Step[0400/0626], Avg Loss: 0.6993
INFO:local_logger:Epoch[064/800], Step[0400/0626], Avg Loss: 0.6992
INFO:local_logger:Epoch[064/800], Step[0400/0626], Avg Loss: 0.6992
INFO:local_logger:Epoch[064/800], Step[0400/0626], Avg Loss: 0.6984
INFO:local_logger:Epoch[064/800], Step[0500/0626], Avg Loss: 0.6989
INFO:local_logger:Epoch[064/800], Step[0500/0626], Avg Loss: 0.6983
INFO:local_logger:Epoch[064/800], Step[0500/0626], Avg Loss: 0.6994
INFO:local_logger:Epoch[064/800], Step[0500/0626], Avg Loss: 0.6992
INFO:local_logger:Epoch[064/800], Step[0500/0626], Avg Loss: 0.6996
INFO:local_logger:Epoch[064/800], Step[0500/0626], Avg Loss: 0.6991
INFO:local_logger:Epoch[064/800], Step[0500/0626], Avg Loss: 0.6988
INFO:master_logger:Epoch[064/800], Step[0500/0626], Avg Loss: 0.6991
INFO:local_logger:Epoch[064/800], Step[0500/0626], Avg Loss: 0.6994
INFO:local_logger:Epoch[064/800], Step[0600/0626], Avg Loss: 0.6991
INFO:local_logger:Epoch[064/800], Step[0600/0626], Avg Loss: 0.6992
INFO:local_logger:Epoch[064/800], Step[0600/0626], Avg Loss: 0.6987
INFO:local_logger:Epoch[064/800], Step[0600/0626], Avg Loss: 0.6993
INFO:local_logger:Epoch[064/800], Step[0600/0626], Avg Loss: 0.6989
INFO:master_logger:Epoch[064/800], Step[0600/0626], Avg Loss: 0.6990
INFO:local_logger:Epoch[064/800], Step[0600/0626], Avg Loss: 0.6988
INFO:local_logger:Epoch[064/800], Step[0600/0626], Avg Loss: 0.6993
INFO:local_logger:Epoch[064/800], Step[0600/0626], Avg Loss: 0.6984
INFO:local_logger:----- Epoch[064/800], Train Loss: 0.6992, time: 883.12
INFO:local_logger:Now training epoch 65. LR=0.000151
INFO:local_logger:----- Epoch[064/800], Train Loss: 0.6983, time: 883.49
INFO:local_logger:Now training epoch 65. LR=0.000151
INFO:local_logger:----- Epoch[064/800], Train Loss: 0.6991, time: 883.58
INFO:local_logger:Now training epoch 65. LR=0.000151
INFO:local_logger:----- Epoch[064/800], Train Loss: 0.6987, time: 883.60
INFO:local_logger:Now training epoch 65. LR=0.000151
INFO:local_logger:----- Epoch[064/800], Train Loss: 0.6992, time: 880.22
INFO:master_logger:----- Epoch[064/800], Train Loss: 0.6990, time: 880.22
INFO:local_logger:----- Epoch[064/800], Train Loss: 0.6988, time: 883.73
INFO:local_logger:Now training epoch 65. LR=0.000151
INFO:local_logger:----- Epoch[064/800], Train Loss: 0.6989, time: 883.71
INFO:local_logger:Now training epoch 65. LR=0.000151
INFO:local_logger:----- Epoch[064/800], Train Loss: 0.6993, time: 883.76
INFO:local_logger:Now training epoch 65. LR=0.000151
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-64-Loss-0.6992388257000982.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-64-Loss-0.6992388257000982.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-64-Loss-0.6992388257000982.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-64-Loss-0.6992388257000982.pdopt
INFO:local_logger:Now training epoch 65. LR=0.000151
INFO:master_logger:Now training epoch 65. LR=0.000151
INFO:local_logger:Epoch[065/800], Step[0000/0626], Avg Loss: 0.7020
INFO:local_logger:Epoch[065/800], Step[0000/0626], Avg Loss: 0.7021
INFO:local_logger:Epoch[065/800], Step[0000/0626], Avg Loss: 0.7097
INFO:master_logger:Epoch[065/800], Step[0000/0626], Avg Loss: 0.6995
INFO:local_logger:Epoch[065/800], Step[0000/0626], Avg Loss: 0.7040
INFO:local_logger:Epoch[065/800], Step[0000/0626], Avg Loss: 0.6936
INFO:local_logger:Epoch[065/800], Step[0000/0626], Avg Loss: 0.6956
INFO:local_logger:Epoch[065/800], Step[0000/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[065/800], Step[0000/0626], Avg Loss: 0.6983
INFO:local_logger:Epoch[065/800], Step[0100/0626], Avg Loss: 0.6981
INFO:local_logger:Epoch[065/800], Step[0100/0626], Avg Loss: 0.6980
INFO:local_logger:Epoch[065/800], Step[0100/0626], Avg Loss: 0.6977
INFO:local_logger:Epoch[065/800], Step[0100/0626], Avg Loss: 0.6977
INFO:local_logger:Epoch[065/800], Step[0100/0626], Avg Loss: 0.6990
INFO:local_logger:Epoch[065/800], Step[0100/0626], Avg Loss: 0.6990
INFO:local_logger:Epoch[065/800], Step[0100/0626], Avg Loss: 0.6972
INFO:local_logger:Epoch[065/800], Step[0100/0626], Avg Loss: 0.6976
INFO:master_logger:Epoch[065/800], Step[0100/0626], Avg Loss: 0.6980
INFO:local_logger:Epoch[065/800], Step[0200/0626], Avg Loss: 0.6989
INFO:local_logger:Epoch[065/800], Step[0200/0626], Avg Loss: 0.6979
INFO:local_logger:Epoch[065/800], Step[0200/0626], Avg Loss: 0.6986
INFO:local_logger:Epoch[065/800], Step[0200/0626], Avg Loss: 0.6989
INFO:local_logger:Epoch[065/800], Step[0200/0626], Avg Loss: 0.6983
INFO:master_logger:Epoch[065/800], Step[0200/0626], Avg Loss: 0.6984
INFO:local_logger:Epoch[065/800], Step[0200/0626], Avg Loss: 0.6981
INFO:local_logger:Epoch[065/800], Step[0200/0626], Avg Loss: 0.6985
INFO:local_logger:Epoch[065/800], Step[0200/0626], Avg Loss: 0.6981
INFO:local_logger:Epoch[065/800], Step[0300/0626], Avg Loss: 0.6983
INFO:local_logger:Epoch[065/800], Step[0300/0626], Avg Loss: 0.6990
INFO:local_logger:Epoch[065/800], Step[0300/0626], Avg Loss: 0.6980
INFO:local_logger:Epoch[065/800], Step[0300/0626], Avg Loss: 0.6984
INFO:local_logger:Epoch[065/800], Step[0300/0626], Avg Loss: 0.6986
INFO:local_logger:Epoch[065/800], Step[0300/0626], Avg Loss: 0.6982
INFO:master_logger:Epoch[065/800], Step[0300/0626], Avg Loss: 0.6984
INFO:local_logger:Epoch[065/800], Step[0300/0626], Avg Loss: 0.6984
INFO:local_logger:Epoch[065/800], Step[0300/0626], Avg Loss: 0.6979
INFO:local_logger:Epoch[065/800], Step[0400/0626], Avg Loss: 0.6985
INFO:local_logger:Epoch[065/800], Step[0400/0626], Avg Loss: 0.6980
INFO:local_logger:Epoch[065/800], Step[0400/0626], Avg Loss: 0.6981
INFO:local_logger:Epoch[065/800], Step[0400/0626], Avg Loss: 0.6990
INFO:master_logger:Epoch[065/800], Step[0400/0626], Avg Loss: 0.6982
INFO:local_logger:Epoch[065/800], Step[0400/0626], Avg Loss: 0.6980
INFO:local_logger:Epoch[065/800], Step[0400/0626], Avg Loss: 0.6978
INFO:local_logger:Epoch[065/800], Step[0400/0626], Avg Loss: 0.6981
INFO:local_logger:Epoch[065/800], Step[0400/0626], Avg Loss: 0.6982
INFO:local_logger:Epoch[065/800], Step[0500/0626], Avg Loss: 0.6984
INFO:local_logger:Epoch[065/800], Step[0500/0626], Avg Loss: 0.6983
INFO:local_logger:Epoch[065/800], Step[0500/0626], Avg Loss: 0.6990
INFO:local_logger:Epoch[065/800], Step[0500/0626], Avg Loss: 0.6981
INFO:local_logger:Epoch[065/800], Step[0500/0626], Avg Loss: 0.6976
INFO:local_logger:Epoch[065/800], Step[0500/0626], Avg Loss: 0.6980
INFO:master_logger:Epoch[065/800], Step[0500/0626], Avg Loss: 0.6981
INFO:local_logger:Epoch[065/800], Step[0500/0626], Avg Loss: 0.6981
INFO:local_logger:Epoch[065/800], Step[0500/0626], Avg Loss: 0.6977
INFO:local_logger:Epoch[065/800], Step[0600/0626], Avg Loss: 0.6982
INFO:local_logger:Epoch[065/800], Step[0600/0626], Avg Loss: 0.6982
INFO:local_logger:Epoch[065/800], Step[0600/0626], Avg Loss: 0.6981
INFO:local_logger:Epoch[065/800], Step[0600/0626], Avg Loss: 0.6977
INFO:local_logger:Epoch[065/800], Step[0600/0626], Avg Loss: 0.6988
INFO:master_logger:Epoch[065/800], Step[0600/0626], Avg Loss: 0.6981
INFO:local_logger:Epoch[065/800], Step[0600/0626], Avg Loss: 0.6978
INFO:local_logger:Epoch[065/800], Step[0600/0626], Avg Loss: 0.6975
INFO:local_logger:Epoch[065/800], Step[0600/0626], Avg Loss: 0.6981
INFO:local_logger:----- Epoch[065/800], Train Loss: 0.6981, time: 871.05
INFO:local_logger:Now training epoch 66. LR=0.000151
INFO:local_logger:----- Epoch[065/800], Train Loss: 0.6988, time: 867.64
INFO:master_logger:----- Epoch[065/800], Train Loss: 0.6980, time: 867.64
INFO:local_logger:----- Epoch[065/800], Train Loss: 0.6977, time: 871.33
INFO:local_logger:Now training epoch 66. LR=0.000151
INFO:local_logger:----- Epoch[065/800], Train Loss: 0.6980, time: 872.01
INFO:local_logger:Now training epoch 66. LR=0.000151
INFO:local_logger:----- Epoch[065/800], Train Loss: 0.6975, time: 871.43
INFO:local_logger:Now training epoch 66. LR=0.000151
INFO:local_logger:----- Epoch[065/800], Train Loss: 0.6982, time: 871.92
INFO:local_logger:Now training epoch 66. LR=0.000151
INFO:local_logger:----- Epoch[065/800], Train Loss: 0.6981, time: 871.81
INFO:local_logger:Now training epoch 66. LR=0.000151
INFO:local_logger:----- Epoch[065/800], Train Loss: 0.6978, time: 871.98
INFO:local_logger:Now training epoch 66. LR=0.000151
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-65-Loss-0.6988199688404415.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-65-Loss-0.6988199688404415.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-65-Loss-0.6988199688404415.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-65-Loss-0.6988199688404415.pdopt
INFO:local_logger:Now training epoch 66. LR=0.000151
INFO:master_logger:Now training epoch 66. LR=0.000151
INFO:local_logger:Epoch[066/800], Step[0000/0626], Avg Loss: 0.6946
INFO:local_logger:Epoch[066/800], Step[0000/0626], Avg Loss: 0.6997
INFO:master_logger:Epoch[066/800], Step[0000/0626], Avg Loss: 0.6995
INFO:local_logger:Epoch[066/800], Step[0000/0626], Avg Loss: 0.6986
INFO:local_logger:Epoch[066/800], Step[0000/0626], Avg Loss: 0.7179
INFO:local_logger:Epoch[066/800], Step[0000/0626], Avg Loss: 0.6919
INFO:local_logger:Epoch[066/800], Step[0000/0626], Avg Loss: 0.6927
INFO:local_logger:Epoch[066/800], Step[0000/0626], Avg Loss: 0.7034
INFO:local_logger:Epoch[066/800], Step[0000/0626], Avg Loss: 0.6970
INFO:local_logger:Epoch[066/800], Step[0100/0626], Avg Loss: 0.6978
INFO:local_logger:Epoch[066/800], Step[0100/0626], Avg Loss: 0.6981
INFO:local_logger:Epoch[066/800], Step[0100/0626], Avg Loss: 0.6987
INFO:master_logger:Epoch[066/800], Step[0100/0626], Avg Loss: 0.6977
INFO:local_logger:Epoch[066/800], Step[0100/0626], Avg Loss: 0.6980
INFO:local_logger:Epoch[066/800], Step[0100/0626], Avg Loss: 0.6973
INFO:local_logger:Epoch[066/800], Step[0100/0626], Avg Loss: 0.6973
INFO:local_logger:Epoch[066/800], Step[0100/0626], Avg Loss: 0.6979
INFO:local_logger:Epoch[066/800], Step[0100/0626], Avg Loss: 0.6965
INFO:local_logger:Epoch[066/800], Step[0200/0626], Avg Loss: 0.6971
INFO:local_logger:Epoch[066/800], Step[0200/0626], Avg Loss: 0.6974
INFO:local_logger:Epoch[066/800], Step[0200/0626], Avg Loss: 0.6972
INFO:local_logger:Epoch[066/800], Step[0200/0626], Avg Loss: 0.6980
INFO:local_logger:Epoch[066/800], Step[0200/0626], Avg Loss: 0.6976
INFO:local_logger:Epoch[066/800], Step[0200/0626], Avg Loss: 0.6988
INFO:local_logger:Epoch[066/800], Step[0200/0626], Avg Loss: 0.6973
INFO:local_logger:Epoch[066/800], Step[0200/0626], Avg Loss: 0.6983
INFO:master_logger:Epoch[066/800], Step[0200/0626], Avg Loss: 0.6977
INFO:local_logger:Epoch[066/800], Step[0300/0626], Avg Loss: 0.6977
INFO:local_logger:Epoch[066/800], Step[0300/0626], Avg Loss: 0.6978
INFO:local_logger:Epoch[066/800], Step[0300/0626], Avg Loss: 0.6973
INFO:local_logger:Epoch[066/800], Step[0300/0626], Avg Loss: 0.6976
INFO:local_logger:Epoch[066/800], Step[0300/0626], Avg Loss: 0.6971
INFO:local_logger:Epoch[066/800], Step[0300/0626], Avg Loss: 0.6975
INFO:local_logger:Epoch[066/800], Step[0300/0626], Avg Loss: 0.6984
INFO:master_logger:Epoch[066/800], Step[0300/0626], Avg Loss: 0.6977
INFO:local_logger:Epoch[066/800], Step[0300/0626], Avg Loss: 0.6984
INFO:local_logger:Epoch[066/800], Step[0400/0626], Avg Loss: 0.6972
INFO:local_logger:Epoch[066/800], Step[0400/0626], Avg Loss: 0.6973
INFO:local_logger:Epoch[066/800], Step[0400/0626], Avg Loss: 0.6975
INFO:local_logger:Epoch[066/800], Step[0400/0626], Avg Loss: 0.6974
INFO:local_logger:Epoch[066/800], Step[0400/0626], Avg Loss: 0.6981
INFO:local_logger:Epoch[066/800], Step[0400/0626], Avg Loss: 0.6970
INFO:master_logger:Epoch[066/800], Step[0400/0626], Avg Loss: 0.6976
INFO:local_logger:Epoch[066/800], Step[0400/0626], Avg Loss: 0.6980
INFO:local_logger:Epoch[066/800], Step[0400/0626], Avg Loss: 0.6979
INFO:local_logger:Epoch[066/800], Step[0500/0626], Avg Loss: 0.6972
INFO:local_logger:Epoch[066/800], Step[0500/0626], Avg Loss: 0.6978
INFO:local_logger:Epoch[066/800], Step[0500/0626], Avg Loss: 0.6972
INFO:local_logger:Epoch[066/800], Step[0500/0626], Avg Loss: 0.6971
INFO:local_logger:Epoch[066/800], Step[0500/0626], Avg Loss: 0.6970
INFO:master_logger:Epoch[066/800], Step[0500/0626], Avg Loss: 0.6974
INFO:local_logger:Epoch[066/800], Step[0500/0626], Avg Loss: 0.6977
INFO:local_logger:Epoch[066/800], Step[0500/0626], Avg Loss: 0.6975
INFO:local_logger:Epoch[066/800], Step[0500/0626], Avg Loss: 0.6977
INFO:local_logger:Epoch[066/800], Step[0600/0626], Avg Loss: 0.6971
INFO:local_logger:Epoch[066/800], Step[0600/0626], Avg Loss: 0.6977
INFO:local_logger:Epoch[066/800], Step[0600/0626], Avg Loss: 0.6976
INFO:local_logger:Epoch[066/800], Step[0600/0626], Avg Loss: 0.6970
INFO:local_logger:Epoch[066/800], Step[0600/0626], Avg Loss: 0.6976
INFO:local_logger:Epoch[066/800], Step[0600/0626], Avg Loss: 0.6972
INFO:local_logger:Epoch[066/800], Step[0600/0626], Avg Loss: 0.6970
INFO:local_logger:Epoch[066/800], Step[0600/0626], Avg Loss: 0.6975
INFO:master_logger:Epoch[066/800], Step[0600/0626], Avg Loss: 0.6973
INFO:local_logger:----- Epoch[066/800], Train Loss: 0.6970, time: 874.18
INFO:master_logger:----- Epoch[066/800], Train Loss: 0.6973, time: 874.18
INFO:local_logger:----- Epoch[066/800], Train Loss: 0.6976, time: 878.36
INFO:local_logger:Now training epoch 67. LR=0.000151
INFO:local_logger:----- Epoch[066/800], Train Loss: 0.6969, time: 877.55
INFO:local_logger:Now training epoch 67. LR=0.000151
INFO:local_logger:----- Epoch[066/800], Train Loss: 0.6971, time: 878.04
INFO:local_logger:Now training epoch 67. LR=0.000151
INFO:local_logger:----- Epoch[066/800], Train Loss: 0.6977, time: 877.55
INFO:local_logger:Now training epoch 67. LR=0.000151
INFO:local_logger:----- Epoch[066/800], Train Loss: 0.6970, time: 878.01
INFO:local_logger:Now training epoch 67. LR=0.000151
INFO:local_logger:----- Epoch[066/800], Train Loss: 0.6975, time: 878.03
INFO:local_logger:Now training epoch 67. LR=0.000151
INFO:local_logger:----- Epoch[066/800], Train Loss: 0.6975, time: 878.01
INFO:local_logger:Now training epoch 67. LR=0.000151
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-66-Loss-0.6970274622691075.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-66-Loss-0.6970274622691075.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-66-Loss-0.6970274622691075.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-66-Loss-0.6970274622691075.pdopt
INFO:local_logger:Now training epoch 67. LR=0.000151
INFO:master_logger:Now training epoch 67. LR=0.000151
INFO:local_logger:Epoch[067/800], Step[0000/0626], Avg Loss: 0.6978
INFO:master_logger:Epoch[067/800], Step[0000/0626], Avg Loss: 0.6958
INFO:local_logger:Epoch[067/800], Step[0000/0626], Avg Loss: 0.6925
INFO:local_logger:Epoch[067/800], Step[0000/0626], Avg Loss: 0.6998
INFO:local_logger:Epoch[067/800], Step[0000/0626], Avg Loss: 0.7094
INFO:local_logger:Epoch[067/800], Step[0000/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[067/800], Step[0000/0626], Avg Loss: 0.6943
INFO:local_logger:Epoch[067/800], Step[0000/0626], Avg Loss: 0.6945
INFO:local_logger:Epoch[067/800], Step[0000/0626], Avg Loss: 0.6944
INFO:local_logger:Epoch[067/800], Step[0100/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[067/800], Step[0100/0626], Avg Loss: 0.6981
INFO:local_logger:Epoch[067/800], Step[0100/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[067/800], Step[0100/0626], Avg Loss: 0.6969
INFO:local_logger:Epoch[067/800], Step[0100/0626], Avg Loss: 0.6956
INFO:local_logger:Epoch[067/800], Step[0100/0626], Avg Loss: 0.6968
INFO:local_logger:Epoch[067/800], Step[0100/0626], Avg Loss: 0.6962
INFO:master_logger:Epoch[067/800], Step[0100/0626], Avg Loss: 0.6966
INFO:local_logger:Epoch[067/800], Step[0100/0626], Avg Loss: 0.6968
INFO:local_logger:Epoch[067/800], Step[0200/0626], Avg Loss: 0.6973
INFO:local_logger:Epoch[067/800], Step[0200/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[067/800], Step[0200/0626], Avg Loss: 0.6968
INFO:local_logger:Epoch[067/800], Step[0200/0626], Avg Loss: 0.6974
INFO:local_logger:Epoch[067/800], Step[0200/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[067/800], Step[0200/0626], Avg Loss: 0.6971
INFO:master_logger:Epoch[067/800], Step[0200/0626], Avg Loss: 0.6968
INFO:local_logger:Epoch[067/800], Step[0200/0626], Avg Loss: 0.6974
INFO:local_logger:Epoch[067/800], Step[0200/0626], Avg Loss: 0.6966
INFO:local_logger:Epoch[067/800], Step[0300/0626], Avg Loss: 0.6969
INFO:local_logger:Epoch[067/800], Step[0300/0626], Avg Loss: 0.6967
INFO:local_logger:Epoch[067/800], Step[0300/0626], Avg Loss: 0.6966
INFO:local_logger:Epoch[067/800], Step[0300/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[067/800], Step[0300/0626], Avg Loss: 0.6973
INFO:local_logger:Epoch[067/800], Step[0300/0626], Avg Loss: 0.6958
INFO:local_logger:Epoch[067/800], Step[0300/0626], Avg Loss: 0.6972
INFO:local_logger:Epoch[067/800], Step[0300/0626], Avg Loss: 0.6964
INFO:master_logger:Epoch[067/800], Step[0300/0626], Avg Loss: 0.6966
INFO:local_logger:Epoch[067/800], Step[0400/0626], Avg Loss: 0.6960
INFO:local_logger:Epoch[067/800], Step[0400/0626], Avg Loss: 0.6972
INFO:local_logger:Epoch[067/800], Step[0400/0626], Avg Loss: 0.6964
INFO:local_logger:Epoch[067/800], Step[0400/0626], Avg Loss: 0.6971
INFO:local_logger:Epoch[067/800], Step[0400/0626], Avg Loss: 0.6967
INFO:local_logger:Epoch[067/800], Step[0400/0626], Avg Loss: 0.6966
INFO:master_logger:Epoch[067/800], Step[0400/0626], Avg Loss: 0.6966
INFO:local_logger:Epoch[067/800], Step[0400/0626], Avg Loss: 0.6968
INFO:local_logger:Epoch[067/800], Step[0400/0626], Avg Loss: 0.6958
INFO:local_logger:Epoch[067/800], Step[0500/0626], Avg Loss: 0.6959
INFO:local_logger:Epoch[067/800], Step[0500/0626], Avg Loss: 0.6968
INFO:local_logger:Epoch[067/800], Step[0500/0626], Avg Loss: 0.6965
INFO:local_logger:Epoch[067/800], Step[0500/0626], Avg Loss: 0.6960
INFO:local_logger:Epoch[067/800], Step[0500/0626], Avg Loss: 0.6964
INFO:local_logger:Epoch[067/800], Step[0500/0626], Avg Loss: 0.6968
INFO:master_logger:Epoch[067/800], Step[0500/0626], Avg Loss: 0.6966
INFO:local_logger:Epoch[067/800], Step[0500/0626], Avg Loss: 0.6972
INFO:local_logger:Epoch[067/800], Step[0500/0626], Avg Loss: 0.6971
INFO:local_logger:Epoch[067/800], Step[0600/0626], Avg Loss: 0.6960
INFO:local_logger:Epoch[067/800], Step[0600/0626], Avg Loss: 0.6965
INFO:local_logger:Epoch[067/800], Step[0600/0626], Avg Loss: 0.6968
INFO:local_logger:Epoch[067/800], Step[0600/0626], Avg Loss: 0.6970
INFO:local_logger:Epoch[067/800], Step[0600/0626], Avg Loss: 0.6965
INFO:local_logger:Epoch[067/800], Step[0600/0626], Avg Loss: 0.6965
INFO:local_logger:Epoch[067/800], Step[0600/0626], Avg Loss: 0.6970
INFO:master_logger:Epoch[067/800], Step[0600/0626], Avg Loss: 0.6965
INFO:local_logger:Epoch[067/800], Step[0600/0626], Avg Loss: 0.6961
INFO:local_logger:----- Epoch[067/800], Train Loss: 0.6970, time: 875.35
INFO:local_logger:Now training epoch 68. LR=0.000151
INFO:local_logger:----- Epoch[067/800], Train Loss: 0.6968, time: 872.12
INFO:master_logger:----- Epoch[067/800], Train Loss: 0.6965, time: 872.12
INFO:local_logger:----- Epoch[067/800], Train Loss: 0.6965, time: 876.01
INFO:local_logger:Now training epoch 68. LR=0.000151
INFO:local_logger:----- Epoch[067/800], Train Loss: 0.6965, time: 875.96
INFO:local_logger:Now training epoch 68. LR=0.000151
INFO:local_logger:----- Epoch[067/800], Train Loss: 0.6970, time: 875.59
INFO:local_logger:Now training epoch 68. LR=0.000151
INFO:local_logger:----- Epoch[067/800], Train Loss: 0.6961, time: 876.05
INFO:local_logger:Now training epoch 68. LR=0.000151
INFO:local_logger:----- Epoch[067/800], Train Loss: 0.6965, time: 875.92
INFO:local_logger:Now training epoch 68. LR=0.000151
INFO:local_logger:----- Epoch[067/800], Train Loss: 0.6960, time: 876.13
INFO:local_logger:Now training epoch 68. LR=0.000151
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-67-Loss-0.696807305239932.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-67-Loss-0.696807305239932.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-67-Loss-0.696807305239932.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-67-Loss-0.696807305239932.pdopt
INFO:local_logger:Now training epoch 68. LR=0.000151
INFO:master_logger:Now training epoch 68. LR=0.000151
INFO:local_logger:Epoch[068/800], Step[0000/0626], Avg Loss: 0.6982
INFO:local_logger:Epoch[068/800], Step[0000/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[068/800], Step[0000/0626], Avg Loss: 0.6916
INFO:local_logger:Epoch[068/800], Step[0000/0626], Avg Loss: 0.7004
INFO:master_logger:Epoch[068/800], Step[0000/0626], Avg Loss: 0.6972
INFO:local_logger:Epoch[068/800], Step[0000/0626], Avg Loss: 0.7004
INFO:local_logger:Epoch[068/800], Step[0000/0626], Avg Loss: 0.7004
INFO:local_logger:Epoch[068/800], Step[0000/0626], Avg Loss: 0.7036
INFO:local_logger:Epoch[068/800], Step[0000/0626], Avg Loss: 0.6894
INFO:local_logger:Epoch[068/800], Step[0100/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[068/800], Step[0100/0626], Avg Loss: 0.6969
INFO:master_logger:Epoch[068/800], Step[0100/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[068/800], Step[0100/0626], Avg Loss: 0.6964
INFO:local_logger:Epoch[068/800], Step[0100/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[068/800], Step[0100/0626], Avg Loss: 0.6950
INFO:local_logger:Epoch[068/800], Step[0100/0626], Avg Loss: 0.6970
INFO:local_logger:Epoch[068/800], Step[0100/0626], Avg Loss: 0.6966
INFO:local_logger:Epoch[068/800], Step[0100/0626], Avg Loss: 0.6976
INFO:local_logger:Epoch[068/800], Step[0200/0626], Avg Loss: 0.6962
INFO:local_logger:Epoch[068/800], Step[0200/0626], Avg Loss: 0.6954
INFO:local_logger:Epoch[068/800], Step[0200/0626], Avg Loss: 0.6952
INFO:local_logger:Epoch[068/800], Step[0200/0626], Avg Loss: 0.6968
INFO:local_logger:Epoch[068/800], Step[0200/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[068/800], Step[0200/0626], Avg Loss: 0.6969
INFO:local_logger:Epoch[068/800], Step[0200/0626], Avg Loss: 0.6966
INFO:local_logger:Epoch[068/800], Step[0200/0626], Avg Loss: 0.6954
INFO:master_logger:Epoch[068/800], Step[0200/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[068/800], Step[0300/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[068/800], Step[0300/0626], Avg Loss: 0.6958
INFO:local_logger:Epoch[068/800], Step[0300/0626], Avg Loss: 0.6964
INFO:local_logger:Epoch[068/800], Step[0300/0626], Avg Loss: 0.6963
INFO:local_logger:Epoch[068/800], Step[0300/0626], Avg Loss: 0.6960
INFO:local_logger:Epoch[068/800], Step[0300/0626], Avg Loss: 0.6950
INFO:local_logger:Epoch[068/800], Step[0300/0626], Avg Loss: 0.6970
INFO:local_logger:Epoch[068/800], Step[0300/0626], Avg Loss: 0.6958
INFO:master_logger:Epoch[068/800], Step[0300/0626], Avg Loss: 0.6959
INFO:local_logger:Epoch[068/800], Step[0400/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[068/800], Step[0400/0626], Avg Loss: 0.6963
INFO:local_logger:Epoch[068/800], Step[0400/0626], Avg Loss: 0.6962
INFO:local_logger:Epoch[068/800], Step[0400/0626], Avg Loss: 0.6956
INFO:local_logger:Epoch[068/800], Step[0400/0626], Avg Loss: 0.6965
INFO:local_logger:Epoch[068/800], Step[0400/0626], Avg Loss: 0.6959
INFO:master_logger:Epoch[068/800], Step[0400/0626], Avg Loss: 0.6959
INFO:local_logger:Epoch[068/800], Step[0400/0626], Avg Loss: 0.6948
INFO:local_logger:Epoch[068/800], Step[0400/0626], Avg Loss: 0.6955
INFO:local_logger:Epoch[068/800], Step[0500/0626], Avg Loss: 0.6962
INFO:local_logger:Epoch[068/800], Step[0500/0626], Avg Loss: 0.6954
INFO:local_logger:Epoch[068/800], Step[0500/0626], Avg Loss: 0.6946
INFO:local_logger:Epoch[068/800], Step[0500/0626], Avg Loss: 0.6965
INFO:local_logger:Epoch[068/800], Step[0500/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[068/800], Step[0500/0626], Avg Loss: 0.6959
INFO:local_logger:Epoch[068/800], Step[0500/0626], Avg Loss: 0.6957
INFO:master_logger:Epoch[068/800], Step[0500/0626], Avg Loss: 0.6958
INFO:local_logger:Epoch[068/800], Step[0500/0626], Avg Loss: 0.6958
INFO:local_logger:Epoch[068/800], Step[0600/0626], Avg Loss: 0.6946
INFO:local_logger:Epoch[068/800], Step[0600/0626], Avg Loss: 0.6960
INFO:local_logger:Epoch[068/800], Step[0600/0626], Avg Loss: 0.6958
INFO:local_logger:Epoch[068/800], Step[0600/0626], Avg Loss: 0.6957
INFO:local_logger:Epoch[068/800], Step[0600/0626], Avg Loss: 0.6957
INFO:local_logger:Epoch[068/800], Step[0600/0626], Avg Loss: 0.6958
INFO:local_logger:Epoch[068/800], Step[0600/0626], Avg Loss: 0.6962
INFO:local_logger:Epoch[068/800], Step[0600/0626], Avg Loss: 0.6960
INFO:master_logger:Epoch[068/800], Step[0600/0626], Avg Loss: 0.6957
INFO:local_logger:----- Epoch[068/800], Train Loss: 0.6946, time: 870.06
INFO:local_logger:Now training epoch 69. LR=0.000151
INFO:local_logger:----- Epoch[068/800], Train Loss: 0.6959, time: 870.67
INFO:local_logger:----- Epoch[068/800], Train Loss: 0.6958, time: 871.34
INFO:local_logger:Now training epoch 69. LR=0.000151
INFO:local_logger:Now training epoch 69. LR=0.000151
INFO:local_logger:----- Epoch[068/800], Train Loss: 0.6961, time: 870.65
INFO:local_logger:Now training epoch 69. LR=0.000151
INFO:local_logger:----- Epoch[068/800], Train Loss: 0.6958, time: 870.67
INFO:local_logger:Now training epoch 69. LR=0.000151
INFO:local_logger:----- Epoch[068/800], Train Loss: 0.6961, time: 870.74
INFO:local_logger:Now training epoch 69. LR=0.000151
INFO:local_logger:----- Epoch[068/800], Train Loss: 0.6960, time: 870.73
INFO:local_logger:Now training epoch 69. LR=0.000151
INFO:local_logger:----- Epoch[068/800], Train Loss: 0.6957, time: 867.42
INFO:master_logger:----- Epoch[068/800], Train Loss: 0.6957, time: 867.42
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-68-Loss-0.6956601794348751.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-68-Loss-0.6956601794348751.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-68-Loss-0.6956601794348751.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-68-Loss-0.6956601794348751.pdopt
INFO:local_logger:Now training epoch 69. LR=0.000151
INFO:master_logger:Now training epoch 69. LR=0.000151
INFO:local_logger:Epoch[069/800], Step[0000/0626], Avg Loss: 0.6915
INFO:local_logger:Epoch[069/800], Step[0000/0626], Avg Loss: 0.6951
INFO:local_logger:Epoch[069/800], Step[0000/0626], Avg Loss: 0.6895
INFO:local_logger:Epoch[069/800], Step[0000/0626], Avg Loss: 0.6998
INFO:local_logger:Epoch[069/800], Step[0000/0626], Avg Loss: 0.6966
INFO:local_logger:Epoch[069/800], Step[0000/0626], Avg Loss: 0.6932
INFO:master_logger:Epoch[069/800], Step[0000/0626], Avg Loss: 0.6968
INFO:local_logger:Epoch[069/800], Step[0000/0626], Avg Loss: 0.7177
INFO:local_logger:Epoch[069/800], Step[0000/0626], Avg Loss: 0.6914
INFO:local_logger:Epoch[069/800], Step[0100/0626], Avg Loss: 0.6957
INFO:local_logger:Epoch[069/800], Step[0100/0626], Avg Loss: 0.6964
INFO:local_logger:Epoch[069/800], Step[0100/0626], Avg Loss: 0.6955
INFO:local_logger:Epoch[069/800], Step[0100/0626], Avg Loss: 0.6955
INFO:local_logger:Epoch[069/800], Step[0100/0626], Avg Loss: 0.6945
INFO:local_logger:Epoch[069/800], Step[0100/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[069/800], Step[0100/0626], Avg Loss: 0.6959
INFO:master_logger:Epoch[069/800], Step[0100/0626], Avg Loss: 0.6956
INFO:local_logger:Epoch[069/800], Step[0100/0626], Avg Loss: 0.6950
INFO:local_logger:Epoch[069/800], Step[0200/0626], Avg Loss: 0.6950
INFO:local_logger:Epoch[069/800], Step[0200/0626], Avg Loss: 0.6948
INFO:local_logger:Epoch[069/800], Step[0200/0626], Avg Loss: 0.6954
INFO:local_logger:Epoch[069/800], Step[0200/0626], Avg Loss: 0.6948
INFO:local_logger:Epoch[069/800], Step[0200/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[069/800], Step[0200/0626], Avg Loss: 0.6960
INFO:local_logger:Epoch[069/800], Step[0200/0626], Avg Loss: 0.6949
INFO:master_logger:Epoch[069/800], Step[0200/0626], Avg Loss: 0.6950
INFO:local_logger:Epoch[069/800], Step[0200/0626], Avg Loss: 0.6945
INFO:local_logger:Epoch[069/800], Step[0300/0626], Avg Loss: 0.6958
INFO:local_logger:Epoch[069/800], Step[0300/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[069/800], Step[0300/0626], Avg Loss: 0.6953
INFO:local_logger:Epoch[069/800], Step[0300/0626], Avg Loss: 0.6950
INFO:local_logger:Epoch[069/800], Step[0300/0626], Avg Loss: 0.6944
INFO:master_logger:Epoch[069/800], Step[0300/0626], Avg Loss: 0.6951
INFO:local_logger:Epoch[069/800], Step[0300/0626], Avg Loss: 0.6958
INFO:local_logger:Epoch[069/800], Step[0300/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[069/800], Step[0300/0626], Avg Loss: 0.6946
INFO:local_logger:Epoch[069/800], Step[0400/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[069/800], Step[0400/0626], Avg Loss: 0.6952
INFO:local_logger:Epoch[069/800], Step[0400/0626], Avg Loss: 0.6960
INFO:local_logger:Epoch[069/800], Step[0400/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[069/800], Step[0400/0626], Avg Loss: 0.6953
INFO:master_logger:Epoch[069/800], Step[0400/0626], Avg Loss: 0.6952
INFO:local_logger:Epoch[069/800], Step[0400/0626], Avg Loss: 0.6960
INFO:local_logger:Epoch[069/800], Step[0400/0626], Avg Loss: 0.6948
INFO:local_logger:Epoch[069/800], Step[0400/0626], Avg Loss: 0.6950
INFO:local_logger:Epoch[069/800], Step[0500/0626], Avg Loss: 0.6956
INFO:local_logger:Epoch[069/800], Step[0500/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[069/800], Step[0500/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[069/800], Step[0500/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[069/800], Step[0500/0626], Avg Loss: 0.6954
INFO:local_logger:Epoch[069/800], Step[0500/0626], Avg Loss: 0.6955
INFO:local_logger:Epoch[069/800], Step[0500/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[069/800], Step[0500/0626], Avg Loss: 0.6950
INFO:master_logger:Epoch[069/800], Step[0500/0626], Avg Loss: 0.6951
INFO:local_logger:Epoch[069/800], Step[0600/0626], Avg Loss: 0.6946
INFO:local_logger:Epoch[069/800], Step[0600/0626], Avg Loss: 0.6948
INFO:local_logger:Epoch[069/800], Step[0600/0626], Avg Loss: 0.6948
INFO:local_logger:Epoch[069/800], Step[0600/0626], Avg Loss: 0.6957
INFO:local_logger:Epoch[069/800], Step[0600/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[069/800], Step[0600/0626], Avg Loss: 0.6947
INFO:master_logger:Epoch[069/800], Step[0600/0626], Avg Loss: 0.6950
INFO:local_logger:Epoch[069/800], Step[0600/0626], Avg Loss: 0.6953
INFO:local_logger:Epoch[069/800], Step[0600/0626], Avg Loss: 0.6953
INFO:local_logger:----- Epoch[069/800], Train Loss: 0.6957, time: 877.32
INFO:local_logger:Now training epoch 70. LR=0.000151
INFO:local_logger:----- Epoch[069/800], Train Loss: 0.6947, time: 876.82
INFO:local_logger:Now training epoch 70. LR=0.000151
INFO:local_logger:----- Epoch[069/800], Train Loss: 0.6954, time: 877.40
INFO:local_logger:Now training epoch 70. LR=0.000151
INFO:local_logger:----- Epoch[069/800], Train Loss: 0.6947, time: 877.36
INFO:local_logger:Now training epoch 70. LR=0.000151
INFO:local_logger:----- Epoch[069/800], Train Loss: 0.6953, time: 877.48
INFO:local_logger:Now training epoch 70. LR=0.000151
INFO:local_logger:----- Epoch[069/800], Train Loss: 0.6948, time: 877.49
INFO:local_logger:Now training epoch 70. LR=0.000151
INFO:local_logger:----- Epoch[069/800], Train Loss: 0.6945, time: 877.44
INFO:local_logger:Now training epoch 70. LR=0.000151
INFO:local_logger:----- Epoch[069/800], Train Loss: 0.6950, time: 873.74
INFO:master_logger:----- Epoch[069/800], Train Loss: 0.6950, time: 873.74
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-69-Loss-0.6949500013049544.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-69-Loss-0.6949500013049544.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-69-Loss-0.6949500013049544.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-69-Loss-0.6949500013049544.pdopt
INFO:local_logger:Now training epoch 70. LR=0.000151
INFO:master_logger:Now training epoch 70. LR=0.000151
INFO:local_logger:Epoch[070/800], Step[0000/0626], Avg Loss: 0.6925
INFO:local_logger:Epoch[070/800], Step[0000/0626], Avg Loss: 0.6983
INFO:master_logger:Epoch[070/800], Step[0000/0626], Avg Loss: 0.6937
INFO:local_logger:Epoch[070/800], Step[0000/0626], Avg Loss: 0.6927
INFO:local_logger:Epoch[070/800], Step[0000/0626], Avg Loss: 0.7030
INFO:local_logger:Epoch[070/800], Step[0000/0626], Avg Loss: 0.7050
INFO:local_logger:Epoch[070/800], Step[0000/0626], Avg Loss: 0.6782
INFO:local_logger:Epoch[070/800], Step[0000/0626], Avg Loss: 0.6987
INFO:local_logger:Epoch[070/800], Step[0000/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[070/800], Step[0100/0626], Avg Loss: 0.6955
INFO:local_logger:Epoch[070/800], Step[0100/0626], Avg Loss: 0.6950
INFO:local_logger:Epoch[070/800], Step[0100/0626], Avg Loss: 0.6950
INFO:local_logger:Epoch[070/800], Step[0100/0626], Avg Loss: 0.6946
INFO:master_logger:Epoch[070/800], Step[0100/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[070/800], Step[0100/0626], Avg Loss: 0.6943
INFO:local_logger:Epoch[070/800], Step[0100/0626], Avg Loss: 0.6958
INFO:local_logger:Epoch[070/800], Step[0100/0626], Avg Loss: 0.6940
INFO:local_logger:Epoch[070/800], Step[0100/0626], Avg Loss: 0.6951
INFO:local_logger:Epoch[070/800], Step[0200/0626], Avg Loss: 0.6956
INFO:local_logger:Epoch[070/800], Step[0200/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[070/800], Step[0200/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[070/800], Step[0200/0626], Avg Loss: 0.6952
INFO:local_logger:Epoch[070/800], Step[0200/0626], Avg Loss: 0.6948
INFO:local_logger:Epoch[070/800], Step[0200/0626], Avg Loss: 0.6944
INFO:master_logger:Epoch[070/800], Step[0200/0626], Avg Loss: 0.6950
INFO:local_logger:Epoch[070/800], Step[0200/0626], Avg Loss: 0.6951
INFO:local_logger:Epoch[070/800], Step[0200/0626], Avg Loss: 0.6958
INFO:local_logger:Epoch[070/800], Step[0300/0626], Avg Loss: 0.6945
INFO:local_logger:Epoch[070/800], Step[0300/0626], Avg Loss: 0.6952
INFO:local_logger:Epoch[070/800], Step[0300/0626], Avg Loss: 0.6946
INFO:master_logger:Epoch[070/800], Step[0300/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[070/800], Step[0300/0626], Avg Loss: 0.6959
INFO:local_logger:Epoch[070/800], Step[0300/0626], Avg Loss: 0.6953
INFO:local_logger:Epoch[070/800], Step[0300/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[070/800], Step[0300/0626], Avg Loss: 0.6944
INFO:local_logger:Epoch[070/800], Step[0300/0626], Avg Loss: 0.6944
INFO:local_logger:Epoch[070/800], Step[0400/0626], Avg Loss: 0.6945
INFO:local_logger:Epoch[070/800], Step[0400/0626], Avg Loss: 0.6946
INFO:local_logger:Epoch[070/800], Step[0400/0626], Avg Loss: 0.6944
INFO:local_logger:Epoch[070/800], Step[0400/0626], Avg Loss: 0.6954
INFO:local_logger:Epoch[070/800], Step[0400/0626], Avg Loss: 0.6943
INFO:local_logger:Epoch[070/800], Step[0400/0626], Avg Loss: 0.6951
INFO:local_logger:Epoch[070/800], Step[0400/0626], Avg Loss: 0.6942
INFO:local_logger:Epoch[070/800], Step[0400/0626], Avg Loss: 0.6949
INFO:master_logger:Epoch[070/800], Step[0400/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[070/800], Step[0500/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[070/800], Step[0500/0626], Avg Loss: 0.6942
INFO:local_logger:Epoch[070/800], Step[0500/0626], Avg Loss: 0.6944
INFO:local_logger:Epoch[070/800], Step[0500/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[070/800], Step[0500/0626], Avg Loss: 0.6954
INFO:local_logger:Epoch[070/800], Step[0500/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[070/800], Step[0500/0626], Avg Loss: 0.6941
INFO:master_logger:Epoch[070/800], Step[0500/0626], Avg Loss: 0.6946
INFO:local_logger:Epoch[070/800], Step[0500/0626], Avg Loss: 0.6944
INFO:local_logger:Epoch[070/800], Step[0600/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[070/800], Step[0600/0626], Avg Loss: 0.6948
INFO:local_logger:Epoch[070/800], Step[0600/0626], Avg Loss: 0.6942
INFO:local_logger:Epoch[070/800], Step[0600/0626], Avg Loss: 0.6944
INFO:local_logger:Epoch[070/800], Step[0600/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[070/800], Step[0600/0626], Avg Loss: 0.6941
INFO:local_logger:Epoch[070/800], Step[0600/0626], Avg Loss: 0.6950
INFO:local_logger:Epoch[070/800], Step[0600/0626], Avg Loss: 0.6941
INFO:master_logger:Epoch[070/800], Step[0600/0626], Avg Loss: 0.6945
INFO:local_logger:----- Epoch[070/800], Train Loss: 0.6942, time: 864.91
INFO:local_logger:Now training epoch 71. LR=0.000151
INFO:local_logger:----- Epoch[070/800], Train Loss: 0.6942, time: 865.61
INFO:local_logger:Now training epoch 71. LR=0.000151
INFO:local_logger:----- Epoch[070/800], Train Loss: 0.6941, time: 865.52
INFO:local_logger:Now training epoch 71. LR=0.000151
INFO:local_logger:----- Epoch[070/800], Train Loss: 0.6949, time: 864.93
INFO:local_logger:Now training epoch 71. LR=0.000151
INFO:local_logger:----- Epoch[070/800], Train Loss: 0.6946, time: 861.12
INFO:master_logger:----- Epoch[070/800], Train Loss: 0.6945, time: 861.12
INFO:local_logger:----- Epoch[070/800], Train Loss: 0.6945, time: 864.92
INFO:local_logger:Now training epoch 71. LR=0.000151
INFO:local_logger:----- Epoch[070/800], Train Loss: 0.6947, time: 864.88
INFO:local_logger:Now training epoch 71. LR=0.000151
INFO:local_logger:----- Epoch[070/800], Train Loss: 0.6948, time: 864.89
INFO:local_logger:Now training epoch 71. LR=0.000151
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-70-Loss-0.6946037372341894.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-70-Loss-0.6946037372341894.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-70-Loss-0.6946037372341894.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-70-Loss-0.6946037372341894.pdopt
INFO:local_logger:Now training epoch 71. LR=0.000151
INFO:master_logger:Now training epoch 71. LR=0.000151
INFO:local_logger:Epoch[071/800], Step[0000/0626], Avg Loss: 0.6850
INFO:master_logger:Epoch[071/800], Step[0000/0626], Avg Loss: 0.6913
INFO:local_logger:Epoch[071/800], Step[0000/0626], Avg Loss: 0.6913
INFO:local_logger:Epoch[071/800], Step[0000/0626], Avg Loss: 0.6757
INFO:local_logger:Epoch[071/800], Step[0000/0626], Avg Loss: 0.6944
INFO:local_logger:Epoch[071/800], Step[0000/0626], Avg Loss: 0.7043
INFO:local_logger:Epoch[071/800], Step[0000/0626], Avg Loss: 0.7002
INFO:local_logger:Epoch[071/800], Step[0000/0626], Avg Loss: 0.6977
INFO:local_logger:Epoch[071/800], Step[0000/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[071/800], Step[0100/0626], Avg Loss: 0.6936
INFO:local_logger:Epoch[071/800], Step[0100/0626], Avg Loss: 0.6939
INFO:local_logger:Epoch[071/800], Step[0100/0626], Avg Loss: 0.6950
INFO:local_logger:Epoch[071/800], Step[0100/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[071/800], Step[0100/0626], Avg Loss: 0.6932
INFO:local_logger:Epoch[071/800], Step[0100/0626], Avg Loss: 0.6924
INFO:local_logger:Epoch[071/800], Step[0100/0626], Avg Loss: 0.6937
INFO:master_logger:Epoch[071/800], Step[0100/0626], Avg Loss: 0.6939
INFO:local_logger:Epoch[071/800], Step[0100/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[071/800], Step[0200/0626], Avg Loss: 0.6943
INFO:local_logger:Epoch[071/800], Step[0200/0626], Avg Loss: 0.6941
INFO:local_logger:Epoch[071/800], Step[0200/0626], Avg Loss: 0.6942
INFO:local_logger:Epoch[071/800], Step[0200/0626], Avg Loss: 0.6935
INFO:local_logger:Epoch[071/800], Step[0200/0626], Avg Loss: 0.6942
INFO:local_logger:Epoch[071/800], Step[0200/0626], Avg Loss: 0.6931
INFO:local_logger:Epoch[071/800], Step[0200/0626], Avg Loss: 0.6936
INFO:master_logger:Epoch[071/800], Step[0200/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[071/800], Step[0200/0626], Avg Loss: 0.6930
INFO:local_logger:Epoch[071/800], Step[0300/0626], Avg Loss: 0.6929
INFO:local_logger:Epoch[071/800], Step[0300/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[071/800], Step[0300/0626], Avg Loss: 0.6945
INFO:local_logger:Epoch[071/800], Step[0300/0626], Avg Loss: 0.6937
INFO:local_logger:Epoch[071/800], Step[0300/0626], Avg Loss: 0.6939
INFO:local_logger:Epoch[071/800], Step[0300/0626], Avg Loss: 0.6944
INFO:master_logger:Epoch[071/800], Step[0300/0626], Avg Loss: 0.6937
INFO:local_logger:Epoch[071/800], Step[0300/0626], Avg Loss: 0.6928
INFO:local_logger:Epoch[071/800], Step[0300/0626], Avg Loss: 0.6939
INFO:local_logger:Epoch[071/800], Step[0400/0626], Avg Loss: 0.6946
INFO:local_logger:Epoch[071/800], Step[0400/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[071/800], Step[0400/0626], Avg Loss: 0.6931
INFO:local_logger:Epoch[071/800], Step[0400/0626], Avg Loss: 0.6944
INFO:local_logger:Epoch[071/800], Step[0400/0626], Avg Loss: 0.6940
INFO:local_logger:Epoch[071/800], Step[0400/0626], Avg Loss: 0.6931
INFO:local_logger:Epoch[071/800], Step[0400/0626], Avg Loss: 0.6940
INFO:master_logger:Epoch[071/800], Step[0400/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[071/800], Step[0400/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[071/800], Step[0500/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[071/800], Step[0500/0626], Avg Loss: 0.6934
INFO:local_logger:Epoch[071/800], Step[0500/0626], Avg Loss: 0.6945
INFO:local_logger:Epoch[071/800], Step[0500/0626], Avg Loss: 0.6939
INFO:local_logger:Epoch[071/800], Step[0500/0626], Avg Loss: 0.6937
INFO:local_logger:Epoch[071/800], Step[0500/0626], Avg Loss: 0.6941
INFO:local_logger:Epoch[071/800], Step[0500/0626], Avg Loss: 0.6944
INFO:master_logger:Epoch[071/800], Step[0500/0626], Avg Loss: 0.6939
INFO:local_logger:Epoch[071/800], Step[0500/0626], Avg Loss: 0.6932
INFO:local_logger:Epoch[071/800], Step[0600/0626], Avg Loss: 0.6939
INFO:local_logger:Epoch[071/800], Step[0600/0626], Avg Loss: 0.6933
INFO:local_logger:Epoch[071/800], Step[0600/0626], Avg Loss: 0.6944
INFO:local_logger:Epoch[071/800], Step[0600/0626], Avg Loss: 0.6941
INFO:local_logger:Epoch[071/800], Step[0600/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[071/800], Step[0600/0626], Avg Loss: 0.6937
INFO:master_logger:Epoch[071/800], Step[0600/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[071/800], Step[0600/0626], Avg Loss: 0.6934
INFO:local_logger:Epoch[071/800], Step[0600/0626], Avg Loss: 0.6943
INFO:local_logger:----- Epoch[071/800], Train Loss: 0.6943, time: 885.74
INFO:local_logger:Now training epoch 72. LR=0.000152
INFO:local_logger:----- Epoch[071/800], Train Loss: 0.6939, time: 886.70
INFO:local_logger:Now training epoch 72. LR=0.000152
INFO:local_logger:----- Epoch[071/800], Train Loss: 0.6940, time: 886.74
INFO:local_logger:----- Epoch[071/800], Train Loss: 0.6935, time: 886.79
INFO:local_logger:Now training epoch 72. LR=0.000152
INFO:local_logger:Now training epoch 72. LR=0.000152
INFO:local_logger:----- Epoch[071/800], Train Loss: 0.6938, time: 886.70
INFO:local_logger:Now training epoch 72. LR=0.000152
INFO:local_logger:----- Epoch[071/800], Train Loss: 0.6934, time: 886.76
INFO:local_logger:Now training epoch 72. LR=0.000152
INFO:local_logger:----- Epoch[071/800], Train Loss: 0.6937, time: 882.95
INFO:master_logger:----- Epoch[071/800], Train Loss: 0.6939, time: 882.95
INFO:local_logger:----- Epoch[071/800], Train Loss: 0.6945, time: 886.78
INFO:local_logger:Now training epoch 72. LR=0.000152
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-71-Loss-0.6937192819392223.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-71-Loss-0.6937192819392223.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-71-Loss-0.6937192819392223.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-71-Loss-0.6937192819392223.pdopt
INFO:local_logger:Now training epoch 72. LR=0.000152
INFO:master_logger:Now training epoch 72. LR=0.000152
INFO:local_logger:Epoch[072/800], Step[0000/0626], Avg Loss: 0.6943
INFO:master_logger:Epoch[072/800], Step[0000/0626], Avg Loss: 0.6974
INFO:local_logger:Epoch[072/800], Step[0000/0626], Avg Loss: 0.6913
INFO:local_logger:Epoch[072/800], Step[0000/0626], Avg Loss: 0.7013
INFO:local_logger:Epoch[072/800], Step[0000/0626], Avg Loss: 0.6924
INFO:local_logger:Epoch[072/800], Step[0000/0626], Avg Loss: 0.7045
INFO:local_logger:Epoch[072/800], Step[0000/0626], Avg Loss: 0.6941
INFO:local_logger:Epoch[072/800], Step[0000/0626], Avg Loss: 0.6954
INFO:local_logger:Epoch[072/800], Step[0000/0626], Avg Loss: 0.7056
INFO:local_logger:Epoch[072/800], Step[0100/0626], Avg Loss: 0.6924
INFO:local_logger:Epoch[072/800], Step[0100/0626], Avg Loss: 0.6941
INFO:local_logger:Epoch[072/800], Step[0100/0626], Avg Loss: 0.6941
INFO:local_logger:Epoch[072/800], Step[0100/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[072/800], Step[0100/0626], Avg Loss: 0.6934
INFO:local_logger:Epoch[072/800], Step[0100/0626], Avg Loss: 0.6944
INFO:master_logger:Epoch[072/800], Step[0100/0626], Avg Loss: 0.6939
INFO:local_logger:Epoch[072/800], Step[0100/0626], Avg Loss: 0.6948
INFO:local_logger:Epoch[072/800], Step[0100/0626], Avg Loss: 0.6936
INFO:local_logger:Epoch[072/800], Step[0200/0626], Avg Loss: 0.6930
INFO:local_logger:Epoch[072/800], Step[0200/0626], Avg Loss: 0.6933
INFO:local_logger:Epoch[072/800], Step[0200/0626], Avg Loss: 0.6936
INFO:local_logger:Epoch[072/800], Step[0200/0626], Avg Loss: 0.6929
INFO:local_logger:Epoch[072/800], Step[0200/0626], Avg Loss: 0.6944
INFO:local_logger:Epoch[072/800], Step[0200/0626], Avg Loss: 0.6937
INFO:master_logger:Epoch[072/800], Step[0200/0626], Avg Loss: 0.6935
INFO:local_logger:Epoch[072/800], Step[0200/0626], Avg Loss: 0.6929
INFO:local_logger:Epoch[072/800], Step[0200/0626], Avg Loss: 0.6941
INFO:local_logger:Epoch[072/800], Step[0300/0626], Avg Loss: 0.6932
INFO:local_logger:Epoch[072/800], Step[0300/0626], Avg Loss: 0.6936
INFO:local_logger:Epoch[072/800], Step[0300/0626], Avg Loss: 0.6939
INFO:local_logger:Epoch[072/800], Step[0300/0626], Avg Loss: 0.6930
INFO:local_logger:Epoch[072/800], Step[0300/0626], Avg Loss: 0.6935
INFO:master_logger:Epoch[072/800], Step[0300/0626], Avg Loss: 0.6934
INFO:local_logger:Epoch[072/800], Step[0300/0626], Avg Loss: 0.6942
INFO:local_logger:Epoch[072/800], Step[0300/0626], Avg Loss: 0.6931
INFO:local_logger:Epoch[072/800], Step[0300/0626], Avg Loss: 0.6928
INFO:local_logger:Epoch[072/800], Step[0400/0626], Avg Loss: 0.6931
INFO:local_logger:Epoch[072/800], Step[0400/0626], Avg Loss: 0.6927
INFO:local_logger:Epoch[072/800], Step[0400/0626], Avg Loss: 0.6934
INFO:local_logger:Epoch[072/800], Step[0400/0626], Avg Loss: 0.6939
INFO:local_logger:Epoch[072/800], Step[0400/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[072/800], Step[0400/0626], Avg Loss: 0.6936
INFO:local_logger:Epoch[072/800], Step[0400/0626], Avg Loss: 0.6930
INFO:master_logger:Epoch[072/800], Step[0400/0626], Avg Loss: 0.6933
INFO:local_logger:Epoch[072/800], Step[0400/0626], Avg Loss: 0.6926
INFO:local_logger:Epoch[072/800], Step[0500/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[072/800], Step[0500/0626], Avg Loss: 0.6934
INFO:local_logger:Epoch[072/800], Step[0500/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[072/800], Step[0500/0626], Avg Loss: 0.6937
INFO:master_logger:Epoch[072/800], Step[0500/0626], Avg Loss: 0.6933
INFO:local_logger:Epoch[072/800], Step[0500/0626], Avg Loss: 0.6928
INFO:local_logger:Epoch[072/800], Step[0500/0626], Avg Loss: 0.6930
INFO:local_logger:Epoch[072/800], Step[0500/0626], Avg Loss: 0.6929
INFO:local_logger:Epoch[072/800], Step[0500/0626], Avg Loss: 0.6929
INFO:local_logger:Epoch[072/800], Step[0600/0626], Avg Loss: 0.6936
INFO:local_logger:Epoch[072/800], Step[0600/0626], Avg Loss: 0.6934
INFO:local_logger:Epoch[072/800], Step[0600/0626], Avg Loss: 0.6929
INFO:local_logger:Epoch[072/800], Step[0600/0626], Avg Loss: 0.6930
INFO:local_logger:Epoch[072/800], Step[0600/0626], Avg Loss: 0.6930
INFO:local_logger:Epoch[072/800], Step[0600/0626], Avg Loss: 0.6937
INFO:master_logger:Epoch[072/800], Step[0600/0626], Avg Loss: 0.6933
INFO:local_logger:Epoch[072/800], Step[0600/0626], Avg Loss: 0.6929
INFO:local_logger:Epoch[072/800], Step[0600/0626], Avg Loss: 0.6937
INFO:local_logger:----- Epoch[072/800], Train Loss: 0.6928, time: 870.67
INFO:local_logger:Now training epoch 73. LR=0.000152
INFO:local_logger:----- Epoch[072/800], Train Loss: 0.6930, time: 869.72
INFO:local_logger:Now training epoch 73. LR=0.000152
INFO:local_logger:----- Epoch[072/800], Train Loss: 0.6936, time: 869.72
INFO:local_logger:Now training epoch 73. LR=0.000152
INFO:local_logger:----- Epoch[072/800], Train Loss: 0.6930, time: 869.77
INFO:local_logger:Now training epoch 73. LR=0.000152
INFO:local_logger:----- Epoch[072/800], Train Loss: 0.6934, time: 870.14
INFO:local_logger:Now training epoch 73. LR=0.000152
INFO:local_logger:----- Epoch[072/800], Train Loss: 0.6936, time: 870.14
INFO:local_logger:Now training epoch 73. LR=0.000152
INFO:local_logger:----- Epoch[072/800], Train Loss: 0.6936, time: 866.43
INFO:local_logger:----- Epoch[072/800], Train Loss: 0.6929, time: 870.14
INFO:master_logger:----- Epoch[072/800], Train Loss: 0.6932, time: 866.43
INFO:local_logger:Now training epoch 73. LR=0.000152
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-72-Loss-0.693554089917601.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-72-Loss-0.693554089917601.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-72-Loss-0.693554089917601.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-72-Loss-0.693554089917601.pdopt
INFO:local_logger:Now training epoch 73. LR=0.000152
INFO:master_logger:Now training epoch 73. LR=0.000152
INFO:local_logger:Epoch[073/800], Step[0000/0626], Avg Loss: 0.6893
INFO:local_logger:Epoch[073/800], Step[0000/0626], Avg Loss: 0.6943
INFO:master_logger:Epoch[073/800], Step[0000/0626], Avg Loss: 0.6940
INFO:local_logger:Epoch[073/800], Step[0000/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[073/800], Step[0000/0626], Avg Loss: 0.6941
INFO:local_logger:Epoch[073/800], Step[0000/0626], Avg Loss: 0.6980
INFO:local_logger:Epoch[073/800], Step[0000/0626], Avg Loss: 0.6944
INFO:local_logger:Epoch[073/800], Step[0000/0626], Avg Loss: 0.6816
INFO:local_logger:Epoch[073/800], Step[0000/0626], Avg Loss: 0.7038
INFO:local_logger:Epoch[073/800], Step[0100/0626], Avg Loss: 0.6919
INFO:local_logger:Epoch[073/800], Step[0100/0626], Avg Loss: 0.6915
INFO:local_logger:Epoch[073/800], Step[0100/0626], Avg Loss: 0.6929
INFO:local_logger:Epoch[073/800], Step[0100/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[073/800], Step[0100/0626], Avg Loss: 0.6935
INFO:local_logger:Epoch[073/800], Step[0100/0626], Avg Loss: 0.6940
INFO:master_logger:Epoch[073/800], Step[0100/0626], Avg Loss: 0.6928
INFO:local_logger:Epoch[073/800], Step[0100/0626], Avg Loss: 0.6929
INFO:local_logger:Epoch[073/800], Step[0100/0626], Avg Loss: 0.6922
INFO:local_logger:Epoch[073/800], Step[0200/0626], Avg Loss: 0.6924
INFO:local_logger:Epoch[073/800], Step[0200/0626], Avg Loss: 0.6928
INFO:local_logger:Epoch[073/800], Step[0200/0626], Avg Loss: 0.6930
INFO:local_logger:Epoch[073/800], Step[0200/0626], Avg Loss: 0.6925
INFO:master_logger:Epoch[073/800], Step[0200/0626], Avg Loss: 0.6927
INFO:local_logger:Epoch[073/800], Step[0200/0626], Avg Loss: 0.6925
INFO:local_logger:Epoch[073/800], Step[0200/0626], Avg Loss: 0.6916
INFO:local_logger:Epoch[073/800], Step[0200/0626], Avg Loss: 0.6939
INFO:local_logger:Epoch[073/800], Step[0200/0626], Avg Loss: 0.6934
INFO:local_logger:Epoch[073/800], Step[0300/0626], Avg Loss: 0.6928
INFO:local_logger:Epoch[073/800], Step[0300/0626], Avg Loss: 0.6932
INFO:local_logger:Epoch[073/800], Step[0300/0626], Avg Loss: 0.6930
INFO:local_logger:Epoch[073/800], Step[0300/0626], Avg Loss: 0.6922
INFO:local_logger:Epoch[073/800], Step[0300/0626], Avg Loss: 0.6937
INFO:master_logger:Epoch[073/800], Step[0300/0626], Avg Loss: 0.6928
INFO:local_logger:Epoch[073/800], Step[0300/0626], Avg Loss: 0.6919
INFO:local_logger:Epoch[073/800], Step[0300/0626], Avg Loss: 0.6928
INFO:local_logger:Epoch[073/800], Step[0300/0626], Avg Loss: 0.6926
INFO:local_logger:Epoch[073/800], Step[0400/0626], Avg Loss: 0.6923
INFO:local_logger:Epoch[073/800], Step[0400/0626], Avg Loss: 0.6929
INFO:local_logger:Epoch[073/800], Step[0400/0626], Avg Loss: 0.6917
INFO:local_logger:Epoch[073/800], Step[0400/0626], Avg Loss: 0.6933
INFO:local_logger:Epoch[073/800], Step[0400/0626], Avg Loss: 0.6930
INFO:local_logger:Epoch[073/800], Step[0400/0626], Avg Loss: 0.6928
INFO:local_logger:Epoch[073/800], Step[0400/0626], Avg Loss: 0.6933
INFO:master_logger:Epoch[073/800], Step[0400/0626], Avg Loss: 0.6927
INFO:local_logger:Epoch[073/800], Step[0400/0626], Avg Loss: 0.6926
INFO:local_logger:Epoch[073/800], Step[0500/0626], Avg Loss: 0.6920
INFO:local_logger:Epoch[073/800], Step[0500/0626], Avg Loss: 0.6916
INFO:local_logger:Epoch[073/800], Step[0500/0626], Avg Loss: 0.6923
INFO:local_logger:Epoch[073/800], Step[0500/0626], Avg Loss: 0.6931
INFO:local_logger:Epoch[073/800], Step[0500/0626], Avg Loss: 0.6928
INFO:local_logger:Epoch[073/800], Step[0500/0626], Avg Loss: 0.6927
INFO:local_logger:Epoch[073/800], Step[0500/0626], Avg Loss: 0.6929
INFO:master_logger:Epoch[073/800], Step[0500/0626], Avg Loss: 0.6926
INFO:local_logger:Epoch[073/800], Step[0500/0626], Avg Loss: 0.6933
INFO:local_logger:Epoch[073/800], Step[0600/0626], Avg Loss: 0.6921
INFO:local_logger:Epoch[073/800], Step[0600/0626], Avg Loss: 0.6927
INFO:local_logger:Epoch[073/800], Step[0600/0626], Avg Loss: 0.6927
INFO:local_logger:Epoch[073/800], Step[0600/0626], Avg Loss: 0.6928
INFO:master_logger:Epoch[073/800], Step[0600/0626], Avg Loss: 0.6925
INFO:local_logger:Epoch[073/800], Step[0600/0626], Avg Loss: 0.6921
INFO:local_logger:Epoch[073/800], Step[0600/0626], Avg Loss: 0.6918
INFO:local_logger:Epoch[073/800], Step[0600/0626], Avg Loss: 0.6930
INFO:local_logger:Epoch[073/800], Step[0600/0626], Avg Loss: 0.6929
INFO:local_logger:----- Epoch[073/800], Train Loss: 0.6926, time: 884.48
INFO:local_logger:Now training epoch 74. LR=0.000152
INFO:local_logger:----- Epoch[073/800], Train Loss: 0.6921, time: 884.48
INFO:local_logger:Now training epoch 74. LR=0.000152
INFO:local_logger:----- Epoch[073/800], Train Loss: 0.6928, time: 884.08
INFO:local_logger:Now training epoch 74. LR=0.000152
INFO:local_logger:----- Epoch[073/800], Train Loss: 0.6926, time: 880.49
INFO:master_logger:----- Epoch[073/800], Train Loss: 0.6925, time: 880.49
INFO:local_logger:----- Epoch[073/800], Train Loss: 0.6917, time: 884.29
INFO:local_logger:Now training epoch 74. LR=0.000152
INFO:local_logger:----- Epoch[073/800], Train Loss: 0.6920, time: 884.32
INFO:local_logger:Now training epoch 74. LR=0.000152
INFO:local_logger:----- Epoch[073/800], Train Loss: 0.6929, time: 884.79
INFO:local_logger:Now training epoch 74. LR=0.000152
INFO:local_logger:----- Epoch[073/800], Train Loss: 0.6930, time: 884.73
INFO:local_logger:Now training epoch 74. LR=0.000152
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-73-Loss-0.6925669066549239.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-73-Loss-0.6925669066549239.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-73-Loss-0.6925669066549239.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-73-Loss-0.6925669066549239.pdopt
INFO:local_logger:Now training epoch 74. LR=0.000152
INFO:master_logger:Now training epoch 74. LR=0.000152
INFO:local_logger:Epoch[074/800], Step[0000/0626], Avg Loss: 0.6904
INFO:local_logger:Epoch[074/800], Step[0000/0626], Avg Loss: 0.6894
INFO:master_logger:Epoch[074/800], Step[0000/0626], Avg Loss: 0.6933
INFO:local_logger:Epoch[074/800], Step[0000/0626], Avg Loss: 0.7013
INFO:local_logger:Epoch[074/800], Step[0000/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[074/800], Step[0000/0626], Avg Loss: 0.6899
INFO:local_logger:Epoch[074/800], Step[0000/0626], Avg Loss: 0.6870
INFO:local_logger:Epoch[074/800], Step[0000/0626], Avg Loss: 0.6982
INFO:local_logger:Epoch[074/800], Step[0000/0626], Avg Loss: 0.7004
INFO:local_logger:Epoch[074/800], Step[0100/0626], Avg Loss: 0.6923
INFO:local_logger:Epoch[074/800], Step[0100/0626], Avg Loss: 0.6921
INFO:local_logger:Epoch[074/800], Step[0100/0626], Avg Loss: 0.6912
INFO:local_logger:Epoch[074/800], Step[0100/0626], Avg Loss: 0.6908
INFO:local_logger:Epoch[074/800], Step[0100/0626], Avg Loss: 0.6927
INFO:master_logger:Epoch[074/800], Step[0100/0626], Avg Loss: 0.6920
INFO:local_logger:Epoch[074/800], Step[0100/0626], Avg Loss: 0.6914
INFO:local_logger:Epoch[074/800], Step[0100/0626], Avg Loss: 0.6919
INFO:local_logger:Epoch[074/800], Step[0100/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[074/800], Step[0200/0626], Avg Loss: 0.6919
INFO:local_logger:Epoch[074/800], Step[0200/0626], Avg Loss: 0.6924
INFO:local_logger:Epoch[074/800], Step[0200/0626], Avg Loss: 0.6925
INFO:local_logger:Epoch[074/800], Step[0200/0626], Avg Loss: 0.6917
INFO:local_logger:Epoch[074/800], Step[0200/0626], Avg Loss: 0.6913
INFO:local_logger:Epoch[074/800], Step[0200/0626], Avg Loss: 0.6916
INFO:master_logger:Epoch[074/800], Step[0200/0626], Avg Loss: 0.6919
INFO:local_logger:Epoch[074/800], Step[0200/0626], Avg Loss: 0.6908
INFO:local_logger:Epoch[074/800], Step[0200/0626], Avg Loss: 0.6927
INFO:local_logger:Epoch[074/800], Step[0300/0626], Avg Loss: 0.6916
INFO:local_logger:Epoch[074/800], Step[0300/0626], Avg Loss: 0.6912
INFO:local_logger:Epoch[074/800], Step[0300/0626], Avg Loss: 0.6920
INFO:local_logger:Epoch[074/800], Step[0300/0626], Avg Loss: 0.6929
INFO:master_logger:Epoch[074/800], Step[0300/0626], Avg Loss: 0.6920
INFO:local_logger:Epoch[074/800], Step[0300/0626], Avg Loss: 0.6923
INFO:local_logger:Epoch[074/800], Step[0300/0626], Avg Loss: 0.6919
INFO:local_logger:Epoch[074/800], Step[0300/0626], Avg Loss: 0.6924
INFO:local_logger:Epoch[074/800], Step[0300/0626], Avg Loss: 0.6921
INFO:local_logger:Epoch[074/800], Step[0400/0626], Avg Loss: 0.6919
INFO:local_logger:Epoch[074/800], Step[0400/0626], Avg Loss: 0.6925
INFO:local_logger:Epoch[074/800], Step[0400/0626], Avg Loss: 0.6921
INFO:local_logger:Epoch[074/800], Step[0400/0626], Avg Loss: 0.6918
INFO:local_logger:Epoch[074/800], Step[0400/0626], Avg Loss: 0.6913
INFO:local_logger:Epoch[074/800], Step[0400/0626], Avg Loss: 0.6926
INFO:local_logger:Epoch[074/800], Step[0400/0626], Avg Loss: 0.6920
INFO:master_logger:Epoch[074/800], Step[0400/0626], Avg Loss: 0.6920
INFO:local_logger:Epoch[074/800], Step[0400/0626], Avg Loss: 0.6918
INFO:local_logger:Epoch[074/800], Step[0500/0626], Avg Loss: 0.6919
INFO:local_logger:Epoch[074/800], Step[0500/0626], Avg Loss: 0.6919
INFO:local_logger:Epoch[074/800], Step[0500/0626], Avg Loss: 0.6922
INFO:local_logger:Epoch[074/800], Step[0500/0626], Avg Loss: 0.6923
INFO:local_logger:Epoch[074/800], Step[0500/0626], Avg Loss: 0.6922
INFO:local_logger:Epoch[074/800], Step[0500/0626], Avg Loss: 0.6918
INFO:local_logger:Epoch[074/800], Step[0500/0626], Avg Loss: 0.6920
INFO:master_logger:Epoch[074/800], Step[0500/0626], Avg Loss: 0.6920
INFO:local_logger:Epoch[074/800], Step[0500/0626], Avg Loss: 0.6915
INFO:local_logger:Epoch[074/800], Step[0600/0626], Avg Loss: 0.6923
INFO:local_logger:Epoch[074/800], Step[0600/0626], Avg Loss: 0.6919
INFO:local_logger:Epoch[074/800], Step[0600/0626], Avg Loss: 0.6918
INFO:local_logger:Epoch[074/800], Step[0600/0626], Avg Loss: 0.6917
INFO:local_logger:Epoch[074/800], Step[0600/0626], Avg Loss: 0.6917
INFO:local_logger:Epoch[074/800], Step[0600/0626], Avg Loss: 0.6921
INFO:local_logger:Epoch[074/800], Step[0600/0626], Avg Loss: 0.6922
INFO:master_logger:Epoch[074/800], Step[0600/0626], Avg Loss: 0.6920
INFO:local_logger:Epoch[074/800], Step[0600/0626], Avg Loss: 0.6922
INFO:local_logger:----- Epoch[074/800], Train Loss: 0.6922, time: 868.83
INFO:local_logger:Now training epoch 75. LR=0.000152
INFO:local_logger:----- Epoch[074/800], Train Loss: 0.6922, time: 869.13
INFO:local_logger:Now training epoch 75. LR=0.000152
INFO:local_logger:----- Epoch[074/800], Train Loss: 0.6916, time: 868.88
INFO:local_logger:Now training epoch 75. LR=0.000152
INFO:local_logger:----- Epoch[074/800], Train Loss: 0.6920, time: 868.87
INFO:local_logger:Now training epoch 75. LR=0.000152
INFO:local_logger:----- Epoch[074/800], Train Loss: 0.6917, time: 869.16
INFO:local_logger:Now training epoch 75. LR=0.000152
INFO:local_logger:----- Epoch[074/800], Train Loss: 0.6917, time: 868.97
INFO:local_logger:Now training epoch 75. LR=0.000152
INFO:local_logger:----- Epoch[074/800], Train Loss: 0.6918, time: 865.36
INFO:master_logger:----- Epoch[074/800], Train Loss: 0.6919, time: 865.36
INFO:local_logger:----- Epoch[074/800], Train Loss: 0.6922, time: 869.28
INFO:local_logger:Now training epoch 75. LR=0.000152
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-74-Loss-0.6918195025700205.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-74-Loss-0.6918195025700205.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-74-Loss-0.6918195025700205.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-74-Loss-0.6918195025700205.pdopt
INFO:local_logger:Now training epoch 75. LR=0.000152
INFO:master_logger:Now training epoch 75. LR=0.000152
INFO:local_logger:Epoch[075/800], Step[0000/0626], Avg Loss: 0.6855
INFO:master_logger:Epoch[075/800], Step[0000/0626], Avg Loss: 0.6909
INFO:local_logger:Epoch[075/800], Step[0000/0626], Avg Loss: 0.6841
INFO:local_logger:Epoch[075/800], Step[0000/0626], Avg Loss: 0.6902
INFO:local_logger:Epoch[075/800], Step[0000/0626], Avg Loss: 0.6874
INFO:local_logger:Epoch[075/800], Step[0000/0626], Avg Loss: 0.6973
INFO:local_logger:Epoch[075/800], Step[0000/0626], Avg Loss: 0.6886
INFO:local_logger:Epoch[075/800], Step[0000/0626], Avg Loss: 0.6943
INFO:local_logger:Epoch[075/800], Step[0000/0626], Avg Loss: 0.6994
INFO:local_logger:Epoch[075/800], Step[0100/0626], Avg Loss: 0.6908
INFO:local_logger:Epoch[075/800], Step[0100/0626], Avg Loss: 0.6913
INFO:local_logger:Epoch[075/800], Step[0100/0626], Avg Loss: 0.6910
INFO:local_logger:Epoch[075/800], Step[0100/0626], Avg Loss: 0.6919
INFO:local_logger:Epoch[075/800], Step[0100/0626], Avg Loss: 0.6908
INFO:local_logger:Epoch[075/800], Step[0100/0626], Avg Loss: 0.6928
INFO:master_logger:Epoch[075/800], Step[0100/0626], Avg Loss: 0.6914
INFO:local_logger:Epoch[075/800], Step[0100/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[075/800], Step[0100/0626], Avg Loss: 0.6915
INFO:local_logger:Epoch[075/800], Step[0200/0626], Avg Loss: 0.6921
INFO:local_logger:Epoch[075/800], Step[0200/0626], Avg Loss: 0.6912
INFO:local_logger:Epoch[075/800], Step[0200/0626], Avg Loss: 0.6916
INFO:local_logger:Epoch[075/800], Step[0200/0626], Avg Loss: 0.6914
INFO:local_logger:Epoch[075/800], Step[0200/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[075/800], Step[0200/0626], Avg Loss: 0.6921
INFO:local_logger:Epoch[075/800], Step[0200/0626], Avg Loss: 0.6911
INFO:master_logger:Epoch[075/800], Step[0200/0626], Avg Loss: 0.6913
INFO:local_logger:Epoch[075/800], Step[0200/0626], Avg Loss: 0.6904
INFO:local_logger:Epoch[075/800], Step[0300/0626], Avg Loss: 0.6908
INFO:local_logger:Epoch[075/800], Step[0300/0626], Avg Loss: 0.6915
INFO:local_logger:Epoch[075/800], Step[0300/0626], Avg Loss: 0.6911
INFO:local_logger:Epoch[075/800], Step[0300/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[075/800], Step[0300/0626], Avg Loss: 0.6916
INFO:local_logger:Epoch[075/800], Step[0300/0626], Avg Loss: 0.6913
INFO:master_logger:Epoch[075/800], Step[0300/0626], Avg Loss: 0.6912
INFO:local_logger:Epoch[075/800], Step[0300/0626], Avg Loss: 0.6916
INFO:local_logger:Epoch[075/800], Step[0300/0626], Avg Loss: 0.6915
INFO:local_logger:Epoch[075/800], Step[0400/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[075/800], Step[0400/0626], Avg Loss: 0.6910
INFO:local_logger:Epoch[075/800], Step[0400/0626], Avg Loss: 0.6917
INFO:local_logger:Epoch[075/800], Step[0400/0626], Avg Loss: 0.6910
INFO:local_logger:Epoch[075/800], Step[0400/0626], Avg Loss: 0.6919
INFO:master_logger:Epoch[075/800], Step[0400/0626], Avg Loss: 0.6913
INFO:local_logger:Epoch[075/800], Step[0400/0626], Avg Loss: 0.6915
INFO:local_logger:Epoch[075/800], Step[0400/0626], Avg Loss: 0.6914
INFO:local_logger:Epoch[075/800], Step[0400/0626], Avg Loss: 0.6913
INFO:local_logger:Epoch[075/800], Step[0500/0626], Avg Loss: 0.6918
INFO:local_logger:Epoch[075/800], Step[0500/0626], Avg Loss: 0.6911
INFO:local_logger:Epoch[075/800], Step[0500/0626], Avg Loss: 0.6917
INFO:local_logger:Epoch[075/800], Step[0500/0626], Avg Loss: 0.6911
INFO:local_logger:Epoch[075/800], Step[0500/0626], Avg Loss: 0.6911
INFO:local_logger:Epoch[075/800], Step[0500/0626], Avg Loss: 0.6918
INFO:local_logger:Epoch[075/800], Step[0500/0626], Avg Loss: 0.6913
INFO:master_logger:Epoch[075/800], Step[0500/0626], Avg Loss: 0.6913
INFO:local_logger:Epoch[075/800], Step[0500/0626], Avg Loss: 0.6904
INFO:local_logger:Epoch[075/800], Step[0600/0626], Avg Loss: 0.6908
INFO:local_logger:Epoch[075/800], Step[0600/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[075/800], Step[0600/0626], Avg Loss: 0.6916
INFO:local_logger:Epoch[075/800], Step[0600/0626], Avg Loss: 0.6910
INFO:master_logger:Epoch[075/800], Step[0600/0626], Avg Loss: 0.6912
INFO:local_logger:Epoch[075/800], Step[0600/0626], Avg Loss: 0.6915
INFO:local_logger:Epoch[075/800], Step[0600/0626], Avg Loss: 0.6918
INFO:local_logger:Epoch[075/800], Step[0600/0626], Avg Loss: 0.6912
INFO:local_logger:Epoch[075/800], Step[0600/0626], Avg Loss: 0.6913
INFO:local_logger:----- Epoch[075/800], Train Loss: 0.6918, time: 889.31
INFO:local_logger:Now training epoch 76. LR=0.000152
INFO:local_logger:----- Epoch[075/800], Train Loss: 0.6917, time: 889.53
INFO:local_logger:Now training epoch 76. LR=0.000152
INFO:local_logger:----- Epoch[075/800], Train Loss: 0.6907, time: 889.63
INFO:local_logger:Now training epoch 76. LR=0.000152
INFO:local_logger:----- Epoch[075/800], Train Loss: 0.6910, time: 885.80
INFO:master_logger:----- Epoch[075/800], Train Loss: 0.6913, time: 885.80
INFO:local_logger:----- Epoch[075/800], Train Loss: 0.6913, time: 889.63
INFO:local_logger:Now training epoch 76. LR=0.000152
INFO:local_logger:----- Epoch[075/800], Train Loss: 0.6912, time: 890.10
INFO:local_logger:Now training epoch 76. LR=0.000152
INFO:local_logger:----- Epoch[075/800], Train Loss: 0.6916, time: 890.11
INFO:local_logger:Now training epoch 76. LR=0.000152
INFO:local_logger:----- Epoch[075/800], Train Loss: 0.6911, time: 890.09
INFO:local_logger:Now training epoch 76. LR=0.000152
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-75-Loss-0.6910110246189355.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-75-Loss-0.6910110246189355.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-75-Loss-0.6910110246189355.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-75-Loss-0.6910110246189355.pdopt
INFO:local_logger:Now training epoch 76. LR=0.000152
INFO:master_logger:Now training epoch 76. LR=0.000152
INFO:local_logger:Epoch[076/800], Step[0000/0626], Avg Loss: 0.6949
INFO:local_logger:Epoch[076/800], Step[0000/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[076/800], Step[0000/0626], Avg Loss: 0.6967
INFO:master_logger:Epoch[076/800], Step[0000/0626], Avg Loss: 0.6895
INFO:local_logger:Epoch[076/800], Step[0000/0626], Avg Loss: 0.6921
INFO:local_logger:Epoch[076/800], Step[0000/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[076/800], Step[0000/0626], Avg Loss: 0.6918
INFO:local_logger:Epoch[076/800], Step[0000/0626], Avg Loss: 0.6833
INFO:local_logger:Epoch[076/800], Step[0000/0626], Avg Loss: 0.6889
INFO:local_logger:Epoch[076/800], Step[0100/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[076/800], Step[0100/0626], Avg Loss: 0.6908
INFO:local_logger:Epoch[076/800], Step[0100/0626], Avg Loss: 0.6890
INFO:local_logger:Epoch[076/800], Step[0100/0626], Avg Loss: 0.6912
INFO:local_logger:Epoch[076/800], Step[0100/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[076/800], Step[0100/0626], Avg Loss: 0.6912
INFO:master_logger:Epoch[076/800], Step[0100/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[076/800], Step[0100/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[076/800], Step[0100/0626], Avg Loss: 0.6922
INFO:local_logger:Epoch[076/800], Step[0200/0626], Avg Loss: 0.6914
INFO:local_logger:Epoch[076/800], Step[0200/0626], Avg Loss: 0.6904
INFO:local_logger:Epoch[076/800], Step[0200/0626], Avg Loss: 0.6908
INFO:local_logger:Epoch[076/800], Step[0200/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[076/800], Step[0200/0626], Avg Loss: 0.6911
INFO:local_logger:Epoch[076/800], Step[0200/0626], Avg Loss: 0.6909
INFO:local_logger:Epoch[076/800], Step[0200/0626], Avg Loss: 0.6909
INFO:local_logger:Epoch[076/800], Step[0200/0626], Avg Loss: 0.6905
INFO:master_logger:Epoch[076/800], Step[0200/0626], Avg Loss: 0.6909
INFO:local_logger:Epoch[076/800], Step[0300/0626], Avg Loss: 0.6911
INFO:local_logger:Epoch[076/800], Step[0300/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[076/800], Step[0300/0626], Avg Loss: 0.6903
INFO:local_logger:Epoch[076/800], Step[0300/0626], Avg Loss: 0.6903
INFO:local_logger:Epoch[076/800], Step[0300/0626], Avg Loss: 0.6906
INFO:master_logger:Epoch[076/800], Step[0300/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[076/800], Step[0300/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[076/800], Step[0300/0626], Avg Loss: 0.6905
INFO:local_logger:Epoch[076/800], Step[0300/0626], Avg Loss: 0.6910
INFO:local_logger:Epoch[076/800], Step[0400/0626], Avg Loss: 0.6901
INFO:local_logger:Epoch[076/800], Step[0400/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[076/800], Step[0400/0626], Avg Loss: 0.6905
INFO:local_logger:Epoch[076/800], Step[0400/0626], Avg Loss: 0.6902
INFO:local_logger:Epoch[076/800], Step[0400/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[076/800], Step[0400/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[076/800], Step[0400/0626], Avg Loss: 0.6908
INFO:master_logger:Epoch[076/800], Step[0400/0626], Avg Loss: 0.6905
INFO:local_logger:Epoch[076/800], Step[0400/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[076/800], Step[0500/0626], Avg Loss: 0.6908
INFO:local_logger:Epoch[076/800], Step[0500/0626], Avg Loss: 0.6902
INFO:local_logger:Epoch[076/800], Step[0500/0626], Avg Loss: 0.6905
INFO:local_logger:Epoch[076/800], Step[0500/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[076/800], Step[0500/0626], Avg Loss: 0.6908
INFO:local_logger:Epoch[076/800], Step[0500/0626], Avg Loss: 0.6911
INFO:local_logger:Epoch[076/800], Step[0500/0626], Avg Loss: 0.6903
INFO:master_logger:Epoch[076/800], Step[0500/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[076/800], Step[0500/0626], Avg Loss: 0.6910
INFO:local_logger:Epoch[076/800], Step[0600/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[076/800], Step[0600/0626], Avg Loss: 0.6904
INFO:local_logger:Epoch[076/800], Step[0600/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[076/800], Step[0600/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[076/800], Step[0600/0626], Avg Loss: 0.6902
INFO:master_logger:Epoch[076/800], Step[0600/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[076/800], Step[0600/0626], Avg Loss: 0.6912
INFO:local_logger:Epoch[076/800], Step[0600/0626], Avg Loss: 0.6910
INFO:local_logger:Epoch[076/800], Step[0600/0626], Avg Loss: 0.6903
INFO:local_logger:----- Epoch[076/800], Train Loss: 0.6911, time: 859.45
INFO:local_logger:Now training epoch 77. LR=0.000152
INFO:local_logger:----- Epoch[076/800], Train Loss: 0.6906, time: 855.76
INFO:master_logger:----- Epoch[076/800], Train Loss: 0.6906, time: 855.76
INFO:local_logger:----- Epoch[076/800], Train Loss: 0.6910, time: 859.92
INFO:local_logger:Now training epoch 77. LR=0.000152
INFO:local_logger:----- Epoch[076/800], Train Loss: 0.6907, time: 859.58
INFO:local_logger:Now training epoch 77. LR=0.000152
INFO:local_logger:----- Epoch[076/800], Train Loss: 0.6906, time: 859.59
INFO:local_logger:Now training epoch 77. LR=0.000152
INFO:local_logger:----- Epoch[076/800], Train Loss: 0.6903, time: 860.18
INFO:local_logger:Now training epoch 77. LR=0.000152
INFO:local_logger:----- Epoch[076/800], Train Loss: 0.6903, time: 860.24
INFO:local_logger:Now training epoch 77. LR=0.000152
INFO:local_logger:----- Epoch[076/800], Train Loss: 0.6904, time: 859.60
INFO:local_logger:Now training epoch 77. LR=0.000152
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-76-Loss-0.6905609986769955.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-76-Loss-0.6905609986769955.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-76-Loss-0.6905609986769955.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-76-Loss-0.6905609986769955.pdopt
INFO:local_logger:Now training epoch 77. LR=0.000152
INFO:master_logger:Now training epoch 77. LR=0.000152
INFO:local_logger:Epoch[077/800], Step[0000/0626], Avg Loss: 0.6953
INFO:local_logger:Epoch[077/800], Step[0000/0626], Avg Loss: 0.6740
INFO:local_logger:Epoch[077/800], Step[0000/0626], Avg Loss: 0.6827
INFO:master_logger:Epoch[077/800], Step[0000/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[077/800], Step[0000/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[077/800], Step[0000/0626], Avg Loss: 0.7074
INFO:local_logger:Epoch[077/800], Step[0000/0626], Avg Loss: 0.7042
INFO:local_logger:Epoch[077/800], Step[0000/0626], Avg Loss: 0.6947
INFO:local_logger:Epoch[077/800], Step[0000/0626], Avg Loss: 0.6790
INFO:local_logger:Epoch[077/800], Step[0100/0626], Avg Loss: 0.6899
INFO:local_logger:Epoch[077/800], Step[0100/0626], Avg Loss: 0.6911
INFO:local_logger:Epoch[077/800], Step[0100/0626], Avg Loss: 0.6890
INFO:local_logger:Epoch[077/800], Step[0100/0626], Avg Loss: 0.6909
INFO:local_logger:Epoch[077/800], Step[0100/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[077/800], Step[0100/0626], Avg Loss: 0.6900
INFO:master_logger:Epoch[077/800], Step[0100/0626], Avg Loss: 0.6902
INFO:local_logger:Epoch[077/800], Step[0100/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[077/800], Step[0100/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[077/800], Step[0200/0626], Avg Loss: 0.6901
INFO:local_logger:Epoch[077/800], Step[0200/0626], Avg Loss: 0.6903
INFO:local_logger:Epoch[077/800], Step[0200/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[077/800], Step[0200/0626], Avg Loss: 0.6905
INFO:local_logger:Epoch[077/800], Step[0200/0626], Avg Loss: 0.6907
INFO:local_logger:Epoch[077/800], Step[0200/0626], Avg Loss: 0.6893
INFO:local_logger:Epoch[077/800], Step[0200/0626], Avg Loss: 0.6892
INFO:master_logger:Epoch[077/800], Step[0200/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[077/800], Step[0200/0626], Avg Loss: 0.6902
INFO:local_logger:Epoch[077/800], Step[0300/0626], Avg Loss: 0.6902
INFO:local_logger:Epoch[077/800], Step[0300/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[077/800], Step[0300/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[077/800], Step[0300/0626], Avg Loss: 0.6904
INFO:local_logger:Epoch[077/800], Step[0300/0626], Avg Loss: 0.6905
INFO:local_logger:Epoch[077/800], Step[0300/0626], Avg Loss: 0.6893
INFO:local_logger:Epoch[077/800], Step[0300/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[077/800], Step[0300/0626], Avg Loss: 0.6908
INFO:master_logger:Epoch[077/800], Step[0300/0626], Avg Loss: 0.6901
INFO:local_logger:Epoch[077/800], Step[0400/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[077/800], Step[0400/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[077/800], Step[0400/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[077/800], Step[0400/0626], Avg Loss: 0.6903
INFO:local_logger:Epoch[077/800], Step[0400/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[077/800], Step[0400/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[077/800], Step[0400/0626], Avg Loss: 0.6902
INFO:master_logger:Epoch[077/800], Step[0400/0626], Avg Loss: 0.6902
INFO:local_logger:Epoch[077/800], Step[0400/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[077/800], Step[0500/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[077/800], Step[0500/0626], Avg Loss: 0.6905
INFO:local_logger:Epoch[077/800], Step[0500/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[077/800], Step[0500/0626], Avg Loss: 0.6903
INFO:local_logger:Epoch[077/800], Step[0500/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[077/800], Step[0500/0626], Avg Loss: 0.6899
INFO:local_logger:Epoch[077/800], Step[0500/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[077/800], Step[0500/0626], Avg Loss: 0.6896
INFO:master_logger:Epoch[077/800], Step[0500/0626], Avg Loss: 0.6902
INFO:local_logger:Epoch[077/800], Step[0600/0626], Avg Loss: 0.6902
INFO:local_logger:Epoch[077/800], Step[0600/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[077/800], Step[0600/0626], Avg Loss: 0.6906
INFO:local_logger:Epoch[077/800], Step[0600/0626], Avg Loss: 0.6899
INFO:local_logger:Epoch[077/800], Step[0600/0626], Avg Loss: 0.6905
INFO:local_logger:Epoch[077/800], Step[0600/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[077/800], Step[0600/0626], Avg Loss: 0.6898
INFO:master_logger:Epoch[077/800], Step[0600/0626], Avg Loss: 0.6902
INFO:local_logger:Epoch[077/800], Step[0600/0626], Avg Loss: 0.6904
INFO:local_logger:----- Epoch[077/800], Train Loss: 0.6899, time: 882.70
INFO:local_logger:Now training epoch 78. LR=0.000152
INFO:local_logger:----- Epoch[077/800], Train Loss: 0.6898, time: 884.37
INFO:local_logger:Now training epoch 78. LR=0.000152
INFO:local_logger:----- Epoch[077/800], Train Loss: 0.6901, time: 883.82
INFO:local_logger:Now training epoch 78. LR=0.000152
INFO:local_logger:----- Epoch[077/800], Train Loss: 0.6906, time: 883.80
INFO:local_logger:----- Epoch[077/800], Train Loss: 0.6902, time: 883.80
INFO:local_logger:Now training epoch 78. LR=0.000152
INFO:local_logger:Now training epoch 78. LR=0.000152
INFO:local_logger:----- Epoch[077/800], Train Loss: 0.6905, time: 883.93
INFO:local_logger:Now training epoch 78. LR=0.000152
INFO:local_logger:----- Epoch[077/800], Train Loss: 0.6898, time: 880.45
INFO:master_logger:----- Epoch[077/800], Train Loss: 0.6902, time: 880.45
INFO:local_logger:----- Epoch[077/800], Train Loss: 0.6905, time: 883.84
INFO:local_logger:Now training epoch 78. LR=0.000152
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-77-Loss-0.6897522572932259.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-77-Loss-0.6897522572932259.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-77-Loss-0.6897522572932259.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-77-Loss-0.6897522572932259.pdopt
INFO:local_logger:Now training epoch 78. LR=0.000152
INFO:master_logger:Now training epoch 78. LR=0.000152
INFO:local_logger:Epoch[078/800], Step[0000/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[078/800], Step[0000/0626], Avg Loss: 0.6877
INFO:master_logger:Epoch[078/800], Step[0000/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[078/800], Step[0000/0626], Avg Loss: 0.6933
INFO:local_logger:Epoch[078/800], Step[0000/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[078/800], Step[0000/0626], Avg Loss: 0.6979
INFO:local_logger:Epoch[078/800], Step[0000/0626], Avg Loss: 0.6883
INFO:local_logger:Epoch[078/800], Step[0000/0626], Avg Loss: 0.6939
INFO:local_logger:Epoch[078/800], Step[0000/0626], Avg Loss: 0.6743
INFO:local_logger:Epoch[078/800], Step[0100/0626], Avg Loss: 0.6903
INFO:local_logger:Epoch[078/800], Step[0100/0626], Avg Loss: 0.6884
INFO:local_logger:Epoch[078/800], Step[0100/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[078/800], Step[0100/0626], Avg Loss: 0.6884
INFO:local_logger:Epoch[078/800], Step[0100/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[078/800], Step[0100/0626], Avg Loss: 0.6902
INFO:master_logger:Epoch[078/800], Step[0100/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[078/800], Step[0100/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[078/800], Step[0100/0626], Avg Loss: 0.6903
INFO:local_logger:Epoch[078/800], Step[0200/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[078/800], Step[0200/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[078/800], Step[0200/0626], Avg Loss: 0.6893
INFO:local_logger:Epoch[078/800], Step[0200/0626], Avg Loss: 0.6892
INFO:local_logger:Epoch[078/800], Step[0200/0626], Avg Loss: 0.6895
INFO:master_logger:Epoch[078/800], Step[0200/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[078/800], Step[0200/0626], Avg Loss: 0.6899
INFO:local_logger:Epoch[078/800], Step[0200/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[078/800], Step[0200/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[078/800], Step[0300/0626], Avg Loss: 0.6896
INFO:master_logger:Epoch[078/800], Step[0300/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[078/800], Step[0300/0626], Avg Loss: 0.6895
INFO:local_logger:Epoch[078/800], Step[0300/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[078/800], Step[0300/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[078/800], Step[0300/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[078/800], Step[0300/0626], Avg Loss: 0.6895
INFO:local_logger:Epoch[078/800], Step[0300/0626], Avg Loss: 0.6903
INFO:local_logger:Epoch[078/800], Step[0300/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[078/800], Step[0400/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[078/800], Step[0400/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[078/800], Step[0400/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[078/800], Step[0400/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[078/800], Step[0400/0626], Avg Loss: 0.6901
INFO:master_logger:Epoch[078/800], Step[0400/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[078/800], Step[0400/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[078/800], Step[0400/0626], Avg Loss: 0.6894
INFO:local_logger:Epoch[078/800], Step[0400/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[078/800], Step[0500/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[078/800], Step[0500/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[078/800], Step[0500/0626], Avg Loss: 0.6894
INFO:local_logger:Epoch[078/800], Step[0500/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[078/800], Step[0500/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[078/800], Step[0500/0626], Avg Loss: 0.6898
INFO:master_logger:Epoch[078/800], Step[0500/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[078/800], Step[0500/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[078/800], Step[0500/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[078/800], Step[0600/0626], Avg Loss: 0.6895
INFO:local_logger:Epoch[078/800], Step[0600/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[078/800], Step[0600/0626], Avg Loss: 0.6895
INFO:local_logger:Epoch[078/800], Step[0600/0626], Avg Loss: 0.6893
INFO:local_logger:Epoch[078/800], Step[0600/0626], Avg Loss: 0.6895
INFO:local_logger:Epoch[078/800], Step[0600/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[078/800], Step[0600/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[078/800], Step[0600/0626], Avg Loss: 0.6897
INFO:master_logger:Epoch[078/800], Step[0600/0626], Avg Loss: 0.6896
INFO:local_logger:----- Epoch[078/800], Train Loss: 0.6895, time: 850.97
INFO:local_logger:Now training epoch 79. LR=0.000152
INFO:local_logger:----- Epoch[078/800], Train Loss: 0.6897, time: 849.85
INFO:local_logger:Now training epoch 79. LR=0.000152
INFO:local_logger:----- Epoch[078/800], Train Loss: 0.6897, time: 850.45
INFO:local_logger:Now training epoch 79. LR=0.000152
INFO:local_logger:----- Epoch[078/800], Train Loss: 0.6894, time: 850.45
INFO:local_logger:Now training epoch 79. LR=0.000152
INFO:local_logger:----- Epoch[078/800], Train Loss: 0.6895, time: 846.72
INFO:master_logger:----- Epoch[078/800], Train Loss: 0.6896, time: 846.72
INFO:local_logger:----- Epoch[078/800], Train Loss: 0.6897, time: 850.44
INFO:local_logger:Now training epoch 79. LR=0.000152
INFO:local_logger:----- Epoch[078/800], Train Loss: 0.6894, time: 850.46
INFO:local_logger:Now training epoch 79. LR=0.000152
INFO:local_logger:----- Epoch[078/800], Train Loss: 0.6898, time: 850.47
INFO:local_logger:Now training epoch 79. LR=0.000152
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-78-Loss-0.6895287958086865.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-78-Loss-0.6895287958086865.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-78-Loss-0.6895287958086865.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-78-Loss-0.6895287958086865.pdopt
INFO:local_logger:Now training epoch 79. LR=0.000152
INFO:master_logger:Now training epoch 79. LR=0.000152
INFO:local_logger:Epoch[079/800], Step[0000/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[079/800], Step[0000/0626], Avg Loss: 0.6870
INFO:master_logger:Epoch[079/800], Step[0000/0626], Avg Loss: 0.6882
INFO:local_logger:Epoch[079/800], Step[0000/0626], Avg Loss: 0.7025
INFO:local_logger:Epoch[079/800], Step[0000/0626], Avg Loss: 0.6862
INFO:local_logger:Epoch[079/800], Step[0000/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[079/800], Step[0000/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[079/800], Step[0000/0626], Avg Loss: 0.6939
INFO:local_logger:Epoch[079/800], Step[0000/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[079/800], Step[0100/0626], Avg Loss: 0.6890
INFO:local_logger:Epoch[079/800], Step[0100/0626], Avg Loss: 0.6901
INFO:local_logger:Epoch[079/800], Step[0100/0626], Avg Loss: 0.6892
INFO:master_logger:Epoch[079/800], Step[0100/0626], Avg Loss: 0.6891
INFO:local_logger:Epoch[079/800], Step[0100/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[079/800], Step[0100/0626], Avg Loss: 0.6902
INFO:local_logger:Epoch[079/800], Step[0100/0626], Avg Loss: 0.6893
INFO:local_logger:Epoch[079/800], Step[0100/0626], Avg Loss: 0.6887
INFO:local_logger:Epoch[079/800], Step[0100/0626], Avg Loss: 0.6882
INFO:local_logger:Epoch[079/800], Step[0200/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[079/800], Step[0200/0626], Avg Loss: 0.6886
INFO:local_logger:Epoch[079/800], Step[0200/0626], Avg Loss: 0.6892
INFO:local_logger:Epoch[079/800], Step[0200/0626], Avg Loss: 0.6893
INFO:local_logger:Epoch[079/800], Step[0200/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[079/800], Step[0200/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[079/800], Step[0200/0626], Avg Loss: 0.6890
INFO:local_logger:Epoch[079/800], Step[0200/0626], Avg Loss: 0.6900
INFO:master_logger:Epoch[079/800], Step[0200/0626], Avg Loss: 0.6893
INFO:local_logger:Epoch[079/800], Step[0300/0626], Avg Loss: 0.6891
INFO:local_logger:Epoch[079/800], Step[0300/0626], Avg Loss: 0.6892
INFO:local_logger:Epoch[079/800], Step[0300/0626], Avg Loss: 0.6894
INFO:master_logger:Epoch[079/800], Step[0300/0626], Avg Loss: 0.6894
INFO:local_logger:Epoch[079/800], Step[0300/0626], Avg Loss: 0.6893
INFO:local_logger:Epoch[079/800], Step[0300/0626], Avg Loss: 0.6889
INFO:local_logger:Epoch[079/800], Step[0300/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[079/800], Step[0300/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[079/800], Step[0300/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[079/800], Step[0400/0626], Avg Loss: 0.6893
INFO:local_logger:Epoch[079/800], Step[0400/0626], Avg Loss: 0.6891
INFO:local_logger:Epoch[079/800], Step[0400/0626], Avg Loss: 0.6894
INFO:local_logger:Epoch[079/800], Step[0400/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[079/800], Step[0400/0626], Avg Loss: 0.6891
INFO:master_logger:Epoch[079/800], Step[0400/0626], Avg Loss: 0.6892
INFO:local_logger:Epoch[079/800], Step[0400/0626], Avg Loss: 0.6891
INFO:local_logger:Epoch[079/800], Step[0400/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[079/800], Step[0400/0626], Avg Loss: 0.6894
INFO:local_logger:Epoch[079/800], Step[0500/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[079/800], Step[0500/0626], Avg Loss: 0.6892
INFO:local_logger:Epoch[079/800], Step[0500/0626], Avg Loss: 0.6895
INFO:local_logger:Epoch[079/800], Step[0500/0626], Avg Loss: 0.6893
INFO:local_logger:Epoch[079/800], Step[0500/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[079/800], Step[0500/0626], Avg Loss: 0.6892
INFO:local_logger:Epoch[079/800], Step[0500/0626], Avg Loss: 0.6895
INFO:local_logger:Epoch[079/800], Step[0500/0626], Avg Loss: 0.6888
INFO:master_logger:Epoch[079/800], Step[0500/0626], Avg Loss: 0.6892
INFO:local_logger:Epoch[079/800], Step[0600/0626], Avg Loss: 0.6887
INFO:local_logger:Epoch[079/800], Step[0600/0626], Avg Loss: 0.6894
INFO:local_logger:Epoch[079/800], Step[0600/0626], Avg Loss: 0.6892
INFO:master_logger:Epoch[079/800], Step[0600/0626], Avg Loss: 0.6892
INFO:local_logger:Epoch[079/800], Step[0600/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[079/800], Step[0600/0626], Avg Loss: 0.6889
INFO:local_logger:Epoch[079/800], Step[0600/0626], Avg Loss: 0.6894
INFO:local_logger:Epoch[079/800], Step[0600/0626], Avg Loss: 0.6896
INFO:local_logger:Epoch[079/800], Step[0600/0626], Avg Loss: 0.6891
INFO:local_logger:----- Epoch[079/800], Train Loss: 0.6892, time: 888.30
INFO:local_logger:Now training epoch 80. LR=0.000152
INFO:local_logger:----- Epoch[079/800], Train Loss: 0.6889, time: 888.13
INFO:local_logger:----- Epoch[079/800], Train Loss: 0.6891, time: 888.74
INFO:local_logger:Now training epoch 80. LR=0.000152
INFO:local_logger:Now training epoch 80. LR=0.000152
INFO:local_logger:----- Epoch[079/800], Train Loss: 0.6896, time: 888.14
INFO:local_logger:Now training epoch 80. LR=0.000152
INFO:local_logger:----- Epoch[079/800], Train Loss: 0.6886, time: 884.16
INFO:master_logger:----- Epoch[079/800], Train Loss: 0.6892, time: 884.16
INFO:local_logger:----- Epoch[079/800], Train Loss: 0.6893, time: 888.24
INFO:local_logger:Now training epoch 80. LR=0.000152
INFO:local_logger:----- Epoch[079/800], Train Loss: 0.6896, time: 888.24
INFO:local_logger:Now training epoch 80. LR=0.000152
INFO:local_logger:----- Epoch[079/800], Train Loss: 0.6893, time: 888.26
INFO:local_logger:Now training epoch 80. LR=0.000152
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-79-Loss-0.6886302635034396.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-79-Loss-0.6886302635034396.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-79-Loss-0.6886302635034396.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-79-Loss-0.6886302635034396.pdopt
INFO:local_logger:Now training epoch 80. LR=0.000152
INFO:master_logger:Now training epoch 80. LR=0.000152
INFO:local_logger:Epoch[080/800], Step[0000/0626], Avg Loss: 0.6883
INFO:local_logger:Epoch[080/800], Step[0000/0626], Avg Loss: 0.6859
INFO:master_logger:Epoch[080/800], Step[0000/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[080/800], Step[0000/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[080/800], Step[0000/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[080/800], Step[0000/0626], Avg Loss: 0.6866
INFO:local_logger:Epoch[080/800], Step[0000/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[080/800], Step[0000/0626], Avg Loss: 0.7038
INFO:local_logger:Epoch[080/800], Step[0000/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[080/800], Step[0100/0626], Avg Loss: 0.6905
INFO:local_logger:Epoch[080/800], Step[0100/0626], Avg Loss: 0.6889
INFO:local_logger:Epoch[080/800], Step[0100/0626], Avg Loss: 0.6877
INFO:master_logger:Epoch[080/800], Step[0100/0626], Avg Loss: 0.6887
INFO:local_logger:Epoch[080/800], Step[0100/0626], Avg Loss: 0.6869
INFO:local_logger:Epoch[080/800], Step[0100/0626], Avg Loss: 0.6883
INFO:local_logger:Epoch[080/800], Step[0100/0626], Avg Loss: 0.6890
INFO:local_logger:Epoch[080/800], Step[0100/0626], Avg Loss: 0.6887
INFO:local_logger:Epoch[080/800], Step[0100/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[080/800], Step[0200/0626], Avg Loss: 0.6885
INFO:local_logger:Epoch[080/800], Step[0200/0626], Avg Loss: 0.6883
INFO:local_logger:Epoch[080/800], Step[0200/0626], Avg Loss: 0.6886
INFO:local_logger:Epoch[080/800], Step[0200/0626], Avg Loss: 0.6886
INFO:master_logger:Epoch[080/800], Step[0200/0626], Avg Loss: 0.6887
INFO:local_logger:Epoch[080/800], Step[0200/0626], Avg Loss: 0.6890
INFO:local_logger:Epoch[080/800], Step[0200/0626], Avg Loss: 0.6891
INFO:local_logger:Epoch[080/800], Step[0200/0626], Avg Loss: 0.6893
INFO:local_logger:Epoch[080/800], Step[0200/0626], Avg Loss: 0.6880
INFO:local_logger:Epoch[080/800], Step[0300/0626], Avg Loss: 0.6883
INFO:local_logger:Epoch[080/800], Step[0300/0626], Avg Loss: 0.6889
INFO:local_logger:Epoch[080/800], Step[0300/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[080/800], Step[0300/0626], Avg Loss: 0.6886
INFO:local_logger:Epoch[080/800], Step[0300/0626], Avg Loss: 0.6891
INFO:local_logger:Epoch[080/800], Step[0300/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[080/800], Step[0300/0626], Avg Loss: 0.6885
INFO:local_logger:Epoch[080/800], Step[0300/0626], Avg Loss: 0.6887
INFO:master_logger:Epoch[080/800], Step[0300/0626], Avg Loss: 0.6886
INFO:local_logger:Epoch[080/800], Step[0400/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[080/800], Step[0400/0626], Avg Loss: 0.6887
INFO:local_logger:Epoch[080/800], Step[0400/0626], Avg Loss: 0.6886
INFO:local_logger:Epoch[080/800], Step[0400/0626], Avg Loss: 0.6891
INFO:local_logger:Epoch[080/800], Step[0400/0626], Avg Loss: 0.6886
INFO:local_logger:Epoch[080/800], Step[0400/0626], Avg Loss: 0.6886
INFO:master_logger:Epoch[080/800], Step[0400/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[080/800], Step[0400/0626], Avg Loss: 0.6890
INFO:local_logger:Epoch[080/800], Step[0400/0626], Avg Loss: 0.6891
INFO:local_logger:Epoch[080/800], Step[0500/0626], Avg Loss: 0.6890
INFO:local_logger:Epoch[080/800], Step[0500/0626], Avg Loss: 0.6886
INFO:local_logger:Epoch[080/800], Step[0500/0626], Avg Loss: 0.6891
INFO:local_logger:Epoch[080/800], Step[0500/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[080/800], Step[0500/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[080/800], Step[0500/0626], Avg Loss: 0.6891
INFO:local_logger:Epoch[080/800], Step[0500/0626], Avg Loss: 0.6888
INFO:master_logger:Epoch[080/800], Step[0500/0626], Avg Loss: 0.6889
INFO:local_logger:Epoch[080/800], Step[0500/0626], Avg Loss: 0.6889
INFO:local_logger:Epoch[080/800], Step[0600/0626], Avg Loss: 0.6890
INFO:local_logger:Epoch[080/800], Step[0600/0626], Avg Loss: 0.6886
INFO:local_logger:Epoch[080/800], Step[0600/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[080/800], Step[0600/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[080/800], Step[0600/0626], Avg Loss: 0.6887
INFO:local_logger:Epoch[080/800], Step[0600/0626], Avg Loss: 0.6888
INFO:master_logger:Epoch[080/800], Step[0600/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[080/800], Step[0600/0626], Avg Loss: 0.6889
INFO:local_logger:Epoch[080/800], Step[0600/0626], Avg Loss: 0.6890
INFO:local_logger:----- Epoch[080/800], Train Loss: 0.6890, time: 849.18
INFO:local_logger:Now training epoch 81. LR=0.000153
INFO:local_logger:----- Epoch[080/800], Train Loss: 0.6888, time: 849.16
INFO:local_logger:Now training epoch 81. LR=0.000153
INFO:local_logger:----- Epoch[080/800], Train Loss: 0.6890, time: 849.36
INFO:local_logger:Now training epoch 81. LR=0.000153
INFO:local_logger:----- Epoch[080/800], Train Loss: 0.6887, time: 849.41
INFO:local_logger:Now training epoch 81. LR=0.000153
INFO:local_logger:----- Epoch[080/800], Train Loss: 0.6888, time: 849.87
INFO:local_logger:Now training epoch 81. LR=0.000153
INFO:local_logger:----- Epoch[080/800], Train Loss: 0.6889, time: 849.31
INFO:local_logger:Now training epoch 81. LR=0.000153
INFO:local_logger:----- Epoch[080/800], Train Loss: 0.6888, time: 845.47
INFO:master_logger:----- Epoch[080/800], Train Loss: 0.6889, time: 845.47
INFO:local_logger:----- Epoch[080/800], Train Loss: 0.6891, time: 849.50
INFO:local_logger:Now training epoch 81. LR=0.000153
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-80-Loss-0.6887507163269282.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-80-Loss-0.6887507163269282.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-80-Loss-0.6887507163269282.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-80-Loss-0.6887507163269282.pdopt
INFO:local_logger:Now training epoch 81. LR=0.000153
INFO:master_logger:Now training epoch 81. LR=0.000153
INFO:local_logger:Epoch[081/800], Step[0000/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[081/800], Step[0000/0626], Avg Loss: 0.6873
INFO:local_logger:Epoch[081/800], Step[0000/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[081/800], Step[0000/0626], Avg Loss: 0.6999
INFO:master_logger:Epoch[081/800], Step[0000/0626], Avg Loss: 0.6908
INFO:local_logger:Epoch[081/800], Step[0000/0626], Avg Loss: 0.6984
INFO:local_logger:Epoch[081/800], Step[0000/0626], Avg Loss: 0.7003
INFO:local_logger:Epoch[081/800], Step[0000/0626], Avg Loss: 0.6882
INFO:local_logger:Epoch[081/800], Step[0000/0626], Avg Loss: 0.6847
INFO:local_logger:Epoch[081/800], Step[0100/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[081/800], Step[0100/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[081/800], Step[0100/0626], Avg Loss: 0.6887
INFO:local_logger:Epoch[081/800], Step[0100/0626], Avg Loss: 0.6887
INFO:local_logger:Epoch[081/800], Step[0100/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[081/800], Step[0100/0626], Avg Loss: 0.6890
INFO:local_logger:Epoch[081/800], Step[0100/0626], Avg Loss: 0.6882
INFO:master_logger:Epoch[081/800], Step[0100/0626], Avg Loss: 0.6883
INFO:local_logger:Epoch[081/800], Step[0100/0626], Avg Loss: 0.6874
INFO:local_logger:Epoch[081/800], Step[0200/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[081/800], Step[0200/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[081/800], Step[0200/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[081/800], Step[0200/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[081/800], Step[0200/0626], Avg Loss: 0.6885
INFO:local_logger:Epoch[081/800], Step[0200/0626], Avg Loss: 0.6892
INFO:local_logger:Epoch[081/800], Step[0200/0626], Avg Loss: 0.6885
INFO:local_logger:Epoch[081/800], Step[0200/0626], Avg Loss: 0.6886
INFO:master_logger:Epoch[081/800], Step[0200/0626], Avg Loss: 0.6883
INFO:local_logger:Epoch[081/800], Step[0300/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[081/800], Step[0300/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[081/800], Step[0300/0626], Avg Loss: 0.6883
INFO:local_logger:Epoch[081/800], Step[0300/0626], Avg Loss: 0.6881
INFO:master_logger:Epoch[081/800], Step[0300/0626], Avg Loss: 0.6882
INFO:local_logger:Epoch[081/800], Step[0300/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[081/800], Step[0300/0626], Avg Loss: 0.6887
INFO:local_logger:Epoch[081/800], Step[0300/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[081/800], Step[0300/0626], Avg Loss: 0.6885
INFO:local_logger:Epoch[081/800], Step[0400/0626], Avg Loss: 0.6879
INFO:local_logger:Epoch[081/800], Step[0400/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[081/800], Step[0400/0626], Avg Loss: 0.6885
INFO:local_logger:Epoch[081/800], Step[0400/0626], Avg Loss: 0.6879
INFO:local_logger:Epoch[081/800], Step[0400/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[081/800], Step[0400/0626], Avg Loss: 0.6880
INFO:local_logger:Epoch[081/800], Step[0400/0626], Avg Loss: 0.6886
INFO:master_logger:Epoch[081/800], Step[0400/0626], Avg Loss: 0.6882
INFO:local_logger:Epoch[081/800], Step[0400/0626], Avg Loss: 0.6888
INFO:local_logger:Epoch[081/800], Step[0500/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[081/800], Step[0500/0626], Avg Loss: 0.6884
INFO:local_logger:Epoch[081/800], Step[0500/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[081/800], Step[0500/0626], Avg Loss: 0.6885
INFO:local_logger:Epoch[081/800], Step[0500/0626], Avg Loss: 0.6879
INFO:local_logger:Epoch[081/800], Step[0500/0626], Avg Loss: 0.6878
INFO:master_logger:Epoch[081/800], Step[0500/0626], Avg Loss: 0.6882
INFO:local_logger:Epoch[081/800], Step[0500/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[081/800], Step[0500/0626], Avg Loss: 0.6885
INFO:local_logger:Epoch[081/800], Step[0600/0626], Avg Loss: 0.6880
INFO:local_logger:Epoch[081/800], Step[0600/0626], Avg Loss: 0.6880
INFO:local_logger:Epoch[081/800], Step[0600/0626], Avg Loss: 0.6882
INFO:local_logger:Epoch[081/800], Step[0600/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[081/800], Step[0600/0626], Avg Loss: 0.6883
INFO:local_logger:Epoch[081/800], Step[0600/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[081/800], Step[0600/0626], Avg Loss: 0.6886
INFO:master_logger:Epoch[081/800], Step[0600/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[081/800], Step[0600/0626], Avg Loss: 0.6885
INFO:local_logger:----- Epoch[081/800], Train Loss: 0.6878, time: 886.96
INFO:local_logger:Now training epoch 82. LR=0.000153
INFO:local_logger:----- Epoch[081/800], Train Loss: 0.6882, time: 887.25
INFO:local_logger:Now training epoch 82. LR=0.000153
INFO:local_logger:----- Epoch[081/800], Train Loss: 0.6882, time: 887.32
INFO:local_logger:Now training epoch 82. LR=0.000153
INFO:local_logger:----- Epoch[081/800], Train Loss: 0.6883, time: 887.55
INFO:local_logger:Now training epoch 82. LR=0.000153
INFO:local_logger:----- Epoch[081/800], Train Loss: 0.6878, time: 887.45
INFO:local_logger:Now training epoch 82. LR=0.000153
INFO:local_logger:----- Epoch[081/800], Train Loss: 0.6883, time: 887.60
INFO:local_logger:Now training epoch 82. LR=0.000153
INFO:local_logger:----- Epoch[081/800], Train Loss: 0.6880, time: 887.62
INFO:local_logger:Now training epoch 82. LR=0.000153
INFO:local_logger:----- Epoch[081/800], Train Loss: 0.6885, time: 883.70
INFO:master_logger:----- Epoch[081/800], Train Loss: 0.6882, time: 883.70
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-81-Loss-0.6885438528329545.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-81-Loss-0.6885438528329545.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-81-Loss-0.6885438528329545.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-81-Loss-0.6885438528329545.pdopt
INFO:local_logger:Now training epoch 82. LR=0.000153
INFO:master_logger:Now training epoch 82. LR=0.000153
INFO:local_logger:Epoch[082/800], Step[0000/0626], Avg Loss: 0.6723
INFO:local_logger:Epoch[082/800], Step[0000/0626], Avg Loss: 0.6813
INFO:master_logger:Epoch[082/800], Step[0000/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[082/800], Step[0000/0626], Avg Loss: 0.6948
INFO:local_logger:Epoch[082/800], Step[0000/0626], Avg Loss: 0.6790
INFO:local_logger:Epoch[082/800], Step[0000/0626], Avg Loss: 0.6783
INFO:local_logger:Epoch[082/800], Step[0000/0626], Avg Loss: 0.6882
INFO:local_logger:Epoch[082/800], Step[0000/0626], Avg Loss: 0.7052
INFO:local_logger:Epoch[082/800], Step[0000/0626], Avg Loss: 0.6743
INFO:local_logger:Epoch[082/800], Step[0100/0626], Avg Loss: 0.6875
INFO:local_logger:Epoch[082/800], Step[0100/0626], Avg Loss: 0.6885
INFO:local_logger:Epoch[082/800], Step[0100/0626], Avg Loss: 0.6874
INFO:master_logger:Epoch[082/800], Step[0100/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[082/800], Step[0100/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[082/800], Step[0100/0626], Avg Loss: 0.6886
INFO:local_logger:Epoch[082/800], Step[0100/0626], Avg Loss: 0.6871
INFO:local_logger:Epoch[082/800], Step[0100/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[082/800], Step[0100/0626], Avg Loss: 0.6882
INFO:local_logger:Epoch[082/800], Step[0200/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[082/800], Step[0200/0626], Avg Loss: 0.6880
INFO:local_logger:Epoch[082/800], Step[0200/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[082/800], Step[0200/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[082/800], Step[0200/0626], Avg Loss: 0.6875
INFO:master_logger:Epoch[082/800], Step[0200/0626], Avg Loss: 0.6879
INFO:local_logger:Epoch[082/800], Step[0200/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[082/800], Step[0200/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[082/800], Step[0200/0626], Avg Loss: 0.6884
INFO:local_logger:Epoch[082/800], Step[0300/0626], Avg Loss: 0.6874
INFO:local_logger:Epoch[082/800], Step[0300/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[082/800], Step[0300/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[082/800], Step[0300/0626], Avg Loss: 0.6873
INFO:local_logger:Epoch[082/800], Step[0300/0626], Avg Loss: 0.6882
INFO:master_logger:Epoch[082/800], Step[0300/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[082/800], Step[0300/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[082/800], Step[0300/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[082/800], Step[0300/0626], Avg Loss: 0.6883
INFO:local_logger:Epoch[082/800], Step[0400/0626], Avg Loss: 0.6880
INFO:local_logger:Epoch[082/800], Step[0400/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[082/800], Step[0400/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[082/800], Step[0400/0626], Avg Loss: 0.6879
INFO:local_logger:Epoch[082/800], Step[0400/0626], Avg Loss: 0.6871
INFO:local_logger:Epoch[082/800], Step[0400/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[082/800], Step[0400/0626], Avg Loss: 0.6879
INFO:master_logger:Epoch[082/800], Step[0400/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[082/800], Step[0400/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[082/800], Step[0500/0626], Avg Loss: 0.6879
INFO:local_logger:Epoch[082/800], Step[0500/0626], Avg Loss: 0.6872
INFO:local_logger:Epoch[082/800], Step[0500/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[082/800], Step[0500/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[082/800], Step[0500/0626], Avg Loss: 0.6874
INFO:master_logger:Epoch[082/800], Step[0500/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[082/800], Step[0500/0626], Avg Loss: 0.6879
INFO:local_logger:Epoch[082/800], Step[0500/0626], Avg Loss: 0.6880
INFO:local_logger:Epoch[082/800], Step[0500/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[082/800], Step[0600/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[082/800], Step[0600/0626], Avg Loss: 0.6880
INFO:local_logger:Epoch[082/800], Step[0600/0626], Avg Loss: 0.6879
INFO:local_logger:Epoch[082/800], Step[0600/0626], Avg Loss: 0.6873
INFO:master_logger:Epoch[082/800], Step[0600/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[082/800], Step[0600/0626], Avg Loss: 0.6871
INFO:local_logger:Epoch[082/800], Step[0600/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[082/800], Step[0600/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[082/800], Step[0600/0626], Avg Loss: 0.6880
INFO:local_logger:----- Epoch[082/800], Train Loss: 0.6876, time: 851.19
INFO:local_logger:Now training epoch 83. LR=0.000153
INFO:local_logger:----- Epoch[082/800], Train Loss: 0.6870, time: 851.29
INFO:local_logger:Now training epoch 83. LR=0.000153
INFO:local_logger:----- Epoch[082/800], Train Loss: 0.6876, time: 851.29
INFO:local_logger:Now training epoch 83. LR=0.000153
INFO:local_logger:----- Epoch[082/800], Train Loss: 0.6881, time: 851.15
INFO:local_logger:Now training epoch 83. LR=0.000153
INFO:local_logger:----- Epoch[082/800], Train Loss: 0.6881, time: 851.69
INFO:local_logger:Now training epoch 83. LR=0.000153
INFO:local_logger:----- Epoch[082/800], Train Loss: 0.6873, time: 851.08
INFO:local_logger:Now training epoch 83. LR=0.000153
INFO:local_logger:----- Epoch[082/800], Train Loss: 0.6877, time: 847.31
INFO:master_logger:----- Epoch[082/800], Train Loss: 0.6876, time: 847.31
INFO:local_logger:----- Epoch[082/800], Train Loss: 0.6879, time: 851.21
INFO:local_logger:Now training epoch 83. LR=0.000153
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-82-Loss-0.687688001142508.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-82-Loss-0.687688001142508.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-82-Loss-0.687688001142508.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-82-Loss-0.687688001142508.pdopt
INFO:local_logger:Now training epoch 83. LR=0.000153
INFO:master_logger:Now training epoch 83. LR=0.000153
INFO:local_logger:Epoch[083/800], Step[0000/0626], Avg Loss: 0.6878
INFO:master_logger:Epoch[083/800], Step[0000/0626], Avg Loss: 0.6887
INFO:local_logger:Epoch[083/800], Step[0000/0626], Avg Loss: 0.6966
INFO:local_logger:Epoch[083/800], Step[0000/0626], Avg Loss: 0.6793
INFO:local_logger:Epoch[083/800], Step[0000/0626], Avg Loss: 0.6913
INFO:local_logger:Epoch[083/800], Step[0000/0626], Avg Loss: 0.6781
INFO:local_logger:Epoch[083/800], Step[0000/0626], Avg Loss: 0.7000
INFO:local_logger:Epoch[083/800], Step[0000/0626], Avg Loss: 0.6871
INFO:local_logger:Epoch[083/800], Step[0000/0626], Avg Loss: 0.6893
INFO:local_logger:Epoch[083/800], Step[0100/0626], Avg Loss: 0.6891
INFO:local_logger:Epoch[083/800], Step[0100/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[083/800], Step[0100/0626], Avg Loss: 0.6873
INFO:local_logger:Epoch[083/800], Step[0100/0626], Avg Loss: 0.6875
INFO:local_logger:Epoch[083/800], Step[0100/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[083/800], Step[0100/0626], Avg Loss: 0.6882
INFO:master_logger:Epoch[083/800], Step[0100/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[083/800], Step[0100/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[083/800], Step[0100/0626], Avg Loss: 0.6872
INFO:local_logger:Epoch[083/800], Step[0200/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[083/800], Step[0200/0626], Avg Loss: 0.6871
INFO:local_logger:Epoch[083/800], Step[0200/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[083/800], Step[0200/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[083/800], Step[0200/0626], Avg Loss: 0.6880
INFO:local_logger:Epoch[083/800], Step[0200/0626], Avg Loss: 0.6869
INFO:local_logger:Epoch[083/800], Step[0200/0626], Avg Loss: 0.6890
INFO:master_logger:Epoch[083/800], Step[0200/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[083/800], Step[0200/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[083/800], Step[0300/0626], Avg Loss: 0.6877
INFO:local_logger:Epoch[083/800], Step[0300/0626], Avg Loss: 0.6871
INFO:local_logger:Epoch[083/800], Step[0300/0626], Avg Loss: 0.6868
INFO:local_logger:Epoch[083/800], Step[0300/0626], Avg Loss: 0.6872
INFO:master_logger:Epoch[083/800], Step[0300/0626], Avg Loss: 0.6873
INFO:local_logger:Epoch[083/800], Step[0300/0626], Avg Loss: 0.6883
INFO:local_logger:Epoch[083/800], Step[0300/0626], Avg Loss: 0.6863
INFO:local_logger:Epoch[083/800], Step[0300/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[083/800], Step[0300/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[083/800], Step[0400/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[083/800], Step[0400/0626], Avg Loss: 0.6874
INFO:local_logger:Epoch[083/800], Step[0400/0626], Avg Loss: 0.6879
INFO:local_logger:Epoch[083/800], Step[0400/0626], Avg Loss: 0.6873
INFO:master_logger:Epoch[083/800], Step[0400/0626], Avg Loss: 0.6873
INFO:local_logger:Epoch[083/800], Step[0400/0626], Avg Loss: 0.6873
INFO:local_logger:Epoch[083/800], Step[0400/0626], Avg Loss: 0.6867
INFO:local_logger:Epoch[083/800], Step[0400/0626], Avg Loss: 0.6865
INFO:local_logger:Epoch[083/800], Step[0400/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[083/800], Step[0500/0626], Avg Loss: 0.6874
INFO:local_logger:Epoch[083/800], Step[0500/0626], Avg Loss: 0.6874
INFO:local_logger:Epoch[083/800], Step[0500/0626], Avg Loss: 0.6875
INFO:local_logger:Epoch[083/800], Step[0500/0626], Avg Loss: 0.6866
INFO:master_logger:Epoch[083/800], Step[0500/0626], Avg Loss: 0.6872
INFO:local_logger:Epoch[083/800], Step[0500/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[083/800], Step[0500/0626], Avg Loss: 0.6874
INFO:local_logger:Epoch[083/800], Step[0500/0626], Avg Loss: 0.6873
INFO:local_logger:Epoch[083/800], Step[0500/0626], Avg Loss: 0.6868
INFO:local_logger:Epoch[083/800], Step[0600/0626], Avg Loss: 0.6872
INFO:local_logger:Epoch[083/800], Step[0600/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[083/800], Step[0600/0626], Avg Loss: 0.6871
INFO:local_logger:Epoch[083/800], Step[0600/0626], Avg Loss: 0.6874
INFO:local_logger:Epoch[083/800], Step[0600/0626], Avg Loss: 0.6875
INFO:local_logger:Epoch[083/800], Step[0600/0626], Avg Loss: 0.6866
INFO:master_logger:Epoch[083/800], Step[0600/0626], Avg Loss: 0.6872
INFO:local_logger:Epoch[083/800], Step[0600/0626], Avg Loss: 0.6874
INFO:local_logger:Epoch[083/800], Step[0600/0626], Avg Loss: 0.6867
INFO:local_logger:----- Epoch[083/800], Train Loss: 0.6872, time: 894.39
INFO:local_logger:----- Epoch[083/800], Train Loss: 0.6868, time: 894.42
INFO:local_logger:Now training epoch 84. LR=0.000153
INFO:local_logger:Now training epoch 84. LR=0.000153
INFO:local_logger:----- Epoch[083/800], Train Loss: 0.6876, time: 894.42
INFO:local_logger:Now training epoch 84. LR=0.000153
INFO:local_logger:----- Epoch[083/800], Train Loss: 0.6871, time: 890.84
INFO:master_logger:----- Epoch[083/800], Train Loss: 0.6872, time: 890.84
INFO:local_logger:----- Epoch[083/800], Train Loss: 0.6876, time: 894.60
INFO:local_logger:Now training epoch 84. LR=0.000153
INFO:local_logger:----- Epoch[083/800], Train Loss: 0.6873, time: 894.67
INFO:local_logger:Now training epoch 84. LR=0.000153
INFO:local_logger:----- Epoch[083/800], Train Loss: 0.6875, time: 894.68
INFO:local_logger:Now training epoch 84. LR=0.000153
INFO:local_logger:----- Epoch[083/800], Train Loss: 0.6866, time: 894.74
INFO:local_logger:Now training epoch 84. LR=0.000153
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-83-Loss-0.6870564654976226.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-83-Loss-0.6870564654976226.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-83-Loss-0.6870564654976226.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-83-Loss-0.6870564654976226.pdopt
INFO:local_logger:Now training epoch 84. LR=0.000153
INFO:master_logger:Now training epoch 84. LR=0.000153
INFO:local_logger:Epoch[084/800], Step[0000/0626], Avg Loss: 0.6927
INFO:master_logger:Epoch[084/800], Step[0000/0626], Avg Loss: 0.6881
INFO:local_logger:Epoch[084/800], Step[0000/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[084/800], Step[0000/0626], Avg Loss: 0.6905
INFO:local_logger:Epoch[084/800], Step[0000/0626], Avg Loss: 0.6929
INFO:local_logger:Epoch[084/800], Step[0000/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[084/800], Step[0000/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[084/800], Step[0000/0626], Avg Loss: 0.6745
INFO:local_logger:Epoch[084/800], Step[0000/0626], Avg Loss: 0.6914
INFO:local_logger:Epoch[084/800], Step[0100/0626], Avg Loss: 0.6875
INFO:local_logger:Epoch[084/800], Step[0100/0626], Avg Loss: 0.6866
INFO:local_logger:Epoch[084/800], Step[0100/0626], Avg Loss: 0.6880
INFO:master_logger:Epoch[084/800], Step[0100/0626], Avg Loss: 0.6870
INFO:local_logger:Epoch[084/800], Step[0100/0626], Avg Loss: 0.6867
INFO:local_logger:Epoch[084/800], Step[0100/0626], Avg Loss: 0.6870
INFO:local_logger:Epoch[084/800], Step[0100/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[084/800], Step[0100/0626], Avg Loss: 0.6883
INFO:local_logger:Epoch[084/800], Step[0100/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[084/800], Step[0200/0626], Avg Loss: 0.6862
INFO:local_logger:Epoch[084/800], Step[0200/0626], Avg Loss: 0.6865
INFO:local_logger:Epoch[084/800], Step[0200/0626], Avg Loss: 0.6867
INFO:master_logger:Epoch[084/800], Step[0200/0626], Avg Loss: 0.6869
INFO:local_logger:Epoch[084/800], Step[0200/0626], Avg Loss: 0.6874
INFO:local_logger:Epoch[084/800], Step[0200/0626], Avg Loss: 0.6873
INFO:local_logger:Epoch[084/800], Step[0200/0626], Avg Loss: 0.6884
INFO:local_logger:Epoch[084/800], Step[0200/0626], Avg Loss: 0.6866
INFO:local_logger:Epoch[084/800], Step[0200/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[084/800], Step[0300/0626], Avg Loss: 0.6867
INFO:local_logger:Epoch[084/800], Step[0300/0626], Avg Loss: 0.6873
INFO:local_logger:Epoch[084/800], Step[0300/0626], Avg Loss: 0.6878
INFO:local_logger:Epoch[084/800], Step[0300/0626], Avg Loss: 0.6865
INFO:local_logger:Epoch[084/800], Step[0300/0626], Avg Loss: 0.6867
INFO:local_logger:Epoch[084/800], Step[0300/0626], Avg Loss: 0.6868
INFO:local_logger:Epoch[084/800], Step[0300/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[084/800], Step[0300/0626], Avg Loss: 0.6868
INFO:master_logger:Epoch[084/800], Step[0300/0626], Avg Loss: 0.6869
INFO:local_logger:Epoch[084/800], Step[0400/0626], Avg Loss: 0.6867
INFO:local_logger:Epoch[084/800], Step[0400/0626], Avg Loss: 0.6866
INFO:local_logger:Epoch[084/800], Step[0400/0626], Avg Loss: 0.6869
INFO:local_logger:Epoch[084/800], Step[0400/0626], Avg Loss: 0.6875
INFO:local_logger:Epoch[084/800], Step[0400/0626], Avg Loss: 0.6870
INFO:local_logger:Epoch[084/800], Step[0400/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[084/800], Step[0400/0626], Avg Loss: 0.6876
INFO:master_logger:Epoch[084/800], Step[0400/0626], Avg Loss: 0.6869
INFO:local_logger:Epoch[084/800], Step[0400/0626], Avg Loss: 0.6866
INFO:local_logger:Epoch[084/800], Step[0500/0626], Avg Loss: 0.6865
INFO:local_logger:Epoch[084/800], Step[0500/0626], Avg Loss: 0.6865
INFO:local_logger:Epoch[084/800], Step[0500/0626], Avg Loss: 0.6870
INFO:local_logger:Epoch[084/800], Step[0500/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[084/800], Step[0500/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[084/800], Step[0500/0626], Avg Loss: 0.6866
INFO:master_logger:Epoch[084/800], Step[0500/0626], Avg Loss: 0.6869
INFO:local_logger:Epoch[084/800], Step[0500/0626], Avg Loss: 0.6868
INFO:local_logger:Epoch[084/800], Step[0500/0626], Avg Loss: 0.6867
INFO:local_logger:Epoch[084/800], Step[0600/0626], Avg Loss: 0.6874
INFO:local_logger:Epoch[084/800], Step[0600/0626], Avg Loss: 0.6865
INFO:local_logger:Epoch[084/800], Step[0600/0626], Avg Loss: 0.6870
INFO:local_logger:Epoch[084/800], Step[0600/0626], Avg Loss: 0.6869
INFO:local_logger:Epoch[084/800], Step[0600/0626], Avg Loss: 0.6875
INFO:local_logger:Epoch[084/800], Step[0600/0626], Avg Loss: 0.6867
INFO:master_logger:Epoch[084/800], Step[0600/0626], Avg Loss: 0.6869
INFO:local_logger:Epoch[084/800], Step[0600/0626], Avg Loss: 0.6865
INFO:local_logger:Epoch[084/800], Step[0600/0626], Avg Loss: 0.6865
INFO:local_logger:----- Epoch[084/800], Train Loss: 0.6870, time: 854.61
INFO:master_logger:----- Epoch[084/800], Train Loss: 0.6869, time: 854.61
INFO:local_logger:----- Epoch[084/800], Train Loss: 0.6868, time: 858.62
INFO:local_logger:Now training epoch 85. LR=0.000153
INFO:local_logger:----- Epoch[084/800], Train Loss: 0.6865, time: 858.88
INFO:local_logger:Now training epoch 85. LR=0.000153
INFO:local_logger:----- Epoch[084/800], Train Loss: 0.6866, time: 858.64
INFO:local_logger:Now training epoch 85. LR=0.000153
INFO:local_logger:----- Epoch[084/800], Train Loss: 0.6874, time: 858.90
INFO:local_logger:Now training epoch 85. LR=0.000153
INFO:local_logger:----- Epoch[084/800], Train Loss: 0.6868, time: 858.67
INFO:local_logger:Now training epoch 85. LR=0.000153
INFO:local_logger:----- Epoch[084/800], Train Loss: 0.6875, time: 858.57
INFO:local_logger:Now training epoch 85. LR=0.000153
INFO:local_logger:----- Epoch[084/800], Train Loss: 0.6864, time: 858.93
INFO:local_logger:Now training epoch 85. LR=0.000153
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-84-Loss-0.6870198997374206.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-84-Loss-0.6870198997374206.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-84-Loss-0.6870198997374206.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-84-Loss-0.6870198997374206.pdopt
INFO:local_logger:Now training epoch 85. LR=0.000153
INFO:master_logger:Now training epoch 85. LR=0.000153
INFO:local_logger:Epoch[085/800], Step[0000/0626], Avg Loss: 0.6915
INFO:local_logger:Epoch[085/800], Step[0000/0626], Avg Loss: 0.6830
INFO:master_logger:Epoch[085/800], Step[0000/0626], Avg Loss: 0.6870
INFO:local_logger:Epoch[085/800], Step[0000/0626], Avg Loss: 0.6898
INFO:local_logger:Epoch[085/800], Step[0000/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[085/800], Step[0000/0626], Avg Loss: 0.6935
INFO:local_logger:Epoch[085/800], Step[0000/0626], Avg Loss: 0.6929
INFO:local_logger:Epoch[085/800], Step[0000/0626], Avg Loss: 0.6837
INFO:local_logger:Epoch[085/800], Step[0000/0626], Avg Loss: 0.6803
INFO:local_logger:Epoch[085/800], Step[0100/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[085/800], Step[0100/0626], Avg Loss: 0.6854
INFO:local_logger:Epoch[085/800], Step[0100/0626], Avg Loss: 0.6868
INFO:local_logger:Epoch[085/800], Step[0100/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[085/800], Step[0100/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[085/800], Step[0100/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[085/800], Step[0100/0626], Avg Loss: 0.6843
INFO:master_logger:Epoch[085/800], Step[0100/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[085/800], Step[0100/0626], Avg Loss: 0.6870
INFO:local_logger:Epoch[085/800], Step[0200/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[085/800], Step[0200/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[085/800], Step[0200/0626], Avg Loss: 0.6866
INFO:local_logger:Epoch[085/800], Step[0200/0626], Avg Loss: 0.6854
INFO:local_logger:Epoch[085/800], Step[0200/0626], Avg Loss: 0.6868
INFO:local_logger:Epoch[085/800], Step[0200/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[085/800], Step[0200/0626], Avg Loss: 0.6865
INFO:master_logger:Epoch[085/800], Step[0200/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[085/800], Step[0200/0626], Avg Loss: 0.6856
INFO:local_logger:Epoch[085/800], Step[0300/0626], Avg Loss: 0.6873
INFO:local_logger:Epoch[085/800], Step[0300/0626], Avg Loss: 0.6867
INFO:local_logger:Epoch[085/800], Step[0300/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[085/800], Step[0300/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[085/800], Step[0300/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[085/800], Step[0300/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[085/800], Step[0300/0626], Avg Loss: 0.6866
INFO:master_logger:Epoch[085/800], Step[0300/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[085/800], Step[0300/0626], Avg Loss: 0.6865
INFO:local_logger:Epoch[085/800], Step[0400/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[085/800], Step[0400/0626], Avg Loss: 0.6856
INFO:local_logger:Epoch[085/800], Step[0400/0626], Avg Loss: 0.6863
INFO:local_logger:Epoch[085/800], Step[0400/0626], Avg Loss: 0.6869
INFO:local_logger:Epoch[085/800], Step[0400/0626], Avg Loss: 0.6866
INFO:local_logger:Epoch[085/800], Step[0400/0626], Avg Loss: 0.6866
INFO:master_logger:Epoch[085/800], Step[0400/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[085/800], Step[0400/0626], Avg Loss: 0.6865
INFO:local_logger:Epoch[085/800], Step[0400/0626], Avg Loss: 0.6863
INFO:local_logger:Epoch[085/800], Step[0500/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[085/800], Step[0500/0626], Avg Loss: 0.6862
INFO:local_logger:Epoch[085/800], Step[0500/0626], Avg Loss: 0.6866
INFO:local_logger:Epoch[085/800], Step[0500/0626], Avg Loss: 0.6868
INFO:local_logger:Epoch[085/800], Step[0500/0626], Avg Loss: 0.6865
INFO:local_logger:Epoch[085/800], Step[0500/0626], Avg Loss: 0.6862
INFO:local_logger:Epoch[085/800], Step[0500/0626], Avg Loss: 0.6860
INFO:master_logger:Epoch[085/800], Step[0500/0626], Avg Loss: 0.6863
INFO:local_logger:Epoch[085/800], Step[0500/0626], Avg Loss: 0.6863
INFO:local_logger:Epoch[085/800], Step[0600/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[085/800], Step[0600/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[085/800], Step[0600/0626], Avg Loss: 0.6865
INFO:local_logger:Epoch[085/800], Step[0600/0626], Avg Loss: 0.6866
INFO:local_logger:Epoch[085/800], Step[0600/0626], Avg Loss: 0.6859
INFO:master_logger:Epoch[085/800], Step[0600/0626], Avg Loss: 0.6862
INFO:local_logger:Epoch[085/800], Step[0600/0626], Avg Loss: 0.6863
INFO:local_logger:Epoch[085/800], Step[0600/0626], Avg Loss: 0.6862
INFO:local_logger:Epoch[085/800], Step[0600/0626], Avg Loss: 0.6862
INFO:local_logger:----- Epoch[085/800], Train Loss: 0.6862, time: 892.65
INFO:local_logger:Now training epoch 86. LR=0.000153
INFO:local_logger:----- Epoch[085/800], Train Loss: 0.6862, time: 893.08
INFO:local_logger:Now training epoch 86. LR=0.000153
INFO:local_logger:----- Epoch[085/800], Train Loss: 0.6859, time: 890.01
INFO:master_logger:----- Epoch[085/800], Train Loss: 0.6862, time: 890.01
INFO:local_logger:----- Epoch[085/800], Train Loss: 0.6864, time: 893.68
INFO:local_logger:Now training epoch 86. LR=0.000153
INFO:local_logger:----- Epoch[085/800], Train Loss: 0.6857, time: 893.70
INFO:local_logger:Now training epoch 86. LR=0.000153
INFO:local_logger:----- Epoch[085/800], Train Loss: 0.6863, time: 893.73
INFO:local_logger:Now training epoch 86. LR=0.000153
INFO:local_logger:----- Epoch[085/800], Train Loss: 0.6867, time: 893.72
INFO:local_logger:Now training epoch 86. LR=0.000153
INFO:local_logger:----- Epoch[085/800], Train Loss: 0.6864, time: 893.76
INFO:local_logger:Now training epoch 86. LR=0.000153
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-85-Loss-0.6859439270970955.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-85-Loss-0.6859439270970955.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-85-Loss-0.6859439270970955.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-85-Loss-0.6859439270970955.pdopt
INFO:local_logger:Now training epoch 86. LR=0.000153
INFO:master_logger:Now training epoch 86. LR=0.000153
INFO:local_logger:Epoch[086/800], Step[0000/0626], Avg Loss: 0.6904
INFO:local_logger:Epoch[086/800], Step[0000/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[086/800], Step[0000/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[086/800], Step[0000/0626], Avg Loss: 0.7015
INFO:local_logger:Epoch[086/800], Step[0000/0626], Avg Loss: 0.7112
INFO:local_logger:Epoch[086/800], Step[0000/0626], Avg Loss: 0.6865
INFO:local_logger:Epoch[086/800], Step[0000/0626], Avg Loss: 0.6773
INFO:master_logger:Epoch[086/800], Step[0000/0626], Avg Loss: 0.6887
INFO:local_logger:Epoch[086/800], Step[0000/0626], Avg Loss: 0.6796
INFO:local_logger:Epoch[086/800], Step[0100/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[086/800], Step[0100/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[086/800], Step[0100/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[086/800], Step[0100/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[086/800], Step[0100/0626], Avg Loss: 0.6876
INFO:master_logger:Epoch[086/800], Step[0100/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[086/800], Step[0100/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[086/800], Step[0100/0626], Avg Loss: 0.6873
INFO:local_logger:Epoch[086/800], Step[0100/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[086/800], Step[0200/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[086/800], Step[0200/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[086/800], Step[0200/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[086/800], Step[0200/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[086/800], Step[0200/0626], Avg Loss: 0.6863
INFO:local_logger:Epoch[086/800], Step[0200/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[086/800], Step[0200/0626], Avg Loss: 0.6862
INFO:master_logger:Epoch[086/800], Step[0200/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[086/800], Step[0200/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[086/800], Step[0300/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[086/800], Step[0300/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[086/800], Step[0300/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[086/800], Step[0300/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[086/800], Step[0300/0626], Avg Loss: 0.6863
INFO:local_logger:Epoch[086/800], Step[0300/0626], Avg Loss: 0.6862
INFO:local_logger:Epoch[086/800], Step[0300/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[086/800], Step[0300/0626], Avg Loss: 0.6860
INFO:master_logger:Epoch[086/800], Step[0300/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[086/800], Step[0400/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[086/800], Step[0400/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[086/800], Step[0400/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[086/800], Step[0400/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[086/800], Step[0400/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[086/800], Step[0400/0626], Avg Loss: 0.6862
INFO:master_logger:Epoch[086/800], Step[0400/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[086/800], Step[0400/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[086/800], Step[0400/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[086/800], Step[0500/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[086/800], Step[0500/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[086/800], Step[0500/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[086/800], Step[0500/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[086/800], Step[0500/0626], Avg Loss: 0.6863
INFO:local_logger:Epoch[086/800], Step[0500/0626], Avg Loss: 0.6862
INFO:master_logger:Epoch[086/800], Step[0500/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[086/800], Step[0500/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[086/800], Step[0500/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[086/800], Step[0600/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[086/800], Step[0600/0626], Avg Loss: 0.6862
INFO:local_logger:Epoch[086/800], Step[0600/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[086/800], Step[0600/0626], Avg Loss: 0.6858
INFO:master_logger:Epoch[086/800], Step[0600/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[086/800], Step[0600/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[086/800], Step[0600/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[086/800], Step[0600/0626], Avg Loss: 0.6862
INFO:local_logger:Epoch[086/800], Step[0600/0626], Avg Loss: 0.6858
INFO:local_logger:----- Epoch[086/800], Train Loss: 0.6860, time: 859.95
INFO:local_logger:Now training epoch 87. LR=0.000153
INFO:local_logger:----- Epoch[086/800], Train Loss: 0.6859, time: 860.07
INFO:local_logger:Now training epoch 87. LR=0.000153
INFO:local_logger:----- Epoch[086/800], Train Loss: 0.6862, time: 856.92
INFO:master_logger:----- Epoch[086/800], Train Loss: 0.6860, time: 856.92
INFO:local_logger:----- Epoch[086/800], Train Loss: 0.6860, time: 861.39
INFO:local_logger:Now training epoch 87. LR=0.000153
INFO:local_logger:----- Epoch[086/800], Train Loss: 0.6862, time: 860.97
INFO:local_logger:Now training epoch 87. LR=0.000153
INFO:local_logger:----- Epoch[086/800], Train Loss: 0.6857, time: 860.38
INFO:local_logger:Now training epoch 87. LR=0.000153
INFO:local_logger:----- Epoch[086/800], Train Loss: 0.6858, time: 860.35
INFO:local_logger:Now training epoch 87. LR=0.000153
INFO:local_logger:----- Epoch[086/800], Train Loss: 0.6859, time: 860.35
INFO:local_logger:Now training epoch 87. LR=0.000153
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-86-Loss-0.6862062182739747.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-86-Loss-0.6862062182739747.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-86-Loss-0.6862062182739747.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-86-Loss-0.6862062182739747.pdopt
INFO:local_logger:Now training epoch 87. LR=0.000153
INFO:master_logger:Now training epoch 87. LR=0.000153
INFO:local_logger:Epoch[087/800], Step[0000/0626], Avg Loss: 0.6924
INFO:local_logger:Epoch[087/800], Step[0000/0626], Avg Loss: 0.6883
INFO:master_logger:Epoch[087/800], Step[0000/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[087/800], Step[0000/0626], Avg Loss: 0.6722
INFO:local_logger:Epoch[087/800], Step[0000/0626], Avg Loss: 0.6880
INFO:local_logger:Epoch[087/800], Step[0000/0626], Avg Loss: 0.6993
INFO:local_logger:Epoch[087/800], Step[0000/0626], Avg Loss: 0.6908
INFO:local_logger:Epoch[087/800], Step[0000/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[087/800], Step[0000/0626], Avg Loss: 0.6712
INFO:local_logger:Epoch[087/800], Step[0100/0626], Avg Loss: 0.6852
INFO:local_logger:Epoch[087/800], Step[0100/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[087/800], Step[0100/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[087/800], Step[0100/0626], Avg Loss: 0.6868
INFO:local_logger:Epoch[087/800], Step[0100/0626], Avg Loss: 0.6870
INFO:local_logger:Epoch[087/800], Step[0100/0626], Avg Loss: 0.6862
INFO:master_logger:Epoch[087/800], Step[0100/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[087/800], Step[0100/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[087/800], Step[0100/0626], Avg Loss: 0.6862
INFO:local_logger:Epoch[087/800], Step[0200/0626], Avg Loss: 0.6863
INFO:local_logger:Epoch[087/800], Step[0200/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[087/800], Step[0200/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[087/800], Step[0200/0626], Avg Loss: 0.6856
INFO:local_logger:Epoch[087/800], Step[0200/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[087/800], Step[0200/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[087/800], Step[0200/0626], Avg Loss: 0.6864
INFO:master_logger:Epoch[087/800], Step[0200/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[087/800], Step[0200/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[087/800], Step[0300/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[087/800], Step[0300/0626], Avg Loss: 0.6856
INFO:local_logger:Epoch[087/800], Step[0300/0626], Avg Loss: 0.6863
INFO:local_logger:Epoch[087/800], Step[0300/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[087/800], Step[0300/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[087/800], Step[0300/0626], Avg Loss: 0.6860
INFO:master_logger:Epoch[087/800], Step[0300/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[087/800], Step[0300/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[087/800], Step[0300/0626], Avg Loss: 0.6864
INFO:local_logger:Epoch[087/800], Step[0400/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[087/800], Step[0400/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[087/800], Step[0400/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[087/800], Step[0400/0626], Avg Loss: 0.6852
INFO:local_logger:Epoch[087/800], Step[0400/0626], Avg Loss: 0.6862
INFO:local_logger:Epoch[087/800], Step[0400/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[087/800], Step[0400/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[087/800], Step[0400/0626], Avg Loss: 0.6858
INFO:master_logger:Epoch[087/800], Step[0400/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[087/800], Step[0500/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[087/800], Step[0500/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[087/800], Step[0500/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[087/800], Step[0500/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[087/800], Step[0500/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[087/800], Step[0500/0626], Avg Loss: 0.6852
INFO:master_logger:Epoch[087/800], Step[0500/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[087/800], Step[0500/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[087/800], Step[0500/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[087/800], Step[0600/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[087/800], Step[0600/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[087/800], Step[0600/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[087/800], Step[0600/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[087/800], Step[0600/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[087/800], Step[0600/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[087/800], Step[0600/0626], Avg Loss: 0.6858
INFO:master_logger:Epoch[087/800], Step[0600/0626], Avg Loss: 0.6856
INFO:local_logger:Epoch[087/800], Step[0600/0626], Avg Loss: 0.6852
INFO:local_logger:----- Epoch[087/800], Train Loss: 0.6853, time: 895.08
INFO:local_logger:Now training epoch 88. LR=0.000153
INFO:local_logger:----- Epoch[087/800], Train Loss: 0.6855, time: 895.09
INFO:local_logger:Now training epoch 88. LR=0.000153
INFO:local_logger:----- Epoch[087/800], Train Loss: 0.6860, time: 895.73
INFO:local_logger:Now training epoch 88. LR=0.000153
INFO:local_logger:----- Epoch[087/800], Train Loss: 0.6852, time: 896.01
INFO:local_logger:Now training epoch 88. LR=0.000153
INFO:local_logger:----- Epoch[087/800], Train Loss: 0.6856, time: 895.77
INFO:local_logger:----- Epoch[087/800], Train Loss: 0.6857, time: 896.11
INFO:local_logger:Now training epoch 88. LR=0.000153
INFO:local_logger:Now training epoch 88. LR=0.000153
INFO:local_logger:----- Epoch[087/800], Train Loss: 0.6857, time: 892.15
INFO:master_logger:----- Epoch[087/800], Train Loss: 0.6856, time: 892.15
INFO:local_logger:----- Epoch[087/800], Train Loss: 0.6855, time: 895.77
INFO:local_logger:Now training epoch 88. LR=0.000153
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-87-Loss-0.6857299549603768.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-87-Loss-0.6857299549603768.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-87-Loss-0.6857299549603768.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-87-Loss-0.6857299549603768.pdopt
INFO:local_logger:Now training epoch 88. LR=0.000153
INFO:master_logger:Now training epoch 88. LR=0.000153
INFO:local_logger:Epoch[088/800], Step[0000/0626], Avg Loss: 0.6931
INFO:local_logger:Epoch[088/800], Step[0000/0626], Avg Loss: 0.6886
INFO:local_logger:Epoch[088/800], Step[0000/0626], Avg Loss: 0.6873
INFO:local_logger:Epoch[088/800], Step[0000/0626], Avg Loss: 0.6866
INFO:local_logger:Epoch[088/800], Step[0000/0626], Avg Loss: 0.6765
INFO:local_logger:Epoch[088/800], Step[0000/0626], Avg Loss: 0.6795
INFO:local_logger:Epoch[088/800], Step[0000/0626], Avg Loss: 0.6839
INFO:master_logger:Epoch[088/800], Step[0000/0626], Avg Loss: 0.6844
INFO:local_logger:Epoch[088/800], Step[0000/0626], Avg Loss: 0.6800
INFO:local_logger:Epoch[088/800], Step[0100/0626], Avg Loss: 0.6870
INFO:local_logger:Epoch[088/800], Step[0100/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[088/800], Step[0100/0626], Avg Loss: 0.6876
INFO:local_logger:Epoch[088/800], Step[0100/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[088/800], Step[0100/0626], Avg Loss: 0.6868
INFO:master_logger:Epoch[088/800], Step[0100/0626], Avg Loss: 0.6863
INFO:local_logger:Epoch[088/800], Step[0100/0626], Avg Loss: 0.6847
INFO:local_logger:Epoch[088/800], Step[0100/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[088/800], Step[0100/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[088/800], Step[0200/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[088/800], Step[0200/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[088/800], Step[0200/0626], Avg Loss: 0.6863
INFO:local_logger:Epoch[088/800], Step[0200/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[088/800], Step[0200/0626], Avg Loss: 0.6852
INFO:local_logger:Epoch[088/800], Step[0200/0626], Avg Loss: 0.6860
INFO:master_logger:Epoch[088/800], Step[0200/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[088/800], Step[0200/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[088/800], Step[0200/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[088/800], Step[0300/0626], Avg Loss: 0.6854
INFO:local_logger:Epoch[088/800], Step[0300/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[088/800], Step[0300/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[088/800], Step[0300/0626], Avg Loss: 0.6861
INFO:local_logger:Epoch[088/800], Step[0300/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[088/800], Step[0300/0626], Avg Loss: 0.6859
INFO:master_logger:Epoch[088/800], Step[0300/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[088/800], Step[0300/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[088/800], Step[0300/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[088/800], Step[0400/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[088/800], Step[0400/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[088/800], Step[0400/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[088/800], Step[0400/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[088/800], Step[0400/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[088/800], Step[0400/0626], Avg Loss: 0.6854
INFO:local_logger:Epoch[088/800], Step[0400/0626], Avg Loss: 0.6859
INFO:master_logger:Epoch[088/800], Step[0400/0626], Avg Loss: 0.6856
INFO:local_logger:Epoch[088/800], Step[0400/0626], Avg Loss: 0.6854
INFO:local_logger:Epoch[088/800], Step[0500/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[088/800], Step[0500/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[088/800], Step[0500/0626], Avg Loss: 0.6854
INFO:local_logger:Epoch[088/800], Step[0500/0626], Avg Loss: 0.6854
INFO:local_logger:Epoch[088/800], Step[0500/0626], Avg Loss: 0.6857
INFO:master_logger:Epoch[088/800], Step[0500/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[088/800], Step[0500/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[088/800], Step[0500/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[088/800], Step[0500/0626], Avg Loss: 0.6857
INFO:local_logger:Epoch[088/800], Step[0600/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[088/800], Step[0600/0626], Avg Loss: 0.6854
INFO:local_logger:Epoch[088/800], Step[0600/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[088/800], Step[0600/0626], Avg Loss: 0.6854
INFO:local_logger:Epoch[088/800], Step[0600/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[088/800], Step[0600/0626], Avg Loss: 0.6854
INFO:master_logger:Epoch[088/800], Step[0600/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[088/800], Step[0600/0626], Avg Loss: 0.6850
INFO:local_logger:Epoch[088/800], Step[0600/0626], Avg Loss: 0.6853
INFO:local_logger:----- Epoch[088/800], Train Loss: 0.6855, time: 854.47
INFO:master_logger:----- Epoch[088/800], Train Loss: 0.6853, time: 854.47
INFO:local_logger:----- Epoch[088/800], Train Loss: 0.6854, time: 859.85
INFO:local_logger:Now training epoch 89. LR=0.000154
INFO:local_logger:----- Epoch[088/800], Train Loss: 0.6849, time: 859.24
INFO:local_logger:Now training epoch 89. LR=0.000154
INFO:local_logger:----- Epoch[088/800], Train Loss: 0.6852, time: 859.32
INFO:local_logger:Now training epoch 89. LR=0.000154
INFO:local_logger:----- Epoch[088/800], Train Loss: 0.6854, time: 859.33
INFO:local_logger:Now training epoch 89. LR=0.000154
INFO:local_logger:----- Epoch[088/800], Train Loss: 0.6853, time: 859.35
INFO:local_logger:----- Epoch[088/800], Train Loss: 0.6853, time: 859.33
INFO:local_logger:Now training epoch 89. LR=0.000154
INFO:local_logger:Now training epoch 89. LR=0.000154
INFO:local_logger:----- Epoch[088/800], Train Loss: 0.6856, time: 860.00
INFO:local_logger:Now training epoch 89. LR=0.000154
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-88-Loss-0.6854734038612285.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-88-Loss-0.6854734038612285.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-88-Loss-0.6854734038612285.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-88-Loss-0.6854734038612285.pdopt
INFO:local_logger:Now training epoch 89. LR=0.000154
INFO:master_logger:Now training epoch 89. LR=0.000154
INFO:local_logger:Epoch[089/800], Step[0000/0626], Avg Loss: 0.6780
INFO:local_logger:Epoch[089/800], Step[0000/0626], Avg Loss: 0.6912
INFO:local_logger:Epoch[089/800], Step[0000/0626], Avg Loss: 0.6884
INFO:local_logger:Epoch[089/800], Step[0000/0626], Avg Loss: 0.6960
INFO:local_logger:Epoch[089/800], Step[0000/0626], Avg Loss: 0.6791
INFO:local_logger:Epoch[089/800], Step[0000/0626], Avg Loss: 0.6747
INFO:local_logger:Epoch[089/800], Step[0000/0626], Avg Loss: 0.6772
INFO:local_logger:Epoch[089/800], Step[0000/0626], Avg Loss: 0.6825
INFO:master_logger:Epoch[089/800], Step[0000/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[089/800], Step[0100/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[089/800], Step[0100/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[089/800], Step[0100/0626], Avg Loss: 0.6856
INFO:local_logger:Epoch[089/800], Step[0100/0626], Avg Loss: 0.6847
INFO:local_logger:Epoch[089/800], Step[0100/0626], Avg Loss: 0.6850
INFO:master_logger:Epoch[089/800], Step[0100/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[089/800], Step[0100/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[089/800], Step[0100/0626], Avg Loss: 0.6848
INFO:local_logger:Epoch[089/800], Step[0100/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[089/800], Step[0200/0626], Avg Loss: 0.6851
INFO:local_logger:Epoch[089/800], Step[0200/0626], Avg Loss: 0.6844
INFO:local_logger:Epoch[089/800], Step[0200/0626], Avg Loss: 0.6846
INFO:local_logger:Epoch[089/800], Step[0200/0626], Avg Loss: 0.6852
INFO:local_logger:Epoch[089/800], Step[0200/0626], Avg Loss: 0.6844
INFO:master_logger:Epoch[089/800], Step[0200/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[089/800], Step[0200/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[089/800], Step[0200/0626], Avg Loss: 0.6852
INFO:local_logger:Epoch[089/800], Step[0200/0626], Avg Loss: 0.6854
INFO:local_logger:Epoch[089/800], Step[0300/0626], Avg Loss: 0.6854
INFO:local_logger:Epoch[089/800], Step[0300/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[089/800], Step[0300/0626], Avg Loss: 0.6841
INFO:local_logger:Epoch[089/800], Step[0300/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[089/800], Step[0300/0626], Avg Loss: 0.6847
INFO:local_logger:Epoch[089/800], Step[0300/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[089/800], Step[0300/0626], Avg Loss: 0.6852
INFO:local_logger:Epoch[089/800], Step[0300/0626], Avg Loss: 0.6847
INFO:master_logger:Epoch[089/800], Step[0300/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[089/800], Step[0400/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[089/800], Step[0400/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[089/800], Step[0400/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[089/800], Step[0400/0626], Avg Loss: 0.6850
INFO:local_logger:Epoch[089/800], Step[0400/0626], Avg Loss: 0.6854
INFO:master_logger:Epoch[089/800], Step[0400/0626], Avg Loss: 0.6850
INFO:local_logger:Epoch[089/800], Step[0400/0626], Avg Loss: 0.6851
INFO:local_logger:Epoch[089/800], Step[0400/0626], Avg Loss: 0.6856
INFO:local_logger:Epoch[089/800], Step[0400/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[089/800], Step[0500/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[089/800], Step[0500/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[089/800], Step[0500/0626], Avg Loss: 0.6851
INFO:local_logger:Epoch[089/800], Step[0500/0626], Avg Loss: 0.6848
INFO:local_logger:Epoch[089/800], Step[0500/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[089/800], Step[0500/0626], Avg Loss: 0.6847
INFO:local_logger:Epoch[089/800], Step[0500/0626], Avg Loss: 0.6849
INFO:master_logger:Epoch[089/800], Step[0500/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[089/800], Step[0500/0626], Avg Loss: 0.6850
INFO:local_logger:Epoch[089/800], Step[0600/0626], Avg Loss: 0.6847
INFO:local_logger:Epoch[089/800], Step[0600/0626], Avg Loss: 0.6850
INFO:local_logger:Epoch[089/800], Step[0600/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[089/800], Step[0600/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[089/800], Step[0600/0626], Avg Loss: 0.6845
INFO:master_logger:Epoch[089/800], Step[0600/0626], Avg Loss: 0.6847
INFO:local_logger:Epoch[089/800], Step[0600/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[089/800], Step[0600/0626], Avg Loss: 0.6848
INFO:local_logger:Epoch[089/800], Step[0600/0626], Avg Loss: 0.6849
INFO:local_logger:----- Epoch[089/800], Train Loss: 0.6845, time: 884.60
INFO:local_logger:Now training epoch 90. LR=0.000154
INFO:local_logger:----- Epoch[089/800], Train Loss: 0.6849, time: 885.46
INFO:local_logger:Now training epoch 90. LR=0.000154
INFO:local_logger:----- Epoch[089/800], Train Loss: 0.6846, time: 882.71
INFO:master_logger:----- Epoch[089/800], Train Loss: 0.6848, time: 882.71
INFO:local_logger:----- Epoch[089/800], Train Loss: 0.6850, time: 885.49
INFO:local_logger:Now training epoch 90. LR=0.000154
INFO:local_logger:----- Epoch[089/800], Train Loss: 0.6845, time: 885.58
INFO:local_logger:Now training epoch 90. LR=0.000154
INFO:local_logger:----- Epoch[089/800], Train Loss: 0.6848, time: 885.61
INFO:local_logger:Now training epoch 90. LR=0.000154
INFO:local_logger:----- Epoch[089/800], Train Loss: 0.6849, time: 885.54
INFO:local_logger:Now training epoch 90. LR=0.000154
INFO:local_logger:----- Epoch[089/800], Train Loss: 0.6849, time: 885.50
INFO:local_logger:Now training epoch 90. LR=0.000154
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-89-Loss-0.6845652029200572.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-89-Loss-0.6845652029200572.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-89-Loss-0.6845652029200572.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-89-Loss-0.6845652029200572.pdopt
INFO:local_logger:Now training epoch 90. LR=0.000154
INFO:master_logger:Now training epoch 90. LR=0.000154
INFO:local_logger:Epoch[090/800], Step[0000/0626], Avg Loss: 0.6924
INFO:local_logger:Epoch[090/800], Step[0000/0626], Avg Loss: 0.6899
INFO:local_logger:Epoch[090/800], Step[0000/0626], Avg Loss: 0.6905
INFO:local_logger:Epoch[090/800], Step[0000/0626], Avg Loss: 0.6763
INFO:local_logger:Epoch[090/800], Step[0000/0626], Avg Loss: 0.6748
INFO:master_logger:Epoch[090/800], Step[0000/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[090/800], Step[0000/0626], Avg Loss: 0.6827
INFO:local_logger:Epoch[090/800], Step[0000/0626], Avg Loss: 0.6787
INFO:local_logger:Epoch[090/800], Step[0000/0626], Avg Loss: 0.6883
INFO:local_logger:Epoch[090/800], Step[0100/0626], Avg Loss: 0.6841
INFO:local_logger:Epoch[090/800], Step[0100/0626], Avg Loss: 0.6851
INFO:local_logger:Epoch[090/800], Step[0100/0626], Avg Loss: 0.6853
INFO:master_logger:Epoch[090/800], Step[0100/0626], Avg Loss: 0.6846
INFO:local_logger:Epoch[090/800], Step[0100/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[090/800], Step[0100/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[090/800], Step[0100/0626], Avg Loss: 0.6841
INFO:local_logger:Epoch[090/800], Step[0100/0626], Avg Loss: 0.6847
INFO:local_logger:Epoch[090/800], Step[0100/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[090/800], Step[0200/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[090/800], Step[0200/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[090/800], Step[0200/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[090/800], Step[0200/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[090/800], Step[0200/0626], Avg Loss: 0.6850
INFO:master_logger:Epoch[090/800], Step[0200/0626], Avg Loss: 0.6844
INFO:local_logger:Epoch[090/800], Step[0200/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[090/800], Step[0200/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[090/800], Step[0200/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[090/800], Step[0300/0626], Avg Loss: 0.6851
INFO:local_logger:Epoch[090/800], Step[0300/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[090/800], Step[0300/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[090/800], Step[0300/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[090/800], Step[0300/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[090/800], Step[0300/0626], Avg Loss: 0.6848
INFO:local_logger:Epoch[090/800], Step[0300/0626], Avg Loss: 0.6841
INFO:master_logger:Epoch[090/800], Step[0300/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[090/800], Step[0300/0626], Avg Loss: 0.6841
INFO:local_logger:Epoch[090/800], Step[0400/0626], Avg Loss: 0.6844
INFO:local_logger:Epoch[090/800], Step[0400/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[090/800], Step[0400/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[090/800], Step[0400/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[090/800], Step[0400/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[090/800], Step[0400/0626], Avg Loss: 0.6845
INFO:master_logger:Epoch[090/800], Step[0400/0626], Avg Loss: 0.6844
INFO:local_logger:Epoch[090/800], Step[0400/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[090/800], Step[0400/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[090/800], Step[0500/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[090/800], Step[0500/0626], Avg Loss: 0.6846
INFO:local_logger:Epoch[090/800], Step[0500/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[090/800], Step[0500/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[090/800], Step[0500/0626], Avg Loss: 0.6842
INFO:master_logger:Epoch[090/800], Step[0500/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[090/800], Step[0500/0626], Avg Loss: 0.6841
INFO:local_logger:Epoch[090/800], Step[0500/0626], Avg Loss: 0.6844
INFO:local_logger:Epoch[090/800], Step[0500/0626], Avg Loss: 0.6846
INFO:local_logger:Epoch[090/800], Step[0600/0626], Avg Loss: 0.6844
INFO:local_logger:Epoch[090/800], Step[0600/0626], Avg Loss: 0.6844
INFO:local_logger:Epoch[090/800], Step[0600/0626], Avg Loss: 0.6842
INFO:master_logger:Epoch[090/800], Step[0600/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[090/800], Step[0600/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[090/800], Step[0600/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[090/800], Step[0600/0626], Avg Loss: 0.6846
INFO:local_logger:Epoch[090/800], Step[0600/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[090/800], Step[0600/0626], Avg Loss: 0.6845
INFO:local_logger:----- Epoch[090/800], Train Loss: 0.6845, time: 851.60
INFO:local_logger:Now training epoch 91. LR=0.000154
INFO:local_logger:----- Epoch[090/800], Train Loss: 0.6839, time: 851.05
INFO:local_logger:Now training epoch 91. LR=0.000154
INFO:local_logger:----- Epoch[090/800], Train Loss: 0.6844, time: 851.17
INFO:local_logger:Now training epoch 91. LR=0.000154
INFO:local_logger:----- Epoch[090/800], Train Loss: 0.6844, time: 851.26
INFO:local_logger:Now training epoch 91. LR=0.000154
INFO:local_logger:----- Epoch[090/800], Train Loss: 0.6847, time: 851.26
INFO:local_logger:Now training epoch 91. LR=0.000154
INFO:local_logger:----- Epoch[090/800], Train Loss: 0.6844, time: 851.23
INFO:local_logger:----- Epoch[090/800], Train Loss: 0.6841, time: 847.55
INFO:local_logger:Now training epoch 91. LR=0.000154
INFO:master_logger:----- Epoch[090/800], Train Loss: 0.6843, time: 847.55
INFO:local_logger:----- Epoch[090/800], Train Loss: 0.6840, time: 851.24
INFO:local_logger:Now training epoch 91. LR=0.000154
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-90-Loss-0.6841203801495528.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-90-Loss-0.6841203801495528.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-90-Loss-0.6841203801495528.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-90-Loss-0.6841203801495528.pdopt
INFO:local_logger:Now training epoch 91. LR=0.000154
INFO:master_logger:Now training epoch 91. LR=0.000154
INFO:local_logger:Epoch[091/800], Step[0000/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[091/800], Step[0000/0626], Avg Loss: 0.6872
INFO:local_logger:Epoch[091/800], Step[0000/0626], Avg Loss: 0.6756
INFO:master_logger:Epoch[091/800], Step[0000/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[091/800], Step[0000/0626], Avg Loss: 0.6859
INFO:local_logger:Epoch[091/800], Step[0000/0626], Avg Loss: 0.6748
INFO:local_logger:Epoch[091/800], Step[0000/0626], Avg Loss: 0.6793
INFO:local_logger:Epoch[091/800], Step[0000/0626], Avg Loss: 0.6961
INFO:local_logger:Epoch[091/800], Step[0000/0626], Avg Loss: 0.6897
INFO:local_logger:Epoch[091/800], Step[0100/0626], Avg Loss: 0.6846
INFO:local_logger:Epoch[091/800], Step[0100/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[091/800], Step[0100/0626], Avg Loss: 0.6833
INFO:local_logger:Epoch[091/800], Step[0100/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[091/800], Step[0100/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[091/800], Step[0100/0626], Avg Loss: 0.6833
INFO:master_logger:Epoch[091/800], Step[0100/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[091/800], Step[0100/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[091/800], Step[0100/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[091/800], Step[0200/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[091/800], Step[0200/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[091/800], Step[0200/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[091/800], Step[0200/0626], Avg Loss: 0.6833
INFO:local_logger:Epoch[091/800], Step[0200/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[091/800], Step[0200/0626], Avg Loss: 0.6846
INFO:local_logger:Epoch[091/800], Step[0200/0626], Avg Loss: 0.6844
INFO:local_logger:Epoch[091/800], Step[0200/0626], Avg Loss: 0.6837
INFO:master_logger:Epoch[091/800], Step[0200/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[091/800], Step[0300/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[091/800], Step[0300/0626], Avg Loss: 0.6850
INFO:local_logger:Epoch[091/800], Step[0300/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[091/800], Step[0300/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[091/800], Step[0300/0626], Avg Loss: 0.6834
INFO:master_logger:Epoch[091/800], Step[0300/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[091/800], Step[0300/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[091/800], Step[0300/0626], Avg Loss: 0.6847
INFO:local_logger:Epoch[091/800], Step[0300/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[091/800], Step[0400/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[091/800], Step[0400/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[091/800], Step[0400/0626], Avg Loss: 0.6847
INFO:local_logger:Epoch[091/800], Step[0400/0626], Avg Loss: 0.6847
INFO:master_logger:Epoch[091/800], Step[0400/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[091/800], Step[0400/0626], Avg Loss: 0.6841
INFO:local_logger:Epoch[091/800], Step[0400/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[091/800], Step[0400/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[091/800], Step[0400/0626], Avg Loss: 0.6848
INFO:local_logger:Epoch[091/800], Step[0500/0626], Avg Loss: 0.6846
INFO:local_logger:Epoch[091/800], Step[0500/0626], Avg Loss: 0.6837
INFO:local_logger:Epoch[091/800], Step[0500/0626], Avg Loss: 0.6841
INFO:local_logger:Epoch[091/800], Step[0500/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[091/800], Step[0500/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[091/800], Step[0500/0626], Avg Loss: 0.6840
INFO:master_logger:Epoch[091/800], Step[0500/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[091/800], Step[0500/0626], Avg Loss: 0.6846
INFO:local_logger:Epoch[091/800], Step[0500/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[091/800], Step[0600/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[091/800], Step[0600/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[091/800], Step[0600/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[091/800], Step[0600/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[091/800], Step[0600/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[091/800], Step[0600/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[091/800], Step[0600/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[091/800], Step[0600/0626], Avg Loss: 0.6842
INFO:master_logger:Epoch[091/800], Step[0600/0626], Avg Loss: 0.6842
INFO:local_logger:----- Epoch[091/800], Train Loss: 0.6843, time: 886.91
INFO:local_logger:Now training epoch 92. LR=0.000154
INFO:local_logger:----- Epoch[091/800], Train Loss: 0.6838, time: 886.90
INFO:local_logger:Now training epoch 92. LR=0.000154
INFO:local_logger:----- Epoch[091/800], Train Loss: 0.6848, time: 887.37
INFO:local_logger:Now training epoch 92. LR=0.000154
INFO:local_logger:----- Epoch[091/800], Train Loss: 0.6844, time: 887.49
INFO:local_logger:Now training epoch 92. LR=0.000154
INFO:local_logger:----- Epoch[091/800], Train Loss: 0.6842, time: 887.83
INFO:local_logger:Now training epoch 92. LR=0.000154
INFO:local_logger:----- Epoch[091/800], Train Loss: 0.6839, time: 887.30
INFO:local_logger:Now training epoch 92. LR=0.000154
INFO:local_logger:----- Epoch[091/800], Train Loss: 0.6839, time: 883.60
INFO:master_logger:----- Epoch[091/800], Train Loss: 0.6841, time: 883.60
INFO:local_logger:----- Epoch[091/800], Train Loss: 0.6838, time: 887.32
INFO:local_logger:Now training epoch 92. LR=0.000154
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-91-Loss-0.6838902651592956.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-91-Loss-0.6838902651592956.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-91-Loss-0.6838902651592956.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-91-Loss-0.6838902651592956.pdopt
INFO:local_logger:Now training epoch 92. LR=0.000154
INFO:master_logger:Now training epoch 92. LR=0.000154
INFO:local_logger:Epoch[092/800], Step[0000/0626], Avg Loss: 0.6957
INFO:local_logger:Epoch[092/800], Step[0000/0626], Avg Loss: 0.6676
INFO:local_logger:Epoch[092/800], Step[0000/0626], Avg Loss: 0.6676
INFO:local_logger:Epoch[092/800], Step[0000/0626], Avg Loss: 0.6866
INFO:master_logger:Epoch[092/800], Step[0000/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[092/800], Step[0000/0626], Avg Loss: 0.6922
INFO:local_logger:Epoch[092/800], Step[0000/0626], Avg Loss: 0.6936
INFO:local_logger:Epoch[092/800], Step[0000/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[092/800], Step[0000/0626], Avg Loss: 0.6874
INFO:local_logger:Epoch[092/800], Step[0100/0626], Avg Loss: 0.6844
INFO:local_logger:Epoch[092/800], Step[0100/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[092/800], Step[0100/0626], Avg Loss: 0.6841
INFO:local_logger:Epoch[092/800], Step[0100/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[092/800], Step[0100/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[092/800], Step[0100/0626], Avg Loss: 0.6840
INFO:master_logger:Epoch[092/800], Step[0100/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[092/800], Step[0100/0626], Avg Loss: 0.6850
INFO:local_logger:Epoch[092/800], Step[0100/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[092/800], Step[0200/0626], Avg Loss: 0.6837
INFO:local_logger:Epoch[092/800], Step[0200/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[092/800], Step[0200/0626], Avg Loss: 0.6831
INFO:master_logger:Epoch[092/800], Step[0200/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[092/800], Step[0200/0626], Avg Loss: 0.6844
INFO:local_logger:Epoch[092/800], Step[0200/0626], Avg Loss: 0.6832
INFO:local_logger:Epoch[092/800], Step[0200/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[092/800], Step[0200/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[092/800], Step[0200/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[092/800], Step[0300/0626], Avg Loss: 0.6832
INFO:local_logger:Epoch[092/800], Step[0300/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[092/800], Step[0300/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[092/800], Step[0300/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[092/800], Step[0300/0626], Avg Loss: 0.6841
INFO:local_logger:Epoch[092/800], Step[0300/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[092/800], Step[0300/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[092/800], Step[0300/0626], Avg Loss: 0.6839
INFO:master_logger:Epoch[092/800], Step[0300/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[092/800], Step[0400/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[092/800], Step[0400/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[092/800], Step[0400/0626], Avg Loss: 0.6841
INFO:local_logger:Epoch[092/800], Step[0400/0626], Avg Loss: 0.6841
INFO:master_logger:Epoch[092/800], Step[0400/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[092/800], Step[0400/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[092/800], Step[0400/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[092/800], Step[0400/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[092/800], Step[0400/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[092/800], Step[0500/0626], Avg Loss: 0.6837
INFO:local_logger:Epoch[092/800], Step[0500/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[092/800], Step[0500/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[092/800], Step[0500/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[092/800], Step[0500/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[092/800], Step[0500/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[092/800], Step[0500/0626], Avg Loss: 0.6840
INFO:master_logger:Epoch[092/800], Step[0500/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[092/800], Step[0500/0626], Avg Loss: 0.6842
INFO:local_logger:Epoch[092/800], Step[0600/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[092/800], Step[0600/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[092/800], Step[0600/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[092/800], Step[0600/0626], Avg Loss: 0.6839
INFO:master_logger:Epoch[092/800], Step[0600/0626], Avg Loss: 0.6837
INFO:local_logger:Epoch[092/800], Step[0600/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[092/800], Step[0600/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[092/800], Step[0600/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[092/800], Step[0600/0626], Avg Loss: 0.6836
INFO:local_logger:----- Epoch[092/800], Train Loss: 0.6839, time: 858.55
INFO:local_logger:Now training epoch 93. LR=0.000154
INFO:local_logger:----- Epoch[092/800], Train Loss: 0.6839, time: 858.56
INFO:local_logger:Now training epoch 93. LR=0.000154
INFO:local_logger:----- Epoch[092/800], Train Loss: 0.6836, time: 858.97
INFO:local_logger:Now training epoch 93. LR=0.000154
INFO:local_logger:----- Epoch[092/800], Train Loss: 0.6833, time: 858.75
INFO:local_logger:Now training epoch 93. LR=0.000154
INFO:local_logger:----- Epoch[092/800], Train Loss: 0.6837, time: 858.76
INFO:local_logger:Now training epoch 93. LR=0.000154
INFO:local_logger:----- Epoch[092/800], Train Loss: 0.6835, time: 858.85
INFO:local_logger:Now training epoch 93. LR=0.000154
INFO:local_logger:----- Epoch[092/800], Train Loss: 0.6841, time: 855.23
INFO:master_logger:----- Epoch[092/800], Train Loss: 0.6837, time: 855.23
INFO:local_logger:----- Epoch[092/800], Train Loss: 0.6836, time: 859.34
INFO:local_logger:Now training epoch 93. LR=0.000154
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-92-Loss-0.6840875268930822.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-92-Loss-0.6840875268930822.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-92-Loss-0.6840875268930822.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-92-Loss-0.6840875268930822.pdopt
INFO:local_logger:Now training epoch 93. LR=0.000154
INFO:master_logger:Now training epoch 93. LR=0.000154
INFO:local_logger:Epoch[093/800], Step[0000/0626], Avg Loss: 0.6662
INFO:local_logger:Epoch[093/800], Step[0000/0626], Avg Loss: 0.6933
INFO:local_logger:Epoch[093/800], Step[0000/0626], Avg Loss: 0.6938
INFO:local_logger:Epoch[093/800], Step[0000/0626], Avg Loss: 0.6859
INFO:master_logger:Epoch[093/800], Step[0000/0626], Avg Loss: 0.6846
INFO:local_logger:Epoch[093/800], Step[0000/0626], Avg Loss: 0.6770
INFO:local_logger:Epoch[093/800], Step[0000/0626], Avg Loss: 0.6934
INFO:local_logger:Epoch[093/800], Step[0000/0626], Avg Loss: 0.6802
INFO:local_logger:Epoch[093/800], Step[0000/0626], Avg Loss: 0.6873
INFO:local_logger:Epoch[093/800], Step[0100/0626], Avg Loss: 0.6837
INFO:local_logger:Epoch[093/800], Step[0100/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[093/800], Step[0100/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[093/800], Step[0100/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[093/800], Step[0100/0626], Avg Loss: 0.6837
INFO:local_logger:Epoch[093/800], Step[0100/0626], Avg Loss: 0.6848
INFO:local_logger:Epoch[093/800], Step[0100/0626], Avg Loss: 0.6832
INFO:master_logger:Epoch[093/800], Step[0100/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[093/800], Step[0100/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[093/800], Step[0200/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[093/800], Step[0200/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[093/800], Step[0200/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[093/800], Step[0200/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[093/800], Step[0200/0626], Avg Loss: 0.6844
INFO:local_logger:Epoch[093/800], Step[0200/0626], Avg Loss: 0.6835
INFO:master_logger:Epoch[093/800], Step[0200/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[093/800], Step[0200/0626], Avg Loss: 0.6837
INFO:local_logger:Epoch[093/800], Step[0200/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[093/800], Step[0300/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[093/800], Step[0300/0626], Avg Loss: 0.6832
INFO:local_logger:Epoch[093/800], Step[0300/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[093/800], Step[0300/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[093/800], Step[0300/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[093/800], Step[0300/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[093/800], Step[0300/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[093/800], Step[0300/0626], Avg Loss: 0.6831
INFO:master_logger:Epoch[093/800], Step[0300/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[093/800], Step[0400/0626], Avg Loss: 0.6832
INFO:local_logger:Epoch[093/800], Step[0400/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[093/800], Step[0400/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[093/800], Step[0400/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[093/800], Step[0400/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[093/800], Step[0400/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[093/800], Step[0400/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[093/800], Step[0400/0626], Avg Loss: 0.6830
INFO:master_logger:Epoch[093/800], Step[0400/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[093/800], Step[0500/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[093/800], Step[0500/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[093/800], Step[0500/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[093/800], Step[0500/0626], Avg Loss: 0.6832
INFO:local_logger:Epoch[093/800], Step[0500/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[093/800], Step[0500/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[093/800], Step[0500/0626], Avg Loss: 0.6837
INFO:local_logger:Epoch[093/800], Step[0500/0626], Avg Loss: 0.6830
INFO:master_logger:Epoch[093/800], Step[0500/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[093/800], Step[0600/0626], Avg Loss: 0.6832
INFO:local_logger:Epoch[093/800], Step[0600/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[093/800], Step[0600/0626], Avg Loss: 0.6833
INFO:local_logger:Epoch[093/800], Step[0600/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[093/800], Step[0600/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[093/800], Step[0600/0626], Avg Loss: 0.6827
INFO:master_logger:Epoch[093/800], Step[0600/0626], Avg Loss: 0.6833
INFO:local_logger:Epoch[093/800], Step[0600/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[093/800], Step[0600/0626], Avg Loss: 0.6835
INFO:local_logger:----- Epoch[093/800], Train Loss: 0.6837, time: 883.02
INFO:local_logger:Now training epoch 94. LR=0.000154
INFO:local_logger:----- Epoch[093/800], Train Loss: 0.6832, time: 882.93
INFO:local_logger:Now training epoch 94. LR=0.000154
INFO:local_logger:----- Epoch[093/800], Train Loss: 0.6827, time: 879.60
INFO:master_logger:----- Epoch[093/800], Train Loss: 0.6834, time: 879.60
INFO:local_logger:----- Epoch[093/800], Train Loss: 0.6832, time: 883.71
INFO:local_logger:Now training epoch 94. LR=0.000154
INFO:local_logger:----- Epoch[093/800], Train Loss: 0.6834, time: 883.93
INFO:local_logger:Now training epoch 94. LR=0.000154
INFO:local_logger:----- Epoch[093/800], Train Loss: 0.6837, time: 883.92
INFO:local_logger:Now training epoch 94. LR=0.000154
INFO:local_logger:----- Epoch[093/800], Train Loss: 0.6836, time: 884.00
INFO:local_logger:Now training epoch 94. LR=0.000154
INFO:local_logger:----- Epoch[093/800], Train Loss: 0.6834, time: 883.71
INFO:local_logger:Now training epoch 94. LR=0.000154
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-93-Loss-0.6827037774682657.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-93-Loss-0.6827037774682657.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-93-Loss-0.6827037774682657.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-93-Loss-0.6827037774682657.pdopt
INFO:local_logger:Now training epoch 94. LR=0.000154
INFO:master_logger:Now training epoch 94. LR=0.000154
INFO:local_logger:Epoch[094/800], Step[0000/0626], Avg Loss: 0.6799
INFO:local_logger:Epoch[094/800], Step[0000/0626], Avg Loss: 0.6772
INFO:local_logger:Epoch[094/800], Step[0000/0626], Avg Loss: 0.6722
INFO:master_logger:Epoch[094/800], Step[0000/0626], Avg Loss: 0.6832
INFO:local_logger:Epoch[094/800], Step[0000/0626], Avg Loss: 0.6904
INFO:local_logger:Epoch[094/800], Step[0000/0626], Avg Loss: 0.6963
INFO:local_logger:Epoch[094/800], Step[0000/0626], Avg Loss: 0.6866
INFO:local_logger:Epoch[094/800], Step[0000/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[094/800], Step[0000/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[094/800], Step[0100/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[094/800], Step[0100/0626], Avg Loss: 0.6834
INFO:master_logger:Epoch[094/800], Step[0100/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[094/800], Step[0100/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[094/800], Step[0100/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[094/800], Step[0100/0626], Avg Loss: 0.6837
INFO:local_logger:Epoch[094/800], Step[0100/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[094/800], Step[0100/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[094/800], Step[0100/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[094/800], Step[0200/0626], Avg Loss: 0.6837
INFO:local_logger:Epoch[094/800], Step[0200/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[094/800], Step[0200/0626], Avg Loss: 0.6847
INFO:local_logger:Epoch[094/800], Step[0200/0626], Avg Loss: 0.6837
INFO:local_logger:Epoch[094/800], Step[0200/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[094/800], Step[0200/0626], Avg Loss: 0.6828
INFO:master_logger:Epoch[094/800], Step[0200/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[094/800], Step[0200/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[094/800], Step[0200/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[094/800], Step[0300/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[094/800], Step[0300/0626], Avg Loss: 0.6849
INFO:local_logger:Epoch[094/800], Step[0300/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[094/800], Step[0300/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[094/800], Step[0300/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[094/800], Step[0300/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[094/800], Step[0300/0626], Avg Loss: 0.6828
INFO:master_logger:Epoch[094/800], Step[0300/0626], Avg Loss: 0.6832
INFO:local_logger:Epoch[094/800], Step[0300/0626], Avg Loss: 0.6832
INFO:local_logger:Epoch[094/800], Step[0400/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[094/800], Step[0400/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[094/800], Step[0400/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[094/800], Step[0400/0626], Avg Loss: 0.6845
INFO:local_logger:Epoch[094/800], Step[0400/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[094/800], Step[0400/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[094/800], Step[0400/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[094/800], Step[0400/0626], Avg Loss: 0.6831
INFO:master_logger:Epoch[094/800], Step[0400/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[094/800], Step[0500/0626], Avg Loss: 0.6833
INFO:local_logger:Epoch[094/800], Step[0500/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[094/800], Step[0500/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[094/800], Step[0500/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[094/800], Step[0500/0626], Avg Loss: 0.6841
INFO:local_logger:Epoch[094/800], Step[0500/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[094/800], Step[0500/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[094/800], Step[0500/0626], Avg Loss: 0.6832
INFO:master_logger:Epoch[094/800], Step[0500/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[094/800], Step[0600/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[094/800], Step[0600/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[094/800], Step[0600/0626], Avg Loss: 0.6840
INFO:local_logger:Epoch[094/800], Step[0600/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[094/800], Step[0600/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[094/800], Step[0600/0626], Avg Loss: 0.6828
INFO:master_logger:Epoch[094/800], Step[0600/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[094/800], Step[0600/0626], Avg Loss: 0.6827
INFO:local_logger:Epoch[094/800], Step[0600/0626], Avg Loss: 0.6825
INFO:local_logger:----- Epoch[094/800], Train Loss: 0.6835, time: 860.64
INFO:local_logger:Now training epoch 95. LR=0.000155
INFO:local_logger:----- Epoch[094/800], Train Loss: 0.6829, time: 861.10
INFO:local_logger:Now training epoch 95. LR=0.000155
INFO:local_logger:----- Epoch[094/800], Train Loss: 0.6834, time: 861.04
INFO:local_logger:Now training epoch 95. LR=0.000155
INFO:local_logger:----- Epoch[094/800], Train Loss: 0.6827, time: 861.88
INFO:local_logger:Now training epoch 95. LR=0.000155
INFO:local_logger:----- Epoch[094/800], Train Loss: 0.6838, time: 861.14
INFO:local_logger:Now training epoch 95. LR=0.000155
INFO:local_logger:----- Epoch[094/800], Train Loss: 0.6830, time: 857.69
INFO:master_logger:----- Epoch[094/800], Train Loss: 0.6831, time: 857.69
INFO:local_logger:----- Epoch[094/800], Train Loss: 0.6825, time: 861.06
INFO:local_logger:Now training epoch 95. LR=0.000155
INFO:local_logger:----- Epoch[094/800], Train Loss: 0.6830, time: 861.17
INFO:local_logger:Now training epoch 95. LR=0.000155
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-94-Loss-0.6830039001247509.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-94-Loss-0.6830039001247509.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-94-Loss-0.6830039001247509.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-94-Loss-0.6830039001247509.pdopt
INFO:local_logger:Now training epoch 95. LR=0.000155
INFO:master_logger:Now training epoch 95. LR=0.000155
INFO:local_logger:Epoch[095/800], Step[0000/0626], Avg Loss: 0.6832
INFO:master_logger:Epoch[095/800], Step[0000/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[095/800], Step[0000/0626], Avg Loss: 0.6771
INFO:local_logger:Epoch[095/800], Step[0000/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[095/800], Step[0000/0626], Avg Loss: 0.6798
INFO:local_logger:Epoch[095/800], Step[0000/0626], Avg Loss: 0.6706
INFO:local_logger:Epoch[095/800], Step[0000/0626], Avg Loss: 0.6905
INFO:local_logger:Epoch[095/800], Step[0000/0626], Avg Loss: 0.6843
INFO:local_logger:Epoch[095/800], Step[0000/0626], Avg Loss: 0.6855
INFO:local_logger:Epoch[095/800], Step[0100/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[095/800], Step[0100/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[095/800], Step[0100/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[095/800], Step[0100/0626], Avg Loss: 0.6826
INFO:master_logger:Epoch[095/800], Step[0100/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[095/800], Step[0100/0626], Avg Loss: 0.6827
INFO:local_logger:Epoch[095/800], Step[0100/0626], Avg Loss: 0.6827
INFO:local_logger:Epoch[095/800], Step[0100/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[095/800], Step[0100/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[095/800], Step[0200/0626], Avg Loss: 0.6833
INFO:local_logger:Epoch[095/800], Step[0200/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[095/800], Step[0200/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[095/800], Step[0200/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[095/800], Step[0200/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[095/800], Step[0200/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[095/800], Step[0200/0626], Avg Loss: 0.6829
INFO:master_logger:Epoch[095/800], Step[0200/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[095/800], Step[0200/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[095/800], Step[0300/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[095/800], Step[0300/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[095/800], Step[0300/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[095/800], Step[0300/0626], Avg Loss: 0.6827
INFO:local_logger:Epoch[095/800], Step[0300/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[095/800], Step[0300/0626], Avg Loss: 0.6833
INFO:local_logger:Epoch[095/800], Step[0300/0626], Avg Loss: 0.6820
INFO:master_logger:Epoch[095/800], Step[0300/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[095/800], Step[0300/0626], Avg Loss: 0.6835
INFO:local_logger:Epoch[095/800], Step[0400/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[095/800], Step[0400/0626], Avg Loss: 0.6832
INFO:local_logger:Epoch[095/800], Step[0400/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[095/800], Step[0400/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[095/800], Step[0400/0626], Avg Loss: 0.6832
INFO:local_logger:Epoch[095/800], Step[0400/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[095/800], Step[0400/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[095/800], Step[0400/0626], Avg Loss: 0.6826
INFO:master_logger:Epoch[095/800], Step[0400/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[095/800], Step[0500/0626], Avg Loss: 0.6832
INFO:local_logger:Epoch[095/800], Step[0500/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[095/800], Step[0500/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[095/800], Step[0500/0626], Avg Loss: 0.6828
INFO:master_logger:Epoch[095/800], Step[0500/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[095/800], Step[0500/0626], Avg Loss: 0.6827
INFO:local_logger:Epoch[095/800], Step[0500/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[095/800], Step[0500/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[095/800], Step[0500/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[095/800], Step[0600/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[095/800], Step[0600/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[095/800], Step[0600/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[095/800], Step[0600/0626], Avg Loss: 0.6827
INFO:local_logger:Epoch[095/800], Step[0600/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[095/800], Step[0600/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[095/800], Step[0600/0626], Avg Loss: 0.6825
INFO:master_logger:Epoch[095/800], Step[0600/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[095/800], Step[0600/0626], Avg Loss: 0.6826
INFO:local_logger:----- Epoch[095/800], Train Loss: 0.6831, time: 887.46
INFO:local_logger:Now training epoch 96. LR=0.000155
INFO:local_logger:----- Epoch[095/800], Train Loss: 0.6830, time: 886.49
INFO:local_logger:Now training epoch 96. LR=0.000155
INFO:local_logger:----- Epoch[095/800], Train Loss: 0.6825, time: 886.66
INFO:local_logger:Now training epoch 96. LR=0.000155
INFO:local_logger:----- Epoch[095/800], Train Loss: 0.6825, time: 886.64
INFO:local_logger:Now training epoch 96. LR=0.000155
INFO:local_logger:----- Epoch[095/800], Train Loss: 0.6829, time: 886.77
INFO:local_logger:Now training epoch 96. LR=0.000155
INFO:local_logger:----- Epoch[095/800], Train Loss: 0.6826, time: 886.77
INFO:local_logger:Now training epoch 96. LR=0.000155
INFO:local_logger:----- Epoch[095/800], Train Loss: 0.6826, time: 883.07
INFO:master_logger:----- Epoch[095/800], Train Loss: 0.6827, time: 883.07
INFO:local_logger:----- Epoch[095/800], Train Loss: 0.6827, time: 886.80
INFO:local_logger:Now training epoch 96. LR=0.000155
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-95-Loss-0.6825694100624208.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-95-Loss-0.6825694100624208.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-95-Loss-0.6825694100624208.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-95-Loss-0.6825694100624208.pdopt
INFO:local_logger:Now training epoch 96. LR=0.000155
INFO:master_logger:Now training epoch 96. LR=0.000155
INFO:local_logger:Epoch[096/800], Step[0000/0626], Avg Loss: 0.6925
INFO:local_logger:Epoch[096/800], Step[0000/0626], Avg Loss: 0.6757
INFO:local_logger:Epoch[096/800], Step[0000/0626], Avg Loss: 0.6684
INFO:master_logger:Epoch[096/800], Step[0000/0626], Avg Loss: 0.6799
INFO:local_logger:Epoch[096/800], Step[0000/0626], Avg Loss: 0.6804
INFO:local_logger:Epoch[096/800], Step[0000/0626], Avg Loss: 0.6683
INFO:local_logger:Epoch[096/800], Step[0000/0626], Avg Loss: 0.6801
INFO:local_logger:Epoch[096/800], Step[0000/0626], Avg Loss: 0.6900
INFO:local_logger:Epoch[096/800], Step[0000/0626], Avg Loss: 0.6837
INFO:local_logger:Epoch[096/800], Step[0100/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[096/800], Step[0100/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[096/800], Step[0100/0626], Avg Loss: 0.6836
INFO:master_logger:Epoch[096/800], Step[0100/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[096/800], Step[0100/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[096/800], Step[0100/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[096/800], Step[0100/0626], Avg Loss: 0.6841
INFO:local_logger:Epoch[096/800], Step[0100/0626], Avg Loss: 0.6827
INFO:local_logger:Epoch[096/800], Step[0100/0626], Avg Loss: 0.6813
INFO:local_logger:Epoch[096/800], Step[0200/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[096/800], Step[0200/0626], Avg Loss: 0.6816
INFO:local_logger:Epoch[096/800], Step[0200/0626], Avg Loss: 0.6816
INFO:local_logger:Epoch[096/800], Step[0200/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[096/800], Step[0200/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[096/800], Step[0200/0626], Avg Loss: 0.6825
INFO:master_logger:Epoch[096/800], Step[0200/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[096/800], Step[0200/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[096/800], Step[0200/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[096/800], Step[0300/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[096/800], Step[0300/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[096/800], Step[0300/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[096/800], Step[0300/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[096/800], Step[0300/0626], Avg Loss: 0.6819
INFO:master_logger:Epoch[096/800], Step[0300/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[096/800], Step[0300/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[096/800], Step[0300/0626], Avg Loss: 0.6827
INFO:local_logger:Epoch[096/800], Step[0300/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[096/800], Step[0400/0626], Avg Loss: 0.6827
INFO:local_logger:Epoch[096/800], Step[0400/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[096/800], Step[0400/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[096/800], Step[0400/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[096/800], Step[0400/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[096/800], Step[0400/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[096/800], Step[0400/0626], Avg Loss: 0.6822
INFO:master_logger:Epoch[096/800], Step[0400/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[096/800], Step[0400/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[096/800], Step[0500/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[096/800], Step[0500/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[096/800], Step[0500/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[096/800], Step[0500/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[096/800], Step[0500/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[096/800], Step[0500/0626], Avg Loss: 0.6821
INFO:master_logger:Epoch[096/800], Step[0500/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[096/800], Step[0500/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[096/800], Step[0500/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[096/800], Step[0600/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[096/800], Step[0600/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[096/800], Step[0600/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[096/800], Step[0600/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[096/800], Step[0600/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[096/800], Step[0600/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[096/800], Step[0600/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[096/800], Step[0600/0626], Avg Loss: 0.6827
INFO:master_logger:Epoch[096/800], Step[0600/0626], Avg Loss: 0.6824
INFO:local_logger:----- Epoch[096/800], Train Loss: 0.6821, time: 868.69
INFO:local_logger:Now training epoch 97. LR=0.000155
INFO:local_logger:----- Epoch[096/800], Train Loss: 0.6821, time: 869.09
INFO:local_logger:Now training epoch 97. LR=0.000155
INFO:local_logger:----- Epoch[096/800], Train Loss: 0.6823, time: 868.99
INFO:local_logger:Now training epoch 97. LR=0.000155
INFO:local_logger:----- Epoch[096/800], Train Loss: 0.6826, time: 868.81
INFO:local_logger:Now training epoch 97. LR=0.000155
INFO:local_logger:----- Epoch[096/800], Train Loss: 0.6824, time: 868.69
INFO:local_logger:----- Epoch[096/800], Train Loss: 0.6827, time: 864.96
INFO:local_logger:Now training epoch 97. LR=0.000155
INFO:master_logger:----- Epoch[096/800], Train Loss: 0.6824, time: 864.96
INFO:local_logger:----- Epoch[096/800], Train Loss: 0.6828, time: 868.82
INFO:local_logger:Now training epoch 97. LR=0.000155
INFO:local_logger:----- Epoch[096/800], Train Loss: 0.6826, time: 868.66
INFO:local_logger:Now training epoch 97. LR=0.000155
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-96-Loss-0.6826821214191926.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-96-Loss-0.6826821214191926.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-96-Loss-0.6826821214191926.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-96-Loss-0.6826821214191926.pdopt
INFO:local_logger:Now training epoch 97. LR=0.000155
INFO:master_logger:Now training epoch 97. LR=0.000155
INFO:local_logger:Epoch[097/800], Step[0000/0626], Avg Loss: 0.6880
INFO:master_logger:Epoch[097/800], Step[0000/0626], Avg Loss: 0.6850
INFO:local_logger:Epoch[097/800], Step[0000/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[097/800], Step[0000/0626], Avg Loss: 0.6940
INFO:local_logger:Epoch[097/800], Step[0000/0626], Avg Loss: 0.6844
INFO:local_logger:Epoch[097/800], Step[0000/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[097/800], Step[0000/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[097/800], Step[0000/0626], Avg Loss: 0.6755
INFO:local_logger:Epoch[097/800], Step[0000/0626], Avg Loss: 0.6928
INFO:local_logger:Epoch[097/800], Step[0100/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[097/800], Step[0100/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[097/800], Step[0100/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[097/800], Step[0100/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[097/800], Step[0100/0626], Avg Loss: 0.6822
INFO:master_logger:Epoch[097/800], Step[0100/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[097/800], Step[0100/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[097/800], Step[0100/0626], Avg Loss: 0.6831
INFO:local_logger:Epoch[097/800], Step[0100/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[097/800], Step[0200/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[097/800], Step[0200/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[097/800], Step[0200/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[097/800], Step[0200/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[097/800], Step[0200/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[097/800], Step[0200/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[097/800], Step[0200/0626], Avg Loss: 0.6823
INFO:master_logger:Epoch[097/800], Step[0200/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[097/800], Step[0200/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[097/800], Step[0300/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[097/800], Step[0300/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[097/800], Step[0300/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[097/800], Step[0300/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[097/800], Step[0300/0626], Avg Loss: 0.6822
INFO:master_logger:Epoch[097/800], Step[0300/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[097/800], Step[0300/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[097/800], Step[0300/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[097/800], Step[0300/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[097/800], Step[0400/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[097/800], Step[0400/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[097/800], Step[0400/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[097/800], Step[0400/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[097/800], Step[0400/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[097/800], Step[0400/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[097/800], Step[0400/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[097/800], Step[0400/0626], Avg Loss: 0.6818
INFO:master_logger:Epoch[097/800], Step[0400/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[097/800], Step[0500/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[097/800], Step[0500/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[097/800], Step[0500/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[097/800], Step[0500/0626], Avg Loss: 0.6826
INFO:master_logger:Epoch[097/800], Step[0500/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[097/800], Step[0500/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[097/800], Step[0500/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[097/800], Step[0500/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[097/800], Step[0500/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[097/800], Step[0600/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[097/800], Step[0600/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[097/800], Step[0600/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[097/800], Step[0600/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[097/800], Step[0600/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[097/800], Step[0600/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[097/800], Step[0600/0626], Avg Loss: 0.6825
INFO:master_logger:Epoch[097/800], Step[0600/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[097/800], Step[0600/0626], Avg Loss: 0.6814
INFO:local_logger:----- Epoch[097/800], Train Loss: 0.6822, time: 881.18
INFO:local_logger:Now training epoch 98. LR=0.000155
INFO:local_logger:----- Epoch[097/800], Train Loss: 0.6818, time: 882.30
INFO:local_logger:Now training epoch 98. LR=0.000155
INFO:local_logger:----- Epoch[097/800], Train Loss: 0.6822, time: 882.31
INFO:local_logger:Now training epoch 98. LR=0.000155
INFO:local_logger:----- Epoch[097/800], Train Loss: 0.6826, time: 882.32
INFO:local_logger:Now training epoch 98. LR=0.000155
INFO:local_logger:----- Epoch[097/800], Train Loss: 0.6813, time: 882.38
INFO:local_logger:Now training epoch 98. LR=0.000155
INFO:local_logger:----- Epoch[097/800], Train Loss: 0.6820, time: 882.40
INFO:local_logger:Now training epoch 98. LR=0.000155
INFO:local_logger:----- Epoch[097/800], Train Loss: 0.6818, time: 878.64
INFO:master_logger:----- Epoch[097/800], Train Loss: 0.6820, time: 878.64
INFO:local_logger:----- Epoch[097/800], Train Loss: 0.6823, time: 882.40
INFO:local_logger:Now training epoch 98. LR=0.000155
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-97-Loss-0.6818455600972856.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-97-Loss-0.6818455600972856.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-97-Loss-0.6818455600972856.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-97-Loss-0.6818455600972856.pdopt
INFO:local_logger:Now training epoch 98. LR=0.000155
INFO:master_logger:Now training epoch 98. LR=0.000155
INFO:local_logger:Epoch[098/800], Step[0000/0626], Avg Loss: 0.6930
INFO:local_logger:Epoch[098/800], Step[0000/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[098/800], Step[0000/0626], Avg Loss: 0.6821
INFO:master_logger:Epoch[098/800], Step[0000/0626], Avg Loss: 0.6860
INFO:local_logger:Epoch[098/800], Step[0000/0626], Avg Loss: 0.6802
INFO:local_logger:Epoch[098/800], Step[0000/0626], Avg Loss: 0.6848
INFO:local_logger:Epoch[098/800], Step[0000/0626], Avg Loss: 0.6756
INFO:local_logger:Epoch[098/800], Step[0000/0626], Avg Loss: 0.7011
INFO:local_logger:Epoch[098/800], Step[0000/0626], Avg Loss: 0.6899
INFO:local_logger:Epoch[098/800], Step[0100/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[098/800], Step[0100/0626], Avg Loss: 0.6824
INFO:master_logger:Epoch[098/800], Step[0100/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[098/800], Step[0100/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[098/800], Step[0100/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[098/800], Step[0100/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[098/800], Step[0100/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[098/800], Step[0100/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[098/800], Step[0100/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[098/800], Step[0200/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[098/800], Step[0200/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[098/800], Step[0200/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[098/800], Step[0200/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[098/800], Step[0200/0626], Avg Loss: 0.6826
INFO:master_logger:Epoch[098/800], Step[0200/0626], Avg Loss: 0.6827
INFO:local_logger:Epoch[098/800], Step[0200/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[098/800], Step[0200/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[098/800], Step[0200/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[098/800], Step[0300/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[098/800], Step[0300/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[098/800], Step[0300/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[098/800], Step[0300/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[098/800], Step[0300/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[098/800], Step[0300/0626], Avg Loss: 0.6822
INFO:master_logger:Epoch[098/800], Step[0300/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[098/800], Step[0300/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[098/800], Step[0300/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[098/800], Step[0400/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[098/800], Step[0400/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[098/800], Step[0400/0626], Avg Loss: 0.6827
INFO:local_logger:Epoch[098/800], Step[0400/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[098/800], Step[0400/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[098/800], Step[0400/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[098/800], Step[0400/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[098/800], Step[0400/0626], Avg Loss: 0.6817
INFO:master_logger:Epoch[098/800], Step[0400/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[098/800], Step[0500/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[098/800], Step[0500/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[098/800], Step[0500/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[098/800], Step[0500/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[098/800], Step[0500/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[098/800], Step[0500/0626], Avg Loss: 0.6823
INFO:master_logger:Epoch[098/800], Step[0500/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[098/800], Step[0500/0626], Avg Loss: 0.6816
INFO:local_logger:Epoch[098/800], Step[0500/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[098/800], Step[0600/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[098/800], Step[0600/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[098/800], Step[0600/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[098/800], Step[0600/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[098/800], Step[0600/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[098/800], Step[0600/0626], Avg Loss: 0.6816
INFO:master_logger:Epoch[098/800], Step[0600/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[098/800], Step[0600/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[098/800], Step[0600/0626], Avg Loss: 0.6820
INFO:local_logger:----- Epoch[098/800], Train Loss: 0.6819, time: 870.20
INFO:master_logger:----- Epoch[098/800], Train Loss: 0.6820, time: 870.20
INFO:local_logger:----- Epoch[098/800], Train Loss: 0.6823, time: 875.16
INFO:local_logger:Now training epoch 99. LR=0.000155
INFO:local_logger:----- Epoch[098/800], Train Loss: 0.6820, time: 874.01
INFO:local_logger:Now training epoch 99. LR=0.000155
INFO:local_logger:----- Epoch[098/800], Train Loss: 0.6825, time: 874.00
INFO:local_logger:Now training epoch 99. LR=0.000155
INFO:local_logger:----- Epoch[098/800], Train Loss: 0.6817, time: 874.10
INFO:local_logger:Now training epoch 99. LR=0.000155
INFO:local_logger:----- Epoch[098/800], Train Loss: 0.6822, time: 874.02
INFO:local_logger:Now training epoch 99. LR=0.000155
INFO:local_logger:----- Epoch[098/800], Train Loss: 0.6815, time: 874.10
INFO:local_logger:Now training epoch 99. LR=0.000155
INFO:local_logger:----- Epoch[098/800], Train Loss: 0.6816, time: 874.02
INFO:local_logger:Now training epoch 99. LR=0.000155
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-98-Loss-0.681889634827903.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-98-Loss-0.681889634827903.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-98-Loss-0.681889634827903.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-98-Loss-0.681889634827903.pdopt
INFO:local_logger:Now training epoch 99. LR=0.000155
INFO:master_logger:Now training epoch 99. LR=0.000155
INFO:local_logger:Epoch[099/800], Step[0000/0626], Avg Loss: 0.6968
INFO:local_logger:Epoch[099/800], Step[0000/0626], Avg Loss: 0.6870
INFO:local_logger:Epoch[099/800], Step[0000/0626], Avg Loss: 0.6865
INFO:master_logger:Epoch[099/800], Step[0000/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[099/800], Step[0000/0626], Avg Loss: 0.6853
INFO:local_logger:Epoch[099/800], Step[0000/0626], Avg Loss: 0.6660
INFO:local_logger:Epoch[099/800], Step[0000/0626], Avg Loss: 0.6719
INFO:local_logger:Epoch[099/800], Step[0000/0626], Avg Loss: 0.6836
INFO:local_logger:Epoch[099/800], Step[0000/0626], Avg Loss: 0.6839
INFO:local_logger:Epoch[099/800], Step[0100/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[099/800], Step[0100/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[099/800], Step[0100/0626], Avg Loss: 0.6834
INFO:local_logger:Epoch[099/800], Step[0100/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[099/800], Step[0100/0626], Avg Loss: 0.6826
INFO:master_logger:Epoch[099/800], Step[0100/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[099/800], Step[0100/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[099/800], Step[0100/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[099/800], Step[0100/0626], Avg Loss: 0.6826
INFO:local_logger:Epoch[099/800], Step[0200/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[099/800], Step[0200/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[099/800], Step[0200/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[099/800], Step[0200/0626], Avg Loss: 0.6828
INFO:local_logger:Epoch[099/800], Step[0200/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[099/800], Step[0200/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[099/800], Step[0200/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[099/800], Step[0200/0626], Avg Loss: 0.6822
INFO:master_logger:Epoch[099/800], Step[0200/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[099/800], Step[0300/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[099/800], Step[0300/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[099/800], Step[0300/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[099/800], Step[0300/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[099/800], Step[0300/0626], Avg Loss: 0.6811
INFO:master_logger:Epoch[099/800], Step[0300/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[099/800], Step[0300/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[099/800], Step[0300/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[099/800], Step[0300/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[099/800], Step[0400/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[099/800], Step[0400/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[099/800], Step[0400/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[099/800], Step[0400/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[099/800], Step[0400/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[099/800], Step[0400/0626], Avg Loss: 0.6825
INFO:local_logger:Epoch[099/800], Step[0400/0626], Avg Loss: 0.6816
INFO:master_logger:Epoch[099/800], Step[0400/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[099/800], Step[0400/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[099/800], Step[0500/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[099/800], Step[0500/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[099/800], Step[0500/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[099/800], Step[0500/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[099/800], Step[0500/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[099/800], Step[0500/0626], Avg Loss: 0.6815
INFO:master_logger:Epoch[099/800], Step[0500/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[099/800], Step[0500/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[099/800], Step[0500/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[099/800], Step[0600/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[099/800], Step[0600/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[099/800], Step[0600/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[099/800], Step[0600/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[099/800], Step[0600/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[099/800], Step[0600/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[099/800], Step[0600/0626], Avg Loss: 0.6817
INFO:master_logger:Epoch[099/800], Step[0600/0626], Avg Loss: 0.6816
INFO:local_logger:Epoch[099/800], Step[0600/0626], Avg Loss: 0.6817
INFO:local_logger:----- Epoch[099/800], Train Loss: 0.6811, time: 873.44
INFO:local_logger:Now training epoch 100. LR=0.000155
INFO:local_logger:----- Epoch[099/800], Train Loss: 0.6817, time: 874.12
INFO:local_logger:Now training epoch 100. LR=0.000155
INFO:local_logger:----- Epoch[099/800], Train Loss: 0.6814, time: 874.14
INFO:local_logger:Now training epoch 100. LR=0.000155
INFO:local_logger:----- Epoch[099/800], Train Loss: 0.6815, time: 874.31
INFO:local_logger:Now training epoch 100. LR=0.000155
INFO:local_logger:----- Epoch[099/800], Train Loss: 0.6816, time: 874.69
INFO:local_logger:Now training epoch 100. LR=0.000155
INFO:local_logger:----- Epoch[099/800], Train Loss: 0.6820, time: 874.75
INFO:local_logger:Now training epoch 100. LR=0.000155
INFO:local_logger:----- Epoch[099/800], Train Loss: 0.6814, time: 871.01
INFO:master_logger:----- Epoch[099/800], Train Loss: 0.6816, time: 871.01
INFO:local_logger:----- Epoch[099/800], Train Loss: 0.6820, time: 874.69
INFO:local_logger:Now training epoch 100. LR=0.000155
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-99-Loss-0.6813920508235197.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-99-Loss-0.6813920508235197.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-99-Loss-0.6813920508235197.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-99-Loss-0.6813920508235197.pdopt
INFO:local_logger:Now training epoch 100. LR=0.000155
INFO:master_logger:Now training epoch 100. LR=0.000155
INFO:local_logger:Epoch[100/800], Step[0000/0626], Avg Loss: 0.6874
INFO:local_logger:Epoch[100/800], Step[0000/0626], Avg Loss: 0.6858
INFO:local_logger:Epoch[100/800], Step[0000/0626], Avg Loss: 0.6763
INFO:master_logger:Epoch[100/800], Step[0000/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[100/800], Step[0000/0626], Avg Loss: 0.6794
INFO:local_logger:Epoch[100/800], Step[0000/0626], Avg Loss: 0.6727
INFO:local_logger:Epoch[100/800], Step[0000/0626], Avg Loss: 0.6920
INFO:local_logger:Epoch[100/800], Step[0000/0626], Avg Loss: 0.6764
INFO:local_logger:Epoch[100/800], Step[0000/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[100/800], Step[0100/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[100/800], Step[0100/0626], Avg Loss: 0.6824
INFO:local_logger:Epoch[100/800], Step[0100/0626], Avg Loss: 0.6801
INFO:local_logger:Epoch[100/800], Step[0100/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[100/800], Step[0100/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[100/800], Step[0100/0626], Avg Loss: 0.6821
INFO:master_logger:Epoch[100/800], Step[0100/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[100/800], Step[0100/0626], Avg Loss: 0.6807
INFO:local_logger:Epoch[100/800], Step[0100/0626], Avg Loss: 0.6809
INFO:local_logger:Epoch[100/800], Step[0200/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[100/800], Step[0200/0626], Avg Loss: 0.6804
INFO:local_logger:Epoch[100/800], Step[0200/0626], Avg Loss: 0.6816
INFO:local_logger:Epoch[100/800], Step[0200/0626], Avg Loss: 0.6816
INFO:local_logger:Epoch[100/800], Step[0200/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[100/800], Step[0200/0626], Avg Loss: 0.6812
INFO:master_logger:Epoch[100/800], Step[0200/0626], Avg Loss: 0.6813
INFO:local_logger:Epoch[100/800], Step[0200/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[100/800], Step[0200/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[100/800], Step[0300/0626], Avg Loss: 0.6816
INFO:local_logger:Epoch[100/800], Step[0300/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[100/800], Step[0300/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[100/800], Step[0300/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[100/800], Step[0300/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[100/800], Step[0300/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[100/800], Step[0300/0626], Avg Loss: 0.6812
INFO:master_logger:Epoch[100/800], Step[0300/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[100/800], Step[0300/0626], Avg Loss: 0.6807
INFO:local_logger:Epoch[100/800], Step[0400/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[100/800], Step[0400/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[100/800], Step[0400/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[100/800], Step[0400/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[100/800], Step[0400/0626], Avg Loss: 0.6815
INFO:master_logger:Epoch[100/800], Step[0400/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[100/800], Step[0400/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[100/800], Step[0400/0626], Avg Loss: 0.6813
INFO:local_logger:Epoch[100/800], Step[0400/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[100/800], Step[0500/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[100/800], Step[0500/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[100/800], Step[0500/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[100/800], Step[0500/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[100/800], Step[0500/0626], Avg Loss: 0.6812
INFO:master_logger:Epoch[100/800], Step[0500/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[100/800], Step[0500/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[100/800], Step[0500/0626], Avg Loss: 0.6816
INFO:local_logger:Epoch[100/800], Step[0500/0626], Avg Loss: 0.6813
INFO:local_logger:Epoch[100/800], Step[0600/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[100/800], Step[0600/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[100/800], Step[0600/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[100/800], Step[0600/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[100/800], Step[0600/0626], Avg Loss: 0.6813
INFO:local_logger:Epoch[100/800], Step[0600/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[100/800], Step[0600/0626], Avg Loss: 0.6814
INFO:master_logger:Epoch[100/800], Step[0600/0626], Avg Loss: 0.6813
INFO:local_logger:Epoch[100/800], Step[0600/0626], Avg Loss: 0.6814
INFO:local_logger:----- Epoch[100/800], Train Loss: 0.6815, time: 871.01
INFO:local_logger:Now training epoch 101. LR=0.000156
INFO:local_logger:----- Epoch[100/800], Train Loss: 0.6814, time: 870.82
INFO:local_logger:Now training epoch 101. LR=0.000156
INFO:local_logger:----- Epoch[100/800], Train Loss: 0.6814, time: 871.44
INFO:local_logger:Now training epoch 101. LR=0.000156
INFO:local_logger:----- Epoch[100/800], Train Loss: 0.6812, time: 871.50
INFO:local_logger:Now training epoch 101. LR=0.000156
INFO:local_logger:----- Epoch[100/800], Train Loss: 0.6812, time: 867.05
INFO:master_logger:----- Epoch[100/800], Train Loss: 0.6813, time: 867.05
INFO:local_logger:----- Epoch[100/800], Train Loss: 0.6811, time: 872.20
INFO:local_logger:Now training epoch 101. LR=0.000156
INFO:local_logger:----- Epoch[100/800], Train Loss: 0.6814, time: 870.95
INFO:local_logger:Now training epoch 101. LR=0.000156
INFO:local_logger:----- Epoch[100/800], Train Loss: 0.6814, time: 871.00
INFO:local_logger:Now training epoch 101. LR=0.000156
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-100-Loss-0.6812047341083004.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-100-Loss-0.6812047341083004.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-100-Loss-0.6812047341083004.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-100-Loss-0.6812047341083004.pdopt
INFO:local_logger:Now training epoch 101. LR=0.000156
INFO:master_logger:Now training epoch 101. LR=0.000156
INFO:local_logger:Epoch[101/800], Step[0000/0626], Avg Loss: 0.6922
INFO:local_logger:Epoch[101/800], Step[0000/0626], Avg Loss: 0.6755
INFO:local_logger:Epoch[101/800], Step[0000/0626], Avg Loss: 0.6834
INFO:master_logger:Epoch[101/800], Step[0000/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[101/800], Step[0000/0626], Avg Loss: 0.6807
INFO:local_logger:Epoch[101/800], Step[0000/0626], Avg Loss: 0.6772
INFO:local_logger:Epoch[101/800], Step[0000/0626], Avg Loss: 0.6966
INFO:local_logger:Epoch[101/800], Step[0000/0626], Avg Loss: 0.6690
INFO:local_logger:Epoch[101/800], Step[0000/0626], Avg Loss: 0.6717
INFO:local_logger:Epoch[101/800], Step[0100/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[101/800], Step[0100/0626], Avg Loss: 0.6798
INFO:local_logger:Epoch[101/800], Step[0100/0626], Avg Loss: 0.6819
INFO:master_logger:Epoch[101/800], Step[0100/0626], Avg Loss: 0.6809
INFO:local_logger:Epoch[101/800], Step[0100/0626], Avg Loss: 0.6809
INFO:local_logger:Epoch[101/800], Step[0100/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[101/800], Step[0100/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[101/800], Step[0100/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[101/800], Step[0100/0626], Avg Loss: 0.6813
INFO:local_logger:Epoch[101/800], Step[0200/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[101/800], Step[0200/0626], Avg Loss: 0.6813
INFO:master_logger:Epoch[101/800], Step[0200/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[101/800], Step[0200/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[101/800], Step[0200/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[101/800], Step[0200/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[101/800], Step[0200/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[101/800], Step[0200/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[101/800], Step[0200/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[101/800], Step[0300/0626], Avg Loss: 0.6809
INFO:local_logger:Epoch[101/800], Step[0300/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[101/800], Step[0300/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[101/800], Step[0300/0626], Avg Loss: 0.6813
INFO:local_logger:Epoch[101/800], Step[0300/0626], Avg Loss: 0.6807
INFO:master_logger:Epoch[101/800], Step[0300/0626], Avg Loss: 0.6809
INFO:local_logger:Epoch[101/800], Step[0300/0626], Avg Loss: 0.6807
INFO:local_logger:Epoch[101/800], Step[0300/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[101/800], Step[0300/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[101/800], Step[0400/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[101/800], Step[0400/0626], Avg Loss: 0.6804
INFO:local_logger:Epoch[101/800], Step[0400/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[101/800], Step[0400/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[101/800], Step[0400/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[101/800], Step[0400/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[101/800], Step[0400/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[101/800], Step[0400/0626], Avg Loss: 0.6810
INFO:master_logger:Epoch[101/800], Step[0400/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[101/800], Step[0500/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[101/800], Step[0500/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[101/800], Step[0500/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[101/800], Step[0500/0626], Avg Loss: 0.6807
INFO:local_logger:Epoch[101/800], Step[0500/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[101/800], Step[0500/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[101/800], Step[0500/0626], Avg Loss: 0.6810
INFO:master_logger:Epoch[101/800], Step[0500/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[101/800], Step[0500/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[101/800], Step[0600/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[101/800], Step[0600/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[101/800], Step[0600/0626], Avg Loss: 0.6807
INFO:local_logger:Epoch[101/800], Step[0600/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[101/800], Step[0600/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[101/800], Step[0600/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[101/800], Step[0600/0626], Avg Loss: 0.6808
INFO:master_logger:Epoch[101/800], Step[0600/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[101/800], Step[0600/0626], Avg Loss: 0.6809
INFO:local_logger:----- Epoch[101/800], Train Loss: 0.6805, time: 867.69
INFO:local_logger:Now training epoch 102. LR=0.000156
INFO:local_logger:----- Epoch[101/800], Train Loss: 0.6806, time: 868.18
INFO:local_logger:Now training epoch 102. LR=0.000156
INFO:local_logger:----- Epoch[101/800], Train Loss: 0.6806, time: 868.58
INFO:local_logger:Now training epoch 102. LR=0.000156
INFO:local_logger:----- Epoch[101/800], Train Loss: 0.6808, time: 864.53
INFO:master_logger:----- Epoch[101/800], Train Loss: 0.6808, time: 864.53
INFO:local_logger:----- Epoch[101/800], Train Loss: 0.6807, time: 868.26
INFO:local_logger:Now training epoch 102. LR=0.000156
INFO:local_logger:----- Epoch[101/800], Train Loss: 0.6810, time: 868.29
INFO:local_logger:Now training epoch 102. LR=0.000156
INFO:local_logger:----- Epoch[101/800], Train Loss: 0.6811, time: 868.42
INFO:local_logger:Now training epoch 102. LR=0.000156
INFO:local_logger:----- Epoch[101/800], Train Loss: 0.6812, time: 868.25
INFO:local_logger:Now training epoch 102. LR=0.000156
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-101-Loss-0.6808065795139766.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-101-Loss-0.6808065795139766.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-101-Loss-0.6808065795139766.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-101-Loss-0.6808065795139766.pdopt
INFO:local_logger:Now training epoch 102. LR=0.000156
INFO:master_logger:Now training epoch 102. LR=0.000156
INFO:local_logger:Epoch[102/800], Step[0000/0626], Avg Loss: 0.6861
INFO:master_logger:Epoch[102/800], Step[0000/0626], Avg Loss: 0.6794
INFO:local_logger:Epoch[102/800], Step[0000/0626], Avg Loss: 0.6910
INFO:local_logger:Epoch[102/800], Step[0000/0626], Avg Loss: 0.6753
INFO:local_logger:Epoch[102/800], Step[0000/0626], Avg Loss: 0.6669
INFO:local_logger:Epoch[102/800], Step[0000/0626], Avg Loss: 0.6722
INFO:local_logger:Epoch[102/800], Step[0000/0626], Avg Loss: 0.6901
INFO:local_logger:Epoch[102/800], Step[0000/0626], Avg Loss: 0.6797
INFO:local_logger:Epoch[102/800], Step[0000/0626], Avg Loss: 0.6743
INFO:local_logger:Epoch[102/800], Step[0100/0626], Avg Loss: 0.6830
INFO:local_logger:Epoch[102/800], Step[0100/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[102/800], Step[0100/0626], Avg Loss: 0.6822
INFO:local_logger:Epoch[102/800], Step[0100/0626], Avg Loss: 0.6813
INFO:local_logger:Epoch[102/800], Step[0100/0626], Avg Loss: 0.6817
INFO:master_logger:Epoch[102/800], Step[0100/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[102/800], Step[0100/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[102/800], Step[0100/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[102/800], Step[0100/0626], Avg Loss: 0.6804
INFO:local_logger:Epoch[102/800], Step[0200/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[102/800], Step[0200/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[102/800], Step[0200/0626], Avg Loss: 0.6807
INFO:master_logger:Epoch[102/800], Step[0200/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[102/800], Step[0200/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[102/800], Step[0200/0626], Avg Loss: 0.6819
INFO:local_logger:Epoch[102/800], Step[0200/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[102/800], Step[0200/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[102/800], Step[0200/0626], Avg Loss: 0.6803
INFO:local_logger:Epoch[102/800], Step[0300/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[102/800], Step[0300/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[102/800], Step[0300/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[102/800], Step[0300/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[102/800], Step[0300/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[102/800], Step[0300/0626], Avg Loss: 0.6814
INFO:master_logger:Epoch[102/800], Step[0300/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[102/800], Step[0300/0626], Avg Loss: 0.6821
INFO:local_logger:Epoch[102/800], Step[0300/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[102/800], Step[0400/0626], Avg Loss: 0.6816
INFO:local_logger:Epoch[102/800], Step[0400/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[102/800], Step[0400/0626], Avg Loss: 0.6804
INFO:local_logger:Epoch[102/800], Step[0400/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[102/800], Step[0400/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[102/800], Step[0400/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[102/800], Step[0400/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[102/800], Step[0400/0626], Avg Loss: 0.6810
INFO:master_logger:Epoch[102/800], Step[0400/0626], Avg Loss: 0.6809
INFO:local_logger:Epoch[102/800], Step[0500/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[102/800], Step[0500/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[102/800], Step[0500/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[102/800], Step[0500/0626], Avg Loss: 0.6809
INFO:local_logger:Epoch[102/800], Step[0500/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[102/800], Step[0500/0626], Avg Loss: 0.6807
INFO:local_logger:Epoch[102/800], Step[0500/0626], Avg Loss: 0.6820
INFO:local_logger:Epoch[102/800], Step[0500/0626], Avg Loss: 0.6811
INFO:master_logger:Epoch[102/800], Step[0500/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[102/800], Step[0600/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[102/800], Step[0600/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[102/800], Step[0600/0626], Avg Loss: 0.6817
INFO:local_logger:Epoch[102/800], Step[0600/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[102/800], Step[0600/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[102/800], Step[0600/0626], Avg Loss: 0.6810
INFO:master_logger:Epoch[102/800], Step[0600/0626], Avg Loss: 0.6811
INFO:local_logger:Epoch[102/800], Step[0600/0626], Avg Loss: 0.6809
INFO:local_logger:Epoch[102/800], Step[0600/0626], Avg Loss: 0.6813
INFO:local_logger:----- Epoch[102/800], Train Loss: 0.6812, time: 872.04
INFO:local_logger:Now training epoch 103. LR=0.000156
INFO:local_logger:----- Epoch[102/800], Train Loss: 0.6812, time: 868.78
INFO:master_logger:----- Epoch[102/800], Train Loss: 0.6811, time: 868.78
INFO:local_logger:----- Epoch[102/800], Train Loss: 0.6809, time: 872.56
INFO:local_logger:Now training epoch 103. LR=0.000156
INFO:local_logger:----- Epoch[102/800], Train Loss: 0.6805, time: 872.77
INFO:local_logger:Now training epoch 103. LR=0.000156
INFO:local_logger:----- Epoch[102/800], Train Loss: 0.6808, time: 873.71
INFO:local_logger:Now training epoch 103. LR=0.000156
INFO:local_logger:----- Epoch[102/800], Train Loss: 0.6809, time: 873.10
INFO:local_logger:Now training epoch 103. LR=0.000156
INFO:local_logger:----- Epoch[102/800], Train Loss: 0.6815, time: 873.19
INFO:local_logger:Now training epoch 103. LR=0.000156
INFO:local_logger:----- Epoch[102/800], Train Loss: 0.6815, time: 873.09
INFO:local_logger:Now training epoch 103. LR=0.000156
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-102-Loss-0.681159841605351.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-102-Loss-0.681159841605351.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-102-Loss-0.681159841605351.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-102-Loss-0.681159841605351.pdopt
INFO:local_logger:Now training epoch 103. LR=0.000156
INFO:master_logger:Now training epoch 103. LR=0.000156
INFO:local_logger:Epoch[103/800], Step[0000/0626], Avg Loss: 0.6787
INFO:master_logger:Epoch[103/800], Step[0000/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[103/800], Step[0000/0626], Avg Loss: 0.6646
INFO:local_logger:Epoch[103/800], Step[0000/0626], Avg Loss: 0.6794
INFO:local_logger:Epoch[103/800], Step[0000/0626], Avg Loss: 0.6850
INFO:local_logger:Epoch[103/800], Step[0000/0626], Avg Loss: 0.6838
INFO:local_logger:Epoch[103/800], Step[0000/0626], Avg Loss: 0.6909
INFO:local_logger:Epoch[103/800], Step[0000/0626], Avg Loss: 0.6721
INFO:local_logger:Epoch[103/800], Step[0000/0626], Avg Loss: 0.6917
INFO:local_logger:Epoch[103/800], Step[0100/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[103/800], Step[0100/0626], Avg Loss: 0.6811
INFO:master_logger:Epoch[103/800], Step[0100/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[103/800], Step[0100/0626], Avg Loss: 0.6829
INFO:local_logger:Epoch[103/800], Step[0100/0626], Avg Loss: 0.6810
INFO:local_logger:Epoch[103/800], Step[0100/0626], Avg Loss: 0.6795
INFO:local_logger:Epoch[103/800], Step[0100/0626], Avg Loss: 0.6823
INFO:local_logger:Epoch[103/800], Step[0100/0626], Avg Loss: 0.6818
INFO:local_logger:Epoch[103/800], Step[0100/0626], Avg Loss: 0.6802
INFO:local_logger:Epoch[103/800], Step[0200/0626], Avg Loss: 0.6799
INFO:local_logger:Epoch[103/800], Step[0200/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[103/800], Step[0200/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[103/800], Step[0200/0626], Avg Loss: 0.6810
INFO:master_logger:Epoch[103/800], Step[0200/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[103/800], Step[0200/0626], Avg Loss: 0.6815
INFO:local_logger:Epoch[103/800], Step[0200/0626], Avg Loss: 0.6804
INFO:local_logger:Epoch[103/800], Step[0200/0626], Avg Loss: 0.6797
INFO:local_logger:Epoch[103/800], Step[0200/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[103/800], Step[0300/0626], Avg Loss: 0.6807
INFO:local_logger:Epoch[103/800], Step[0300/0626], Avg Loss: 0.6803
INFO:local_logger:Epoch[103/800], Step[0300/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[103/800], Step[0300/0626], Avg Loss: 0.6799
INFO:local_logger:Epoch[103/800], Step[0300/0626], Avg Loss: 0.6803
INFO:local_logger:Epoch[103/800], Step[0300/0626], Avg Loss: 0.6801
INFO:local_logger:Epoch[103/800], Step[0300/0626], Avg Loss: 0.6806
INFO:master_logger:Epoch[103/800], Step[0300/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[103/800], Step[0300/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[103/800], Step[0400/0626], Avg Loss: 0.6807
INFO:local_logger:Epoch[103/800], Step[0400/0626], Avg Loss: 0.6809
INFO:local_logger:Epoch[103/800], Step[0400/0626], Avg Loss: 0.6802
INFO:local_logger:Epoch[103/800], Step[0400/0626], Avg Loss: 0.6800
INFO:local_logger:Epoch[103/800], Step[0400/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[103/800], Step[0400/0626], Avg Loss: 0.6805
INFO:master_logger:Epoch[103/800], Step[0400/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[103/800], Step[0400/0626], Avg Loss: 0.6814
INFO:local_logger:Epoch[103/800], Step[0400/0626], Avg Loss: 0.6803
INFO:local_logger:Epoch[103/800], Step[0500/0626], Avg Loss: 0.6802
INFO:local_logger:Epoch[103/800], Step[0500/0626], Avg Loss: 0.6812
INFO:local_logger:Epoch[103/800], Step[0500/0626], Avg Loss: 0.6802
INFO:local_logger:Epoch[103/800], Step[0500/0626], Avg Loss: 0.6807
INFO:local_logger:Epoch[103/800], Step[0500/0626], Avg Loss: 0.6808
INFO:local_logger:Epoch[103/800], Step[0500/0626], Avg Loss: 0.6809
INFO:local_logger:Epoch[103/800], Step[0500/0626], Avg Loss: 0.6804
INFO:master_logger:Epoch[103/800], Step[0500/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[103/800], Step[0500/0626], Avg Loss: 0.6804
INFO:local_logger:Epoch[103/800], Step[0600/0626], Avg Loss: 0.6806
INFO:local_logger:Epoch[103/800], Step[0600/0626], Avg Loss: 0.6809
INFO:local_logger:Epoch[103/800], Step[0600/0626], Avg Loss: 0.6801
INFO:local_logger:Epoch[103/800], Step[0600/0626], Avg Loss: 0.6802
INFO:master_logger:Epoch[103/800], Step[0600/0626], Avg Loss: 0.6805
INFO:local_logger:Epoch[103/800], Step[0600/0626], Avg Loss: 0.6802
INFO:local_logger:Epoch[103/800], Step[0600/0626], Avg Loss: 0.6809
INFO:local_logger:Epoch[103/800], Step[0600/0626], Avg Loss: 0.6802
INFO:local_logger:Epoch[103/800], Step[0600/0626], Avg Loss: 0.6806
INFO:local_logger:----- Epoch[103/800], Train Loss: 0.6801, time: 859.70
INFO:local_logger:Now training epoch 104. LR=0.000156
INFO:local_logger:----- Epoch[103/800], Train Loss: 0.6803, time: 859.89
INFO:local_logger:Now training epoch 104. LR=0.000156
INFO:local_logger:----- Epoch[103/800], Train Loss: 0.6805, time: 859.89
INFO:local_logger:Now training epoch 104. LR=0.000156
INFO:local_logger:----- Epoch[103/800], Train Loss: 0.6809, time: 856.80
INFO:master_logger:----- Epoch[103/800], Train Loss: 0.6805, time: 856.80
INFO:local_logger:----- Epoch[103/800], Train Loss: 0.6806, time: 860.28
INFO:local_logger:Now training epoch 104. LR=0.000156
INFO:local_logger:----- Epoch[103/800], Train Loss: 0.6801, time: 859.89
INFO:local_logger:Now training epoch 104. LR=0.000156
INFO:local_logger:----- Epoch[103/800], Train Loss: 0.6802, time: 859.91
INFO:local_logger:Now training epoch 104. LR=0.000156
INFO:local_logger:----- Epoch[103/800], Train Loss: 0.6809, time: 860.42
INFO:local_logger:Now training epoch 104. LR=0.000156
INFO:local_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-103-Loss-0.6808819352382769.pdparams
INFO:local_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-103-Loss-0.6808819352382769.pdopt
INFO:master_logger:----- Save model: ./output/train-20211219-17-07-40/MAE-Epoch-103-Loss-0.6808819352382769.pdparams
INFO:master_logger:----- Save optim: ./output/train-20211219-17-07-40/MAE-Epoch-103-Loss-0.6808819352382769.pdopt
INFO:local_logger:Now training epoch 104. LR=0.000156
INFO:master_logger:Now training epoch 104. LR=0.000156
INFO:local_logger:Epoch[104/800], Step[0000/0626], Avg Loss: 0.6583
INFO:local_logger:Epoch[104/800], Step[0000/0626], Avg Loss: 0.6660
INFO:local_logger:Epoch[104/800], Step[0000/0626], Avg Loss: 0.6799
INFO:master_logger:Epoch[104/800], Step[0000/0626], Avg Loss: 0.6722
INFO:local_logger:Epoch[104/800], Step[0000/0626], Avg Loss: 0.6668
INFO:local_logger:Epoch[104/800], Step[0000/0626], Avg Loss: 0.6885
INFO:local_logger:Epoch[104/800], Step[0000/0626], Avg Loss: 0.6790
INFO:local_logger:Epoch[104/800], Step[0000/0626], Avg Loss: 0.6707
INFO:local_logger:Epoch[104/800], Step[0000/0626], Avg Loss: 0.6680


--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::GpuMemcpySync(void*, void const*, unsigned long, cudaMemcpyKind)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1639995159 (unix time) try "date -d @1639995159" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x84e5) received by PID 25456 (TID 0x7f771efbe700) from PID 34021 ***]



--------------------------------------
C++ Traceback (most recent call last):
--------------------------------------
0   paddle::platform::GpuMemcpySync(void*, void const*, unsigned long, cudaMemcpyKind)

----------------------
Error Message Summary:
----------------------
FatalError: `Termination signal` is detected by the operating system.
  [TimeInfo: *** Aborted at 1639995171 (unix time) try "date -d @1639995171" if you are using GNU date ***]
  [SignalInfo: *** SIGTERM (@0x84e5) received by PID 25537 (TID 0x7fcf37fc6700) from PID 34021 ***]

Traceback (most recent call last):
  File "main_multi_gpu_pretrain.py", line 416, in <module>
    main()
  File "main_multi_gpu_pretrain.py", line 412, in main
    dist.spawn(main_worker, args=(config, dataset_train, ), nprocs=config.NGPUS)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/distributed/spawn.py", line 502, in spawn
    while not context.join():
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/distributed/spawn.py", line 312, in join
    self._throw_exception(error_index)
  File "/opt/conda/envs/py36/lib/python3.6/site-packages/paddle/distributed/spawn.py", line 320, in _throw_exception
    (error_index, name))
Exception: Process 7 terminated with signal SIGTERM.
/opt/conda/envs/py36/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 14 leaked semaphores to clean up at shutdown
  len(cache))
/opt/conda/envs/py36/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 14 leaked semaphores to clean up at shutdown
  len(cache))
/opt/conda/envs/py36/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 14 leaked semaphores to clean up at shutdown
  len(cache))
/opt/conda/envs/py36/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 14 leaked semaphores to clean up at shutdown
  len(cache))
/opt/conda/envs/py36/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 14 leaked semaphores to clean up at shutdown
  len(cache))
/opt/conda/envs/py36/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 14 leaked semaphores to clean up at shutdown
  len(cache))
/opt/conda/envs/py36/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 14 leaked semaphores to clean up at shutdown
  len(cache))
/opt/conda/envs/py36/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 14 leaked semaphores to clean up at shutdown
  len(cache))
/opt/conda/envs/py36/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 20 leaked semaphores to clean up at shutdown
  len(cache))
/opt/conda/envs/py36/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 20 leaked semaphores to clean up at shutdown
  len(cache))
/opt/conda/envs/py36/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 20 leaked semaphores to clean up at shutdown
  len(cache))
/opt/conda/envs/py36/lib/python3.6/multiprocessing/semaphore_tracker.py:143: UserWarning: semaphore_tracker: There appear to be 20 leaked semaphores to clean up at shutdown
  len(cache))
